### GBDT

通过将多个单个决策树(弱学习器)进行线性组合构成一个强学习器的过程，



一是效果确实挺不错。二是即可以用于分类也可以用于回归。三是可以筛选特征。

- gbdt 的算法的流程？
- gbdt 如何选择特征 ？
- gbdt 如何构建特征 ？
- gbdt 如何用于分类？
- gbdt 通过什么方式减少误差 ？
- gbdt的效果相比于传统的LR，SVM效果为什么好一些 ？
- gbdt 如何加速训练？
- gbdt的参数有哪些，如何调参 ？
- gbdt 实战当中遇到的一些问题 ？
- gbdt的优缺点 ？



idea:

  首先gbdt 是通过采用加法模型（即基函数的线性组合），以及不断减小训练过程产生的残差来达到将数据分类或者回归的算法。

![image-20211219195332849](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20211219195332849.png)

gbdt通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的**残差**基础上进行训练。对弱分类器的要求一般是足够简单，并且是**低方差高偏差**的。因为训练的过程是通过降低偏差来不断提高最终分类器的精度，（此处是可以证明的）。

​    弱分类器一般会选择为CART TREE（也就是分类回归树）。由于上述**高偏差和简单的要求 每个分类回归树的深度不会很深。**最终的总分类器是将每轮训练得到的**弱分类器加权求**和得到的（也就是加法模型）。



**操作流程：**

在上一轮分类器的**残差**基础上进行训练：意思就是，每个样本送到分类器中，得到预测值和真实值的残差，将残差作为新的label，送到下一个弱分类器训练，循环。



<img src="https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimgimage-20211220203041536.png" style="zoom:67%;" />

代码框架：

<img src="https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20211220203618896.png" alt="image-20211220203618896" style="zoom:67%;" />

