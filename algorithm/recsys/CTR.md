<img src="https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20211219192349938.png" alt="image-20211219192349938" style="zoom:50%;" />

一般会划分为**召回**和**排序**两层。

- **召回**负责从**百万级**物品中粗选出千级数量物品，常用算法有协同过滤、用户画像等，有时候也叫**粗排层**；
- **排序**负责对召回层召回的**千级**物品进行精细排序，也叫**精排层**；



召回：



排序：

**CTR，Click-Through-Rate，也就是点击率预估**，指的是**精排层**的排序。所以 CTR 模型的候选排序集一般是千级数量。



**CTR，Click-Through-Rate，\**也就是\**点击率预估**，指的是**精排层**的排序。所以 CTR 模型的候选排序集一般是千级数量。

CTR 模型的输入（即训练数据）是：大量成对的 **(features, label) **数据。

**features**可以分为如下四类：

1. **用户本身特征**，例如用户的年龄、性别等；
2. **用户行为特征**，例如点击过的物品、购买过的商品等；
3. **上下文特征**，例如用户登录设备(IOS or Android)、当前时间等；
4. **待排序物品特征**，例如物品 ID、物品被点击次数、物品的点击率等；



    可以看到，上面所有的 features 都是我们能够收集到的信息，其中有离散型特征（如物品 ID），也有连续型特征（如点击率）。
    
    但是，计算机只能处理数字编码，所以需要对 features 进行编码。常用的编码手段有：
    离散型特征使用 one-hot 或embedding；
    连续型特征可以不处理，也可以分段离散化，再使用 one-hot 编码；



**label**

label 即为模型的预测目标，CTR 场景预测点击率，label 即为用户对该物品是否产生点击行为，点击则 label 为 1，不点击则 label 为 0



所以训练时 CTR 模型输入即为：特征向量和其对应的 0, 1 标签。

预测时，输入只有特征向量，模型输出一个 0~1 之间的数字，代表预估的 CTR 值，可以用来做排序。

所以，建模之后，本质上 CTR 预估问题是一个二分类问题



**model**

1、LR



![image-20211219193045670](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20211219193045670.png)

优点：简单，容易实现；
 缺点：易学难精，需要**手工构造特征（特征交叉）**，需要**大量领域内知识**；另外，LR 非常适合处理离散特征，而对于连续特征，需要手工分段或归一化处理。

特征交叉是什么？

    组合特征的方式，例如有两个原始特征：年龄和性别。对应于 LR 就只有 2 个权重，但事实上我们可以将 “年龄x性别” 作为一个组合特征，构造出第三个特征，这样 LR 就有 3 个权重了。“18岁的男性” 就是组合特征 “年龄x性别” 的一种情况。

为什么需要手工构造特征（交叉特征）？

    LR 本质上是线性模型，只能捕捉到低阶特征，无法表达高阶特征信息，特征交叉本质是手工构造高阶特征的过程（特征的阶数可以类比代数几何中多项式的阶数）。

只要特征交叉做的足够好，简单的 LR 就可以取得非常好的效果。



==为什么不直接用SVR？==



2、 GBDT

Gradient Boosting Decison Tree，梯度提升决策树。是算法比赛中使用最多的模型之一。

核心思想：用回归树（CART 树）来拟合残差。
常用工具包：Xgboost，Lightgbm。

GBDT 使用的是 CART 树，是回归树的一种，如下图所示，回归树最后的预测结果是一个浮点数。



限于篇幅，这里不展开讲 GBDT 的具体原理，建议不了解的同学深入学习一下，**拟合残差**的思想是非常经典的。

优点：得益于 CART 树，可以方便地处理连续特征；另一个重要用处是输出特征重要性；
 缺点：比较容易过拟合，不是很适合处理离散特征。



3. GBDT + LR

问题背景：前面提到，LR 不适合处理连续型特征（如物品点击率、播放次数等），但实际应用场景中，连续型特征非常常见。

但是 GBDT 又非常适合处理连续特征，于是 Facebook 提出一种解决方案：GBDT + LR。核心思想是利用 GBDT 来自动对连续特征离散化。

如右图所示，可以通过 GBDT 中叶子节点的索引位置获得输入特征的离散化编码，可以看到，结果分别落在第 2 和第 1个叶子节点，转换特征编码为 [0, 1, 0] + [1, 0]，即 [0, 1, 0, 1, 0]，之后再将该编码输入到 LR 中训练。



将连续特征送到GBDT后，得到其离散化编码，再送到LR中训练。

优点：无需对连续特征手工处理；
 缺点：需要预训练 GBDT 模型；

4. FM (Factorization Machines)

问题背景：前面提到 LR 还有一个缺点，就是需要很多领域知识做人工特征交叉；

有没有办法将特征交叉这一过程自动化呢？

FM 就是为解决一个问题提出来的。

先看 LR 的线性部分：

在这里插入图片描述

可以看到，LR 没有考虑特征之间的相关性；例如 18 岁的男性相对于 18 岁的人和男性，喜欢电子游戏的概率明显要更大。LR 使用人工的方法，手动挖掘这类关联特征。

考虑极端情况，对所有特征进行二阶交叉：

![image-20211219202531819](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20211219202531819.png)

在这里插入图片描述
但这里会有一个问题：若特征数是 100w 级别，那么全交叉之后，特征数为 10000亿级别，对应的权重也有 10000亿，用单精度浮点数存储，需要 1TB，明显是不可能实现的。

所以，在 FM 提出之前，需要专家从海量可能的交叉特征中选取部分，然后实验，接着再选取。

FM 解决全交叉导致权重爆炸的核心思想是：权重的分解和共享，这也是其命令的由来。

先看分解，FM 提出将交叉特征的权重 wij 做一次分解，分解为两个隐向量（lantent vector）的乘积，如下：

<img src="https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20211219202821697.png" alt="image-20211219202821697" style="zoom:50%;" />



< ，> 代表向量乘法，k 是超参数，即 lantent vector 的维度。

光分解还是不够，因为权重爆炸的问题并没有解决，所以接下来就是共享隐向量：每个原始特征都对应同一个隐向量，这个隐向量是被所有该特征的交叉特征权重共享的。这样一来，若有 N 个基本特征，共享之后的参数量就为 N*k，而不是 N*N 了，一般情况 k << N。

将交叉特征权重分解为两个隐向量的乘积是 FM 的思想精华所在，通过这种方式，我们可以对原始特征进行全交叉，省去了手工交叉特征的麻烦。


5. FFM (Field-aware Factorization Machines)

FFM 可以理解为 FM 的一个 trick：跟 FM 的全局共享不一样，FFM 中隐向量不是全局共享的，而是在特征域内共享；

FFM 公式：

<img src="https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20211219202955583.png" alt="image-20211219202955583" style="zoom:50%;" />


其中，fj 是第 j 的特征所属的字段。如果隐向量的长度为 k，那么 FFM 的二次参数有 n*f*k 个，远多于 FM 模型的 n*k 个。
6. Wide & Deep

问题背景：FM 虽然解决了 LR 手工交叉特征的问题，但由于复杂度的原因，FM 只支持二阶特征的交叉，无法引入更高阶的交叉特征；

Wide & Deep 的核心思想就是：Wide 部分通过手工交叉特征提取低阶特征，Deep 部分通过神经网络提取高阶特征。

wide 部分等价于 LR，如下图：

![image-20211219204740254](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20211219204740254.png)



Note：wide&deep 是思想非常重要，它首次将神经网络引入了 CTR 模型，并且将模型分为 wide 和 deep 两部分的思路也启发了很多人，后面的模型本质上都可以算它的变种。

7. DeepFM

问题背景：wide & deep 模型中，wide 部分仍然需要人工交叉特征。

所以，很自然的，将 wide&deep 中的 **wide 部分替换成 FM **就好了。

DeepFM 模型：

![image-20211219204846569](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20211219204846569.png)



8. Deep&Cross

也是 wide&deep 的思路，跟 DeepFM 类似，也是改动了 wide 部分，模型如下：

![image-20211219205153705](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20211219205153705.png)<img src="https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20211219205716590.png" alt="image-20211219205716590" style="zoom:50%;" />

9. NFM

同样是 wide&deep 思路，不过改进的是 deep 部分，模型公式如下：

![image-20211219205825665](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20211219205825665.png) 

10. DIN

Deep Interest Network，来自阿里妈妈的精准定向检索及基础算法团队，已应用于实际业务，非常具有参考价值。

DIN 原始的应用场景为淘宝购物，作者在实践中观察到用户的两个特点：

    **Diversity：**用户的购物兴趣是非常广泛的；
    **Local Activation：**一个爱游泳的人，他之前购买书籍、冰激凌、薯片、游泳镜。当前给他推荐的护目镜。那么他是否会点击这次广告，跟他之前是否购买过薯片、书籍、冰激凌一点关系也没有，而是与他之前购买过游泳帽有关系。也就是说在这一次 CTR 预估中，部分历史数据 (游泳镜) 起了决定作用，而其他的基本都没用。

如何来建模 Local Activation 这一行为呢？
答：引入起始于机器翻译中的 Attention 机制；

核心思想：将待排序物品和单个历史购买物品输入进神经网络，神经网络的输出为单个数值，就是该历史购买物品的注意力权重。

DIN 中的 Attention 网络结构如下图：
![image-20211219210029909](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20211219210029909.png)

11. YouTube Deep Ranking

前面讲的都是标准的 CTR 模型，但事实上，有时候我们预测的目标并不是 CTR，比如在视频推荐中，如果仅仅预测 CTR，那很有可能会推出大量的标题党，我们可能希望预测的是用户观看该视频的时长。YouTube 给出了一个很好的解决方案。

核心思想是：使用带权重的 LR，对于正样本，权重为用户观看时长，对于负样本，权重为 1，这样就将预测目标由 CTR 变为了预测用户观看该视频的期望时长。

网络结构如下，其实就是普通的 deep 网络，只是最后使用了带权重的 LR loss。



