作者：言煜秋
链接：https://zhuanlan.zhihu.com/p/368806030
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



## 定点

Q格式：

- [DSP上浮点数据定点化处理 Q格式(Q15)_Nessaj Heng-CSDN博客](https://blog.csdn.net/HengZo/article/details/50255563?utm_source=blogxgwz0)
- [FPGA浮点小数与定点小数的换算及应用_Flip Program-CSDN博客](https://blog.csdn.net/github_33678609/article/details/53465626)

-cp 11表示5.11的精度（11是小数的精度），即4位整数11位小数，1个符号位（共16位，实际运算时上限为32位，int型）。

-cp 7.9表示6位整数，9位小数，1个符号位。此时原小数不能超过64（2^6）。

精度设置：当设置为7.9时，weight和feature尽量64以内，超过±64会被clip掉，这种参数分布多了精度就会损失很大，且每次的feature也要测算，不能超过这样的精度，如果超的多可以试着加scale层来平衡一下（每次卷积后加个scale控制大小），如果统计发现参数和feature map整体都偏小，甚至小于8，小于 4，那么这个精度可以设置成4.12，甚至每一层可以有不同的精度，要先做好统计，怎样进行定点化合适，用统计工具计算数值的分布，确定一个值。

matlab写sdk，把x（输入）和w（权重），都限制在（-1,1），然后scale到（-2^B，2^B），即最简单的非动态对称量化。测试得B=11时量化结果和浮点差距不大，但使用1608对应工具量化效果和浮点结果差距较大（后续改进）。

## 评估模型

RK1608理论16bit乘法：32次/cycle 频率:600M 合19.2G次/s。

统计Unet-16模型耗时（us），MAC，MAC/cycle，MAC利用率，数据搬运量，带宽（带宽上限2G）等。

- 总耗时：52ms。
- MAC：207M
- MAC利用率17%（最高一层31%，测试其他语义网络最高一层到60%）
- 各层数据搬运量在1G/s以内

在npu（1808）上8bit卷积利用率7%（理论通用网络如shufflenet测得最高30%），需要dw改为普通卷积，imresize改为transf，通道数12->64。此时模型复杂度提高了156倍。

npu测试量化效果如下：

![img](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgv2-549a88068e7558ce2092a4b29ae1de97_b.jpg)![img](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgv2-549a88068e7558ce2092a4b29ae1de97_720w.jpg)分类网络测试npu量化效果

## ceva指令

## pingpang

dsp 内部共 256K（实际可用230k左右），每次读入的块需要考虑要ping-pang，按照4倍来算，同时要考虑查表占的空间。

## ddr

![img](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgv2-21e31a9147d80d0057bafd70c0c6b664_b.png)![img](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgv2-21e31a9147d80d0057bafd70c0c6b664_720w.png)

DDR跑528MHz(理论带宽是528MHz*42=4.224*65%=2.7GB/s)，DSP跑600MHz(接口理论带宽是300MHz*16B=4.8GB/s) 。

表中“带宽DDR”是指从DDR读/写10bit数据的带宽，“带宽DMEM”是指从DMEM读/写16bit数据的带宽。

1. 无论是哪一种模式，其实**搬运速率受限于从DMEM读/写**，因为DMEM端的数据量是以16bit为单位进行计算的，也就是说，DDR->DMEM时，速率受限于往DMEM写的速率;DMEM->DDR时,速率受限于从DMEM读的速率。
2. DMEM接口上的理论带宽是300MHz*16B=4.8GB/s。因为noc设计的原因，在noc内部，写数据会和读命令会共用一个通道，读数据和写响应会用另外一个通道。RDMA传输的burst长度为8，这样，写数据或者读数据会占用8个周期，写响应或者读命令会占用1个周期。当往DMEM写/读数据时，实际能免使用的带宽是4.8GB/s*(8/(8+1))~=4.3GB/s。实际使用中，读写不一定是等量以及并不是所的burst都是8，实际可用带宽会在4.3GB/s上下浮动。

结合以上几点：

1. 当RDMA工作在264MHz时，真正带宽是受限于RDMA的时钟频率，所以带宽会低一些;
2. 当RDMA工作在528MHz时，真正带宽是受限于DSP的时钟频率，这时候，“带宽DMEM”已经在4.3GB/s左右，可以认为已经达到了理论上估计的最大带宽。

修改outstand num后实际芯片测出来的数据：

1. RDMA跑200MHz时,"带宽DDR": DDR->DMEM时为1.4G/s， DMEM->DDR时为1.8GB/s。
2. 提高RDMA的频率到400MHz,带宽提升的效果不明显。

带宽没有RTL仿真的好，且提高RDMA频率也没收益，但RTL仿真时，提升RDMA频率能提高整体的带宽（目的是确保限制带宽的不是RDMA的频率），因此，初步怀疑实际芯片中限制整体带宽的可能是DDR这边的效率而不是DMEM这边的效率。（理论上，真正限制带宽的应该是DMEM这边的效率）影响DDR效率的，主要是DDRC和NOC内部的相关timing配置，可调整相关参数看是否有效果。

## 数据对齐

[Data alignment: Straighten up and fly right](https://developer.ibm.com/technologies/systems/articles/pa-dalign/#)