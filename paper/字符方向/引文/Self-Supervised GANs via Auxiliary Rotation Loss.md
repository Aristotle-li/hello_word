

> 题目：Self-Supervised GANs via Auxiliary Rotation Loss
>
> 来源：2019
>
> 作者：

### 





在GANs的背景下，学习不同层次的细节、结构和纹理可以被视为不同的任务。例如，如果生成器首先学习全局结构，那么鉴别器自然会尝试构建一个表示，该表示允许它仅基于全局结构的差异或缺乏局部结构来有效地惩罚生成器。因此，训练中不稳定的一个来源是，只要当前表示有助于区分不同类别，就不会激励鉴别器保持有用的数据表示