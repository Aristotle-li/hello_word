



### introduction：

##### RD-GAN: Few/Zero-Shot Chinese Character Style Transfer via Radical Decomposition and Rendering

随着深度学习的发展，字符识别已经达到了前所未有的发展阶段；但是，它非常依赖于数据。在许多情况下，例如在历史文献中，字符样本价格昂贵/难以获取。获取字符样本的最有效方法之一是通过样式转换生成字符数据。不幸的是，与字符的自动识别相比，字符生成仍然是一个相对未被探索的问题[24,34]。这种不平衡的进展不利于光学字符识别的发展。最近，有很多人尝试生成简单字符，如英语和拉丁语字符[13,21]；然而，汉字生成还没有得到广泛的研究。与英文或拉丁文字符生成相比，汉字生成更具挑战性，因为它具有以下特点。首先，汉字的词汇量非常大。为了解决这个问题，[61]通过引入基于递归神经网络（RNN）的模型生成汉字，并学习低维字符标签嵌入。但是，此方法只能生成模型已看到的角色。不幸的是，几乎不可能从固定样式中获得所有类别的样本。生成看不见的汉字仍然是一个紧迫的问题。此外，汉字包含大量的字形，其内容和特征样式复杂，与部件形状和笔画样式不同。最近的作品，如“重写”[2]及其高级版本“子2zi”[3]，通过学习将源样式映射到目标样式（具有数千个字符对）来生成汉字，以便进行强有力的监督。然而，这些方法仍然不能生成看不见的汉字。最后，与从照片到艺术品的任务不同，汉字具有复杂而灵活的结构。骨骼和笔划中的细微错误是明显的，也是不可接受的。通过组合部首和笔划的组成部分，在汉字生成方面进行了一些尝试[52,48,55]。然而，这些方法表现不佳的原因有两个：1）它们在很大程度上依赖于自由基/笔划提取的性能，而在实际应用中，完美的自动自由基/笔划提取几乎是不可能的；2）他们更注重部首/笔划的呈现，而忽略了它们之间的内在联系。尽管汉字的词汇量非常大，但大约1000个部首可以组成10000多个字符[46]。



Introduction **With the development of 随着---的发展** deep learning, character recognition **已经达到了一个前所未有的发展水平has reached an unprecedented stage of development**; however, it is very **data-dependent 数据依赖**. **In many cases在很多情况下**, such as in historical documents, **character samples are expensive/difficult to obtain 字符样本获取成本很大**. **One of the most efficient ways to** obtain character samples **is to** generate character data **via** style transfer. Unfortunately, character generation remains a relatively under-explored problem compared with the automatic recognition of characters [24, 34]. This unbalanced progress **is detrimental to 不利于** the development of **optical character recognition**. Recently, **there have been many attempts to** generate simple characters such as English and Latin characters [13, 21]; however, Chinese character generation has not been explored extensively. **Compared with** English or Latin character generation, Chinese character generation is **much more challenging** **owing to由于** the following characteristics. First, Chinese characters **share an extremely large vocabulary有大量词汇**. **To address this issue为了解决这个问题**, [61] generated Chinese characters **by introducing** a Recurrent Neural Network (RNN)-based model and learned a low-dimensional character label embedding. However, this method can only generate characters that the model has seen. **Unfortunately, it is almost impossible to---是几乎不可能的** obtain all of the categories’ samples from a fixed style. Generating unseen Chinese characters **remains an urgent problem**. **Moreover, Chinese characters contain a large number of glyphs with complicated content and characteristic style that vary from the shapes of the component and the stroke styles**. **Recent works such as最近的工作例如** “Rewrite” [2] and its advanced version “zi2zi” [3] generated Chinese characters **by learning to map通过学习一个映射从---到--** the source style **to** a target style **with用** thousands of character pairs **for** strong supervision. **However, these methods still cannot generate unseen Chinese characters**. Finally, unlike the photo-to-artwork task, **Chinese characters have a complex and flexible structure.** Subtle errors in the skeleton and stroke are obvious and unacceptable. **Some attempts have been made in** Chinese character generation **通过 by assembling components of radicals and strokes** [52, 48, 55]. **这些性能不好有两个原因However, these performed poorly for two reasons:** 1) they are largely dependent on the performance of radical/stroke extraction while perfect automatic radical/stroke extraction is almost impossible in real applications; and 2) **they pay more attention to** the rendering of the radical/stroke while ignoring their internal relationship. Although Chinese characters comprise an extremely large vocabulary, **---能被---组成 more than 10,000 characters can be composed by approximately 1000 radicals** [46]. 



同时，所有汉字都可以分解成一个唯一的部首字符串。当人们学习汉字时，他们首先学习构成汉字的部首和结构。通过学习部首和结构，学习读写汉字的难度显著降低。基于上述观察结果，我们提出了一种新的基于根分解和渲染的汉字风格转换方法（RD-GAN），该方法可以在少量样本的情况下高效地生成看不见的汉字。RDGAN由三个组件组成：用于粗略提取部首的部首提取模块（REM）、学习如何使用目标样式中的笔划细节渲染部首的部首渲染模块（RRM）以及确保生成的角色图像的全局结构和局部细节的多级鉴别器（MLD）。所提出的RD-GAN的优点可以概括如下：–由于字符和部首之间关系的特殊性，我们可以只使用少数样本有效地生成看不见的汉字。这可以大大减少收集培训数据的难度和工作量通过将汉字分解为部首，渲染难度显著降低。由于采用了多级鉴别器，我们可以生成样式化的汉字，这些汉字不仅具有良好的细节，而且具有更真实的组合成分RD-GAN可以生成真实的字符样本，用于训练具有少量真实数据的字符分类器。实验表明，我们的方法可以有效地传输看不见的汉字，并且比目前最先进的方法具有更好的性能。

Meanwhile, all Chinese characters **---能被---分解成---can be decomposed into a unique radical string.** When people learn Chinese characters, they first learn the radicals and structures that form characters. By learning radicals and structures, the difficulty of learning to read and write Chinese characters decreases significantly. **Compelled by the above observations,** **we propose a novel radical decompositionand-rendering-based GAN(RD-GAN) for Chinese character style transfer that can efficiently generate unseen Chinese characters with a few samples**. 

The RDGAN **consists of由---组成** three components: a radical extraction module (REM) **to extract** the radical roughly, radical rendering module (RRM) **that learns how to** render the radical with stroke details in the target style, and multi-level discriminator (MLD) **that guarantees** the global structure and local details of the generated character images. The advantages of the proposed RD-GAN can be summarized as follows: 

– **Owing to the specificity of the relationship between** characters and radicals, we can use only a few samples to generate unseen Chinese characters efficiently. This can largely reduce the difficulty and labor of collecting training data. 

– By decomposing Chinese characters into radicals, the rendering difficulty decreases significantly. Owing to the multi-level discriminator, we can generate stylized Chinese characters **that not only have good details but also have more realistically combined components.**

 – RD-GAN can generate realistic character samples for training character classifiers **with** few real data. **Experiments show that our method can effectively** transfer unseen Chinese characters and obtain better performance than recent state-of-the-art methods.





##### SCFont: Structure-Guided Chinese Font Generation via Deep Stacked Networks

Introduction **Compared to/with** uniform-looking glyphs, now more and more people prefer using personalized fonts, **especially those** in distinguished handwriting styles, **in many scenarios.** **On the one hand,** handwriting styles are flexible to express personality and **赋予endow texts with writers’ distinctive characteristics and hallmarks**. On the other hand, **glyphs in** personalized handwriting styles **bring about** dynamic visual perceptions, **which are able to attract more attentions in various social networking media.能吸引更多注意力在各种社交场景中** However, creating a handwriting Chinese font library **is still a time-consuming and laborious work with long production cycle费时且劳动量大生产周期长**. **The reasons are threefold原因主要有三点**: 1) Chinese characters **have complex structures** **and thus yield great variance in writing styles因此产生了很大的差异在书写风格中**. 2) Unlike the English or Latin **typefaces** that **only contain a small number of glyphs**, **即使最常用的汉字集也有--组成even the most commonly used Chinese charset (i.e., GB2312) is composed of** 6763 characters. **It is hard for** ordinary people **to** **write out写下** such large amounts of characters while still maintaining the style consistency. 3) The handwriting fonts now available on electronic devices are mainly produced by professional font designers in commercial companies. They **rely heavily on严重依赖** elaborate adjustments for each glyph, which **is impractical不切实际的** and **infeasible** for fast font generation aiming at common customers. **Thereby,** generating Chinese handwriting fonts for ordinary people is still a tough task. **The majority of traditional CG-based methods take advantage of structural correlations among Chinese characters to reuse parts of input glyphs.利用汉字之间结构相关性去重用部分输入字形** Typically, the input character images **are first decomposed into** pre-defined strokes or radicals. Then, unseen characters **are synthesized by** **properly assembling the extracted components.看不见的汉字被合成通过正确装配抽取的组件** Nevertheless, the qualities of many synthesized glyphs are unsatisfactory such that **manual interventions are typically required.通常需要人工干预**

 Recently, **the utilization of 利用---** **deep learning-based approaches** **enables** the font generation problem **to be** resolved **in** an end-to-end **manner**. For one thing, the font generation can **be regarded as** a style transfer problem where characters in the reference style are transferred to a specific style **while maintaining the consistency of the contents**. For another, **with the advances of** generative adversarial networks (GAN), **more realistic and higher-quality glyphs** can be synthesized. However, the entire generative process is uncontrollable and unpredictable. **Blur and ghosting artifacts are often contained in the generated glyphs.** Besides, **for characters with complicated structures and/or in cursive handwriting styles**, those end-to-end approaches often produce results with unreasonable strokes or/and incorrect structures. **In this paper，基于以上观察**, **we propose a structure-guided Chinese font generation system, SCFont, which integrates the prior domain knowledge of Chinese characters with deep stacked networks to synthesize visually-pleasing character images with correct contents整合汉字先验信息到合成的字符图片中.** We **decompose** the font generation **task into two separate procedures**, namely writing trajectories synthesis and font style rendering. **The whole pipeline is shown in** Figure 1**. In the first phase**, each character **is represented as** **a series of** writing trajectories **with separated strokes,** **which is also named as** the **skeleton of character** in this paper. We **utilize利用** a multi-stage CNN model to transfer writing trajectories in the reference style into those in the target style.

 **In the second phase**, synthesized skeletons of characters **are rendered with--用--通过--** a specific handwriting style **via** a GAN model **to recover** shape details **on** the contour of glyphs. 

Finally, the complete font library composed of 6763 characters **in** the writer’s handwriting style **can be obtained.** **Compared with existing methods in the literature, the proposed SCFont is more capable of与现存的文献资料相比，提出的--更** **handling two crucial requirements in font generation 解决字体生成中两个关键需求**, **i.e.即**, **structure correctness and style consistency.** To be specific, writing trajectories can be synthesized by the learned skeleton flow vectors which indicate how to map the corresponding pixels in the input to those in the output. In this manner, we not only make the learning problem more tractable, which avoids learning to generate writing trajectories from scratches, but also provide a natural way of preserving the identity and structure of glyphs. Additionally, **to further enhance style similarity with the original handwritings为了进一步增强和---的风格相似性**, a stacked generation network **is designed to refine the ambiguous parts** or/and artifacts and precisely recover the stylish details on the contour of glyphs.

简介与外观统一的字形相比，现在越来越多的人更喜欢在许多场景中使用个性化字体，尤其是那些具有独特笔迹风格的字体。一方面，笔迹风格灵活多变，表现个性，赋予文本以作家的鲜明特征和特征。另一方面，个性化笔迹风格中的字形带来了动态的视觉感知，能够在各种社交网络媒体中吸引更多的注意力。然而，创建手写汉字字库仍然是一项耗时费力、生产周期长的工作。原因有三：1）汉字结构复杂，书写风格差异较大。2） 与仅包含少量字形的英文或拉丁字体不同，即使是最常用的中文字符集（即GB2312）也由6763个字符组成。普通人很难写出这么多的字，同时又能保持风格的一致性。3） 目前电子设备上可用的手写字体主要由商业公司的专业字体设计师制作。他们严重依赖于对每个字形的精细调整，这对于针对普通客户的快速字体生成是不切实际和不可行的。因此，为普通人生成中文手写字体仍然是一项艰巨的任务。大多数基于CG的传统方法利用汉字之间的结构相关性来重用部分输入字形。通常，输入字符图像首先分解为预定义的笔划或半径。然后，通过正确地组合提取的组件来合成看不见的字符。然而，许多合成字形的质量并不令人满意，因此通常需要手动干预。最近，基于深度学习的方法的使用使得字体生成问题能够以端到端的方式得到解决。首先，字体生成可以被视为一个样式转换问题，其中参考样式中的字符被转换为特定样式，同时保持内容的一致性。另一方面，随着生成性对抗网络（GAN）的发展，可以合成更真实、更高质量的字形。然而，整个生成过程是不可控和不可预测的。模糊和重影瑕疵通常包含在生成的图示符中。此外，对于具有复杂结构和/或草书书写风格的字符，这些端到端方法通常会产生笔划不合理或/或结构不正确的结果。在本文中，我们提出了一个结构导向的汉字字体生成系统SCFont，该系统将汉字的先验领域知识与深度堆叠网络相结合，以合成具有正确内容的视觉愉悦的汉字图像。我们将字体生成任务分解为两个独立的过程，即书写轨迹合成和字体样式渲染。整个管道如图1所示。在第一阶段，每个字符被表示为一系列笔划分离的书写轨迹，本文称之为字符骨架。我们利用一个多阶段CNN模型将参考风格的写作轨迹转换为目标风格的写作轨迹。在第二阶段，通过GAN模型以特定笔迹风格渲染合成的字符骨架，以恢复字形轮廓上的形状细节。最后，可以获得由6763个手写体字符组成的完整字体库。与文献中已有的方法相比，提出的SCFont更能处理字体生成中的两个关键要求，即结构正确性和样式一致性。具体地说，写入轨迹可以由学习的骨架流向量合成，该骨架流向量指示如何将输入中的对应像素映射到输出中的对应像素。通过这种方式，我们不仅使学习问题更容易处理，从而避免学习从划痕生成书写轨迹，而且还提供了一种自然的方式来保持字形的身份和结构。此外，为了进一步增强与原始手写体的风格相似性，设计了一个堆叠生成网络，以细化模棱两可的部分或/和工件，并精确恢复字形轮廓上的时尚细节。

##### Multi-scale multi-class conditional generative adversarial network for handwritten character generation

1 Introduction **There are two different ways to present a character有两种不同的概念理解字符**. One is to treat it as a combination of pixels and another is to consider it as a sequence or combination of strokes. The former is also called **an image for a character.** As to the latter, in the sequence we can find the basic elements (strokes) that a character has and how these sequences are connected with each other. Moreover, **we can even create new characters with this sequences**. However, generating character in this way will encounter many problems. One major issue is that we could get many elements, some of which can only fit specific characters, or we will get unnatural characters, especially when generating Chinese characters which are usually composed of complex strokes. Another problem is that we can hardly determine the similarity between origin characters and generated characters. **Therefore, treating the characters as images is a more convenient choice in character generation**. Among the machine learning methods to perform image-related work such as object discrimination and images classification, convolutional neural net (CNN) is the commonly used one. CNN learns to minimize a loss function **which is used to rate the quality of results用于评估结果质量**. **The reason why** CNN excels this mission is that the CNN model usually has more adjustable parameters than other models, which also means that the CNN model can **fit适用于** the objective function which may be very complex. To train a CNN model, we need to tell the model which objective needs to be minimized; thus, this model is a supervised learning method. As reported in the literature, CNN model **is not only** good at handling discriminate problem like image classification, **it can also** work on generation problems **well**. **Thus, it can be naturally inferred that因此可以很自然的推断出** the loss definition problem can also use the CNN model**, which is the main idea of这是--的主要思想** generative adversarial network (GAN) [1]. GAN gives us a general and effective way to train generative models with neural networks. Basically, GAN has two models, a generator model and a discriminator model. The generator tries its best to generate images to let the discriminator hardly distinguish them from the ground truth images. In the meantime, the discriminator also tries its best to distinguish these two images. But since the inputs of GAN are random noise, for the traditional GAN model, the outputs are hardly controlled. Thus, researchers have tried to add conditions into the GAN model. This type of GAN models are named as conditional generative adversarial network (CGAN) [2]. CGAN is a kind of model that can adjust itself on external information. By giving condition in both generator and discriminator, the generator can know which images the user want to generate and at the same time the discriminator can not only know which images the generator generated but also how realistic these images are by comparing to ground truth images. Thus, we can train a generator which can generate different type of objects by giving multi-class conditional information and proper training parameters. The remaining of this article is structured as follows: In Sect. 2, different methods to generate characters and recent research of GAN and CGAN in the literature are introduced. We propose a novel method MSMC-CGAN (multi-scale multi-class generative adversarial network) to generate realistic and natural characters and describe our training method detail in Sect. 3. In Sect. 4, the evaluation method MOS is introduced and the experiment results are presented. We also compare the results of other GAN models by the MOS score. Finally in Sect. 5, some conclusion is drawn.

1导言呈现角色有两种不同的方式。一种是把它看成是像素的组合，另一种是把它看成是笔画的序列或组合。前者也称为角色的图像。至于后者，在序列中我们可以找到一个字符的基本元素（笔划）以及这些序列如何相互连接。此外，我们甚至可以用这个序列创建新角色。然而，以这种方式生成角色将遇到许多问题。一个主要问题是，我们可以得到许多元素，其中一些元素只能适合特定的字符，或者我们会得到不自然的字符，特别是在生成通常由复杂笔划组成的汉字时。另一个问题是，我们很难确定原始字符和生成字符之间的相似性。因此，将字符视为图像是字符生成中更方便的选择。在执行图像相关工作（如目标识别和图像分类）的机器学习方法中，卷积神经网络（CNN）是最常用的一种。CNN学习最小化用于评估结果质量的损失函数。CNN优于此任务的原因是CNN模型通常比其他模型具有更多可调参数，这也意味着CNN模型可以拟合可能非常复杂的目标函数。为了训练CNN模型，我们需要告诉模型需要最小化哪个目标；因此，该模型是一种监督学习方法。据文献报道，CNN模型不仅能很好地处理图像分类等判别问题，还能很好地处理生成问题。因此，可以很自然地推断，损失定义问题也可以使用CNN模型，这是生成性对抗网络（GAN）[1]的主要思想。GAN为我们提供了一种用神经网络训练生成模型的通用而有效的方法。基本上，GAN有两个模型，一个发生器模型和一个鉴别器模型。生成器尽最大努力生成图像，使鉴别器几乎无法将其与地面真实图像区分开来。同时，鉴别器也尽力区分这两幅图像。但是由于GAN的输入是随机噪声，对于传统的GAN模型，输出很难控制。因此，研究人员试图在GAN模型中添加条件。这种类型的GAN模型被称为条件生成对抗网络（CGAN）[2]。CGAN是一种能够根据外部信息进行自我调整的模型。通过在生成器和鉴别器中给定条件，生成器可以知道用户想要生成哪些图像，同时鉴别器不仅可以知道生成器生成了哪些图像，还可以通过与地面真实图像进行比较来知道这些图像的真实度。因此，我们可以通过提供多类条件信息和适当的训练参数来训练生成不同类型对象的生成器。本文的其余部分结构如下：在第节中。2、介绍了不同的字符生成方法以及文献中对GAN和CGAN的最新研究。我们提出了一种新的方法MSMC-CGAN（多尺度多类生成对抗网络）来生成真实自然的角色，并在第节详细描述了我们的训练方法。3.在门派中。4、介绍了MOS的评价方法，并给出了实验结果。我们还通过MOS评分比较了其他GAN模型的结果。最后在门派。5、得出了一些结论。

##### Learning to Write Stylized Chinese Characters by Reading a Handful of Examples

The field of character generation **remains relatively underexplored保持相对的探索不足** **compared with和---相比** the automatic recognition of characters [Pal and Chaudhuri, 2004; Zhong et al., 2016]. This unbalanced progress **is disadvantageous for不利于** the development of information processing of all languages, especially Chinese, **one of the most widely used languages that** has its characters **incorporated into融入** many other Asian languages, such as Japanese and Korean. Learning to write stylized Chinese characters by reading a handful of examples (as shown in Fig. 1) **is also a good testbed for** machine intelligence as the task can be finished effectively by humans while still remaining challenging for machines. **Though great efforts have been made on synthesizing虽然很多努力已经被作出在合成--领域** simple English or Latin characters [Graves, 2013; Lake et al., 2015], **only a little exploration has been done on Chinese character generation只有一小部分探索被做在中文生成领域**. **This is not surprising as这并不奇怪，因为---** Chinese characters **pose new challenges due to--有新的挑战由于--** their **complexity and diversity复杂性和多样性**. First, Chinese characters have a much larger dictionary **compared with** the alphabetic writings. For instance, **there are 27,533 unique Chinese characters in the official GB18030 charse官方--数据集字符有27533个字符**, **每天经常使用的多达with daily used ones up to** 3,000. Recently, Zhang et al. [2017] propose to generate Chinese characters **based on Recurrent Neural Networks** [Hochreiter and Schmidhuber, 1997], **which learns a low dimensional embedding as学习字符的一个低维嵌入** the character label to solve the issue of large dictionary. However, this method only generates characters without any style information, and heavily relies on the online temporal information of the strokes, which cannot generalize to the off-line applications. **Moreover, Chinese characters have more complex shapes and structures than other symbolic characters such as Latin alphabets此外，汉字有更加复杂的形状和结构比其他符号字符**. Some attempts have been made on Chinese character generation by assembling components of radicals and strokes [Xu et al., 2009; Zong and Zhu, 2014; Lian et al., 2016]. However, these models rely on the preceding parsing, which requires each character to have a clear structure. Therefore, they cannot deal with a joined-up writing style , in which strokes are connected and cursive. Finally, the generation of stylized Chinese characters is also challenged by the complexity in writing styles. The recent “Rewrite” [Tian, 2016] and its advanced version “zi2zi” [Tian, 2017] based on the “pix2pix” [Isola et al., 2016] implement font style transfer of Chinese characters by learning to map the source style to a target style with thousands of character pairs. However, these methods have to re-train a model from scratch when observing some Chinese characters with a new style, failing to re-utilize the knowledge learned before.

与字符的自动识别相比，字符生成领域的探索相对不足[Pal和Chaudhuri，2004；Zhong等人，2016]。这种不平衡的进展不利于所有语言的信息处理发展，特别是汉语，汉语是使用最广泛的语言之一，其字符已融入许多其他亚洲语言，如日语和朝鲜语。通过阅读一些示例（如图1所示）来学习书写风格化汉字也是机器智能的一个很好的测试平台，因为这项任务可以由人类有效地完成，同时仍然对机器具有挑战性。尽管在合成简单的英语或拉丁语字符方面做出了巨大的努力[Graves，2013；Lake等人，2015]，但在汉字生成方面只做了很少的探索。这并不奇怪，因为汉字由于其复杂性和多样性而带来了新的挑战。首先，与字母书写相比，汉字的字典要大得多。例如，官方GB18030字符集中有27533个独特的汉字，每天使用的汉字多达3000个。最近，Zhang等人[2017]提出基于递归神经网络生成汉字[Hochreiter and Schmidhuber，1997]，该网络学习低维嵌入作为字符标签，以解决大词典的问题。然而，该方法只生成没有任何样式信息的字符，并且严重依赖于笔画的在线时态信息，不能推广到离线应用。此外，汉字的形状和结构比其他符号字符（如拉丁字母）更为复杂。已经有人尝试通过组装部首和笔划组件来生成汉字[Xu等人，2009；Zong和Zhu，2014；Lian等人，2016]。然而，这些模型依赖于前面的解析，这要求每个字符都有一个清晰的结构。因此，他们无法处理一种连体书写风格，即笔画是连体的和草书的。最后，风格化汉字的产生也受到书写风格复杂性的挑战。最近的“重写”[Tian，2016]及其高级版本“zi2zi”[Tian，2017]基于“pix2pix”[Isola等人，2016]通过学习将源样式映射到具有数千个字符对的目标样式，实现了汉字的字体样式转换。然而，这些方法在观察一些新的汉字时，必须从头开始重新训练模型，无法重新利用以前学到的知识。



##### CalliGAN: Style and Structure-aware Chinese Calligraphy Character Generator

Chinese characters are logograms developed for the writing of Chinese. Unlike an alphabet, every Chinese character has its own meaning and an entire sound. Chinese characters were invented several thousand years ago, initially as scripts inscribed on animal bones or turtle plastrons. Around 300 BC, ink brushes were invented. During the Qin Dynasty (221 BC to 206 BC), Chinese characters were first standardized as the Qin script. Thereafter, they were developed into different forms in the long history such as the clerical, regular, semi-cursive, and cursive scripts. Along with its long history, Chinese calligraphy belongs to the quintessence of Chinese culture. While calligraphers use brushes to write characters, they also embody their artistic expressions in their creatures. Therefore, every brush-written character image is unique and irregular like a picture. In contract, fonts are created by companies and font-rendered images often contain common regions such as radicals. In addition, different fonts cover different numbers of characters. For example, the widely used Chinese font Sim Sun version 5.16 covers 28762 Unicode characters and its extension package version 0.90 covers 42809 rarely used Chinese characters1, but some fonts only cover limited numbers of characters. Brush-written characters, in particular masterpieces, have another problem that some characters become unclear or damaged if their papers and steles decay. The absence of many characters restrains calligraphy beginners from emulating masterpieces and designers from using masters’ works. Therefore, there is a need to generate character images like Figure 1 and many methods have been published to address this problem. Because Chinese characters are highly structured, some early developed methods use the split-and-merge approach to decompose a character into strokes, and then assemble each stroke’s synthesized calligraphy counterpart into a calligraphy character [31, 29]. However, the approach has a limitation that Chinese characters with complex structures are difficult to be decomposed automatically, and require manual decomposition for certain styles such as the cursive script [30]. With the advance of neural networks and computer vision, a study called style transfer, which aims to add painters’ artistic styles to photos captured by cameras, shows remarkable success [6, 13, 26, 5, 9]. Style transfer gradually evolves to image-to-image translation [10, 34, 32, 14, 17, 24, 3], which aims to not only add style details to target images but also convert objects from one domain to another, for example, horses to zebras, and vice versa. Because every Chinese calligrapher has his or her own style in terms to form strokes and shapes, generating calligraphy characters can be viewed as translating character images from one domain to another [2, 1, 28, 11, 12, 23, 33, 22]. A Chinese font can easily render numerous character images. Given two fonts, we can easily obtain numerous wellaligned character pairs. Therefore, it is a practical approach to generate characters by training an image-to-image translation model which take font-rendered character images as input, and this approach generates the state-of-the-art quality [2]. Compared with font-rendered character images, brush-written character images are more irregular. In addition, they take time and effort to create. To the best of our knowledge, there is no well-defined dataset of brush-written calligraphy character images available, and there is only one existing paper using brush-written calligraphy character images to conduct experiments [18]. This paper is the second to deal with this image type. Using brush-written images, we develop a method of multi-style image-to-image translation. We define styles basically as calligraphers’ identities. If a calligrapher has distinct styles at different periods of creation, we define multiple style labels for that calligrapher. To validate the developed method, we conduct head-to-head comparisons with an existing method. To sum up, this paper has two contributions: • While existing multi-font Chinese character generating methods are designed to generate highly different fonts, this paper is the first one dealing styles at the fine-grained level. In addition, this paper is the second paper reporting experimental results of brush-written calligraphy images. Our code and dataset are publicly available to help researchers reproduce our results. • The proposed method has a novel component encoder. To the best of our knowledge, the proposed method is the first to decompose Chinese characters into components and encoder them through a recurrent neural network. The proposed method generates promising images which lead to favorable numerical evaluations and subjective opinions.

汉字是为书写汉语而开发的符号。与字母表不同，每个汉字都有自己的意思和完整的发音。汉字是几千年前发明的，最初是刻在动物骨骼上的文字或甲鱼的橡皮筋。大约公元前300年，墨笔被发明。在秦朝（公元前221年至公元前206年），汉字首次被标准化为秦文字。此后，它们在漫长的历史中发展成不同的形式，如文书、常规、半草书和草书。中国书法历史悠久，是中国文化的精髓。书法家在用毛笔书写文字的同时，也将其艺术表现力体现在他们的作品中。因此，每一个毛笔书写的字符图像都是独一无二的，就像一幅图画一样不规则。在合同中，字体由公司创建，字体呈现的图像通常包含常见区域，如部首。此外，不同的字体包含不同数量的字符。例如，广泛使用的中文字体Sim Sun版本5.16包含28762个Unicode字符，其扩展包版本0.90包含42809个很少使用的中文字符1，但一些字体只包含有限数量的字符。毛笔书写的文字，特别是杰作，还有一个问题，即如果纸张和石碑腐烂，一些文字会变得不清晰或损坏。由于缺少许多汉字，书法初学者无法模仿大师作品，设计师也无法使用大师作品。因此，需要生成如图1所示的字符图像，并且已经发布了许多方法来解决这个问题。由于汉字是高度结构化的，一些早期开发的方法使用拆分合并方法将汉字分解为笔画，然后将每个笔画的合成书法对应物组合成书法字符[31,29]。然而，该方法有一个局限性，即具有复杂结构的汉字难以自动分解，并且对于某些样式（如草书）需要手动分解[30]。随着神经网络和计算机视觉的发展，一项名为“风格转移”的研究取得了显著的成功[6,13,26,5,9]，该研究旨在将画家的艺术风格添加到相机拍摄的照片中。样式转换逐渐演变为图像到图像的转换[10,34,32,14,17,24,3]，其目的不仅是向目标图像添加样式细节，还将对象从一个域转换为另一个域，例如，马转换为斑马，反之亦然。因为每个中国书法家在笔画和形状方面都有自己的风格，所以生成书法字符可以看作是将字符图像从一个领域转换到另一个领域[2,1,28,11,12,23,33,22]。中文字体可以很容易地呈现大量的字符图像。给定两种字体，我们可以轻松获得许多对齐良好的字符对。因此，通过训练以字体渲染的字符图像作为输入的图像到图像的转换模型来生成字符是一种实用的方法，并且这种方法生成最先进的质量[2]。与字体渲染的字符图像相比，毛笔书写的字符图像更不规则。此外，它们需要时间和精力来创建。据我们所知，目前还没有明确定义的毛笔书法字符图像数据集，只有一篇论文使用毛笔书法字符图像进行实验[18]。本文是第二篇处理这种图像类型的文章。使用毛笔书写的图像，我们开发了一种多风格图像到图像的翻译方法。我们基本上将风格定义为书法家的身份。如果一个书法家在不同的创作时期有不同的风格，我们将为该书法家定义多个风格标签。为了验证所开发的方法，我们与现有方法进行了正面比较。综上所述，本文有两个贡献：•虽然现有的多字体汉字生成方法旨在生成高度不同的字体，但本文是第一个在细粒度级别处理样式的方法。此外，本文是第二篇报道毛笔书法图像实验结果的论文。我们的代码和数据集可公开获取，以帮助研究人员重现我们的结果该方法具有一种新颖的分量编码器。**据我们所知，该方法是第一个将汉字分解成部件并通过递归神经网络对其进行编码的方法。**该方法生成的图像具有良好的数值评价和主观评价。

##### Multiple Heads are Better than One: Few-shot Font Generation with Multiple Localized Experts

Introduction A few-shot font generation task (FFG) [42, 54, 12, 41, 6, 7, 37] aims to generate a new font library using only a few reference glyphs, e.g., less than 10 glyph images, without additional model fine-tuning at the test time. FFG is especially a desirable task when designing a new font library for glyph-rich scripts, e.g., Chinese (> 50K glyphs), Korean (≈ 11K glyphs), or Thai (≈ 11K glyphs). It is because the traditional font design process is very labor-intensive due to the complex characteristics of the font domain. Another real-world scenario of FFG is to extend an existing font design to different language systems. For example, an international multi-media content, such as a video game or movie designed with a creative font, is required to re-design coherent style fonts for different languages. A high-quality font design is obliged to satisfy two objectives. First, the generated glyph should maintain all the detailed structure of the target character, particularly important for glyph-rich scripts with highly complex structure. For example, even very small damages on a local component of a Chinese glyph can hurt the meaning of the target character. As another objective, a generated glyph should have a diverse local style of the reference glyphs, e.g., serifness, strokes, thickness, or size. To achieve both objectives, existing methods formulate FFG by disentangling the content information and the style information from the given glyphs [42, 54, 12, 6, 37]. They combine the content features from the source glyph and the style features from the reference glyphs to generate a glyph with the reference style. Due to the complex nature of the font domain, the major challenge of FFG is to correctly disentangle the global content structure and the diverse local styles. However, as shown in our experiments, we observe that existing methods are insufficient to capture diverse local styles or to preserve the global structures of unseen language systems. We categorize existing FFG methods into universal style representation methods [42, 54, 34, 12] and componentconditioned methods [6, 37]. Universal style representation methods [42, 54, 34, 12] extract only a single style representation for each style – see Figure 2 (a). As glyph images are highly complex, these methods often fail to capture diverse local styles. To address the issue, component-conditioned methods [6, 37] utilize compositionality; a character can be decomposed into a number of sub-characters, or components – see Figure 2 (b). They explicitly extract componentconditioned features, beneficial to preserve the local component information. Despite their promising performances, their encoder is tightly coupled with specific component labels of the target language domain, which hinders processing the glyphs with unseen components or conducting a cross-lingual font generation. In this paper, we propose a novel few-shot font generation method, named Multiple Localized eXperts Few-shot Font Generation Network (MX-Font), which can capture multiple local styles, but not limited to a specific language system. MX-Font has a multi-headed encoder, named multiple localized experts. Each localized expert is specialized for different local sub-concepts from the given complex glyph image. Unlike component-conditioned methods, our experts are not explicitly mapped to a specific component, but each expert implicitly learns different local concepts by weak supervision i.e. component and style classifiers. To prevent that different experts learn the same local component, we formulate the component label allocation problem as a graph matching problem, optimally solved by the Hungarian algorithm [29] (Figure 4). We also employ the independence loss and the content-style adversarial loss to enforce the content-style disentanglement by each localized expert. Interestingly, with only weak component-wise supervision (i.e. image-level not pixel-level labels), we observe that each localized expert is specialized for different local areas, e.g., attending the left-side of the image (Figure 7). While we inherit the advantage of componentconditioned methods [6, 37] by introducing the multiple local features, our method is not limited to a specific language by removing the explicit component dependency in extracting features. Consequently, MX-Font outperforms the stateof-the-art FFG in two scenarios: In-domain transfer scenario, training on Chinese fonts and generating an unseen Chinese font, and zero-shot cross-lingual transfer scenario, training on Chinese fonts and generating a Korean font. Our ablation and model analysis support that the proposed modules and optimization objectives are important to capture multiple diverse local concepts.

简介少量字体生成任务（FFG）[42、54、12、41、6、7、37]旨在仅使用少量参考字形生成新的字体库，例如，少于10个字形图像，而无需在测试时进行额外的模型微调。当为字形丰富的脚本（例如，中文（>50K字形）、韩文和韩文）设计新的字体库时，FFG是一项特别理想的任务(≈ 11K字形），或泰语(≈ 11K字形）。这是因为，由于字体领域的复杂特性，传统的字体设计过程非常劳动密集。FFG的另一个真实场景是将现有字体设计扩展到不同的语言系统。例如，国际多媒体内容（如视频游戏或电影）需要使用创造性字体重新设计不同语言的连贯风格字体。高质量的字体设计必须满足两个目标。首先，生成的字形应该保持目标角色的所有详细结构，对于具有高度复杂结构的字形丰富的脚本尤其重要。例如，即使对汉字字形的局部部分造成很小的损害，也会损害目标字符的含义。作为另一个目标，生成的图示符应该具有参考图示符的不同局部样式，例如，衬度、笔划、厚度或大小。为了实现这两个目标，现有的方法通过将内容信息和样式信息从给定的字形中分离出来来制定FFG[42、54、12、6、37]。它们将源图示符中的内容特征与参考图示符中的样式特征相结合，以生成具有参考样式的图示符。由于字体领域的复杂性，FFG面临的主要挑战是正确区分全局内容结构和不同的本地样式。然而，正如我们的实验所显示的，我们观察到现有的方法不足以捕捉不同的本地风格或保存看不见的语言系统的全局结构。我们将现有的FFG方法分为通用风格表示方法[42,54,34,12]和组件条件表示方法[6,37]。通用样式表示方法[42,54,34,12]仅为每个样式提取一个样式表示–见图2（a）。由于字形图像非常复杂，这些方法通常无法捕获不同的本地样式。为了解决这个问题，组件条件化方法[6,37]利用了组合性；一个字符可以分解成若干子字符或组件——见图2（b）。它们显式地提取组件条件特征，有利于保留本地组件信息。尽管它们的性能很有前途，但它们的编码器与目标语言域的特定组件标签紧密耦合，这阻碍了处理包含未知组件的字形或进行跨语言字体生成。在本文中，我们提出了一种新的少镜头字体生成方法，称为多本地化专家少镜头字体生成网络（MX font），它可以捕获多种本地样式，但不限于特定的语言系统。MX字体有一个多头编码器，名为多个本地化专家。每个本地化专家都专门处理给定复杂轮廓图像中的不同局部子概念。与组件条件方法不同，我们的专家没有明确映射到特定组件，但每个专家通过弱监督（即组件和样式分类器）隐式学习不同的局部概念。为了防止不同的专家学习相同的局部组件，我们将组件标签分配问题描述为一个图匹配问题，通过匈牙利算法[29]进行优化解决（图4）。我们还利用独立性损失和内容风格对抗性损失来强制每个本地化专家进行内容风格分解。有趣的是，由于只有微弱的组件级监控（即图像级而非像素级标签），我们观察到每个本地化专家都专门负责不同的局部区域，例如，参与图像的左侧（图7）。虽然我们通过引入多个局部特征继承了ComponentConditional方法[6,37]的优点，但我们的方法在提取特征时去除了显式的组件依赖性，因此不局限于特定的语言。因此，MX字体在两种情况下优于最先进的FFG：域内转移情况，中文字体培训并生成一种看不见的中文字体，以及零镜头跨语言转移情况，中文字体培训并生成一种韩文字体。我们的烧蚀和模型分析支持，提出的模块和优化目标对于捕获多种多样的局部概念非常重要。

##### Adversarial Generation of Handwritten Text Images Conditioned on Sequences

I. INTRODUCTION Having computers able to recognize text from images is an old problem that has many practical applications, such as automatic content search on scanned documents. Transcribing printed text is now a reliable technology. However, automatically recognizing handwritten text is still a hard and open problem. Unlike printed text, cursive handwriting cannot be segmented into individual characters, since their boundaries are ill-defined. Graves et al. [1] introduced the Connectionist Temporal Classification (CTC), a loss enabling to train neural networks to recognize sequences without explicit segmentation. Today, in order to deal with such a complex problem, state-of-the-art solutions [2], [3] are all based on deep neural networks and the CTC loss. The supervised training of these neural networks requires large amounts of annotated data; in our case, images of handwritten text with corresponding transcripts. However, annotating images of text is a costly, time-consuming task. We therefore propose a system to reverse the annotation process: starting from a given word, we generate a corresponding image of cursive text. We first tackle the challenge of generating realistic data, and then address the question of using such synthetic data to train neural networks in order to improve the performance of handwritten text recognition. The problem of generating images of handwritten text has already been addressed in the past. Many techniques [4] are based on a collection of templates of a few characters, either human-written or built using Bezier curves. These templates are possibly perturbed and finally concatenated. However, this class of solutions, that simply concatenates character models, cannot faithfully reproduce the distribution of real-world images. It is also complex to have templates that are generic enough to result in truly cursive text. Alternatively, following the online approach, we can consider the handwriting as a trajectory, typically recorded with pen computing. In this setting, the model aims at producing a sequence of pen positions to generate a given word. Graves et al. [5] use a Long Short-Term Memory (LSTM) [6], [7] recurrent neural network to predict such a sequence, and let the network condition its prediction on the target string to synthesize handwriting. However, this method does not allow to deal with some features useful for offline recognition, such as background texture or line thickness variations. Generative Adversarial Networks (GAN) [8] offer a powerful framework for generative modeling. This architecture enables the generation of highly realistic and diverse images. The original GAN does not allow any control over the generated images, but many works [9]–[11] proposed a modified GAN for class-conditional image generation. However, we want to condition our generation on the sequence of characters to render, not on a single class. Closer to our goal, Reed et al. [12] conditions the generation on a textual description of the image to be produced. In addition to a random vector, their generator receives an embedding of the description text, and their discriminator is trained to classify as fake a real image with a non-matching description, to enforce the generator to produce description-matching images. To the best of our knowledge, there is only one work [13] on a GAN for text image synthesis. While our generation process is directly conditioned on a sequence of characters, this method follows a style transfer approach, resorting to a CycleGAN [14] to render images of isolated handwritten Chinese characters from a printed font. Data augmentation techniques based on distortion and additive noise do not allow to enlarge the textual contents of the training data. Moreover, having control of the generated text enables the creation of training material that covers even rare sequences of characters, which can be expected to improve the recognition performance. This, combined with the intrinsic diversity, provides a strong motivation to use a conditional GAN for the generation of cursive text images. In this paper, we make the following contributions: • We propose an adversarial architecture, schematically represented in Fig. 1, to generate realistic images of handwritten words. – We use bidirectional LSTM recurrent layers to encode the sequence of characters to be produced. – We introduce an auxiliary network for text recognition, in order to control the textual content of the generated images. • We obtain realistic images on both French and Arabic datasets. • Finally, we slightly improve text recognition performance on the RIMES dataset [15], using a neural network trained on a dataset extended with synthetic images.

I.导言让计算机能够从图像中识别文本是一个古老的问题，具有许多实际应用，例如扫描文档的自动内容搜索。抄写印刷文本现在是一项可靠的技术。然而，手写文本的自动识别仍然是一个难题。与打印文本不同，草书书写无法分割为单个字符，因为它们的边界定义不清。Graves等人[1]介绍了连接主义时间分类（CTC），这是一种能够训练神经网络识别序列而无需显式分割的损失。今天，为了处理如此复杂的问题，最先进的解决方案[2]，[3]都是基于深度神经网络和CTC损失。这些神经网络的监督训练需要大量带注释的数据；在我们的例子中，手写文本的图像与相应的转录本。然而，注释文本图像是一项成本高昂、耗时的任务。因此，我们提出了一个系统来反转注释过程：从给定的单词开始，我们生成相应的草书文本图像。我们首先解决生成真实数据的挑战，然后解决使用这些合成数据训练神经网络以提高手写文本识别性能的问题。生成手写文本图像的问题在过去已经得到解决。许多技术[4]都是基于一组由几个字符组成的模板，可以是人工编写的，也可以是使用贝塞尔曲线构建的。这些模板可能会受到干扰并最终连接起来。然而，这类仅仅连接角色模型的解决方案无法忠实地再现真实世界图像的分布。拥有足够通用的模板以生成真正的草书文本也很复杂。或者，在在线方法之后，我们可以考虑手写作为轨迹，通常用钢笔计算记录。在此设置中，模型的目标是生成一系列笔位置以生成给定单词。Graves等人[5]使用长短时记忆（LSTM）[6]，[7]递归神经网络来预测这样的序列，并让网络将其预测条件设定在目标字符串上，以合成笔迹。但是，该方法不允许处理一些对脱机识别有用的特征，例如背景纹理或线条厚度变化。生成性对抗网络（GAN）[8]为生成性建模提供了一个强大的框架。这种体系结构可以生成高度逼真和多样化的图像。原始的GAN不允许对生成的图像进行任何控制，但许多著作[9]–[11]提出了一种用于类条件图像生成的改进GAN。但是，我们希望根据要渲染的字符序列而不是单个类来设置生成条件。更接近我们的目标的是，Reed等人[12]根据要生成的图像的文本描述来设定生成条件。除了一个随机向量外，它们的生成器还接收描述文本的嵌入，并且它们的鉴别器被训练为将具有非匹配描述的真实图像分类为伪图像，以强制生成器生成描述匹配图像。据我们所知，关于用于文本图像合成的GAN，只有一项工作[13]。虽然我们的生成过程直接以字符序列为条件，但该方法遵循样式转换方法，借助CycleGAN[14]从打印字体渲染孤立手写汉字的图像。基于失真和加性噪声的数据增强技术不允许扩大训练数据的文本内容。此外，通过控制生成的文本，可以创建涵盖罕见字符序列的训练材料，这有望提高识别性能。这与固有的多样性相结合，为使用条件GAN生成草书文本图像提供了强大的动力。在本文中，我们做出了以下贡献：•我们提出了一种对抗性架构，如图1所示，用于生成手写单词的真实图像我们使用双向LSTM循环层对要生成的字符序列进行编码我们引入了一个用于文本识别的辅助网络，以控制生成图像的文本内容我们在法语和阿拉伯语数据集上都获得了逼真的图像最后，我们使用在合成图像扩展的数据集上训练的神经网络略微提高了RIMES数据集[15]的文本识别性能。

##### Multi-Content GAN for Few-Shot Font Style Transfer

Text is a prominent visual element of 2D design. Artists invest significant time into designing glyphs that are visually compatible with other elements in their shape and texture. This process is labor intensive and artists often design only the subset of glyphs that are necessary for a title or an annotation, which makes it difficult to alter the text after the design is created, or to transfer an observed instance of a font to your own project. In this work, we propose a neural network architecture that automatically synthesizes the missing glyphs from a few image examples. Early research on glyph synthesis focused on geometric modeling of outlines [29, 2, 27], which is limited to particular glyph topology (e.g., cannot be applied to decorative or hand-written glyphs) and cannot be used with image input. With the rise of deep neural networks, researchers have looked at modeling glyphs from images [1, 33, 22, 3]. We improve this approach by leveraging recent advances in conditional generative adversarial networks (cGANS) [11], which have been successful in many generative applications, but produce significant artifacts when directly used to generate fonts (Figure 6, 2nd row). Instead of training a single network for all possible typeface ornamentations, we show how to use our multi-content GAN architecture to retrain a customized network for each observed character set with only a handful of observed glyphs. Our network operates in two stages, first modeling the overall glyph shape and then synthesizing the final appearance with color and texture, enabling transfer of fine decorative elements. Some recent texture transfer techniques directly leverage glyph structure as guiding channels to improve the placement of decorative elements [34]. While this approach provides good results on clean glyphs it tends to fail on automatically-generated glyphs, as the artifacts of the synthesis procedure make it harder to obtain proper guidance from the glyph structure. Instead, we propose to train an ornamentation network jointly with the glyph generation network, enabling our ornament synthesis approach to learn how to decorate automatically generated glyphs with color and texture and also fix issues that arise during glyph generation. We demonstrate that users strongly preferred the output of our glyph ornamentation network in the end-to-end glyph synthesis pipeline. Our Contributions. In this paper, we propose the first end-to-end solution to synthesizing ornamented glyphs from images of a few example glyphs in the same style. To enable this, we develop a novel stacked cGAN architecture to predict the coarse glyph shapes, and a novel ornamentation network to predict color and texture of the final glyphs. These networks are trained jointly and specialized for each typeface using a very small number of observations, and we demonstrate the benefit of each component in our architecture (Figure 6). We use a perceptual evaluation to demonstrate the benefit of our jointly-trained network over effect transfer approaches augmented with a baseline glyph-outline inference network (Section 5.3). Our Multi-Content GAN (MC-GAN) code and dataset are available at https://github.com/azadis/ MC-GAN.

文本是2D设计中一个突出的视觉元素。艺术家投入大量时间来设计在形状和纹理上与其他元素视觉兼容的字形。这个过程是劳动密集型的，艺术家通常只设计标题或注释所需的字形子集，这使得在创建设计后很难修改文本，也很难将观察到的字体实例转移到您自己的项目中。在这项工作中，我们提出了一种神经网络架构，它可以自动从一些图像示例中合成缺失的字形。关于字形合成的早期研究集中于轮廓的几何建模[29,2,27]，其仅限于特定字形拓扑（例如，不能应用于装饰性或手写字形），并且不能用于图像输入。随着深层神经网络的兴起，研究人员已经开始研究从图像中建模字形[1,33,22,3]。我们通过利用条件生成对抗性网络（CGAN）[11]的最新进展来改进这种方法，该网络在许多生成性应用程序中都取得了成功，但直接用于生成字体时会产生显著的工件（图6，第二行）。我们展示了如何使用我们的多内容GAN体系结构为每个观察到的字符集重新训练一个定制的网络，而不是为所有可能的字体装饰训练一个单一的网络。我们的网络分两个阶段运行，首先对整个字形形状进行建模，然后用颜色和纹理合成最终外观，从而实现精细装饰元素的转移。最近的一些纹理转移技术直接利用字形结构作为引导通道来改进装饰元素的放置[34]。虽然这种方法在干净的图示符上提供了很好的结果，但在自动生成的图示符上它往往失败，因为合成过程的工件使得从图示符结构获得正确的指导变得更加困难。相反，我们建议与字形生成网络一起训练装饰网络，使我们的装饰合成方法能够学习如何用颜色和纹理装饰自动生成的字形，并解决字形生成过程中出现的问题。我们证明，用户强烈倾向于在端到端字形合成管道中输出我们的字形装饰网络。我们的贡献。在本文中，我们提出了第一个端到端的解决方案，从几个相同样式的示例字形图像合成装饰字形。为了实现这一点，我们开发了一种新的堆叠cGAN架构来预测粗略的轮廓形状，以及一种新的装饰网络来预测最终轮廓的颜色和纹理。这些网络是联合训练的，专门针对每种字体，使用非常少量的观察，我们展示了架构中每个组件的好处（图6）。我们使用感知评估来证明我们联合训练的网络优于使用基线轮廓推断网络（第5.3节）增强的效果传递方法。我们的多内容GAN（MC-GAN）代码和数据集可在https://github.com/azadis/ MC-GAN。

##### ScrabbleGAN: Semi-Supervised Varying Length Handwritten Text Generation

. Introduction Documentation of knowledge using handwriting is one of the biggest achievements of mankind: the oldest written records mark the transition from prehistory into history, and indeed, most evidence of historic events can be found in handwritten scripts and markings. Handwriting remained the dominant way of documenting events and data well after Gutenberg’s printing press in the mid-1400s. Both printing and handwriting are becoming somewhat obsolete in the digital era, when courtroom stenographers are being replaced by technology [4], further, most of the text we type remains in digital form and never meets a paper. Nevertheless, handwritten text still has many applications today, a huge of amount of handwritten text has accumulated over the years, ripe to be processed, and still continues to be written today. Two prominent cases where handwriting is still being used today are healthcare and financial institutions. There is a growing need for those to be extracted and made accessible, e.g. by modern search engines. While modern OCRs seem to be mature enough to handle printed text [18, 19], handwritten text recognition (HTR) does not seem to be on par. We attribute this gap to both the lack of versatile, annotated handwritten text, and the difficulty to obtain it. In this work, we attempt to address this gap by creating real-looking synthesized text, reducing the need for annotations and enriching the variety of training data in both style and lexicon. Our contributions are threefold; First, we present a novel fully convolutional handwritten text generation architecture, which allows for arbitrarily long outputs. This is in contrast to the vast majority of text related solutions which rely on recurrent neural networks (RNN). Our approach is able to generate arbitrarily long words (e.g., see Figure 1) or even complete sentences altogether. Another benefit of this architecture is that it learns character embeddings without the need for character level annotation. Our method’s name was chosen as an analogy between the generation process to the way words are created during the game of Scrabble, i.e. by concatenating some letter-tokens together into a word. Second, we show how to train this generator in a semisupervised regime, allowing adaptation to unlabeled data in general, and specifically to the test time images. To the best of our knowledge, this is the first use of unlabeled data to train a handwritten text synthesis framework. Finally, we provide empirical evidence that the training lexicon matters no less than the richness of styles for HTR training. This fact emphasizes the advantage of our method over ones that only warp and manipulate the training images.

. 使用手写记录知识是人类最大的成就之一：最古老的书面记录标志着从史前到历史的过渡，事实上，历史事件的大部分证据都可以在手写的文字和标记中找到。在14世纪中期古腾堡印刷机问世之后，手写仍然是记录事件和数据的主要方式。在数字时代，印刷和手写都变得有些过时，法庭速记员被技术所取代[4]，此外，我们键入的大多数文本仍然是数字形式，从未与纸张相遇。尽管如此，手写文本在今天仍然有很多应用，多年来积累了大量的手写文本，可以进行处理，直到今天仍在继续书写。今天仍在使用手写的两个突出案例是医疗和金融机构。人们越来越需要将这些信息提取出来并使其可访问，例如通过现代搜索引擎。虽然现代ORC似乎已经成熟到足以处理印刷文本（18, 19），但手写文本识别（HTR）似乎并不等同。我们将这一差距归因于缺乏多功能、带注释的手写文本，以及难以获取。在这项工作中，我们试图通过创建真实的合成文本、减少注释的需要以及丰富样式和词汇方面的各种训练数据来解决这一差距。我们的贡献是三倍的；首先，我们提出了一种新颖的全卷积手写文本生成体系结构，它允许任意长的输出。这与绝大多数依赖于递归神经网络（RNN）的文本相关解决方案形成对比。我们的方法能够生成任意长的单词（例如，见图1）甚至完整的句子。这种体系结构的另一个好处是，它学习字符嵌入，而不需要字符级注释。选择我们的方法名称是为了将生成过程与拼字游戏中创建单词的方式进行类比，即将一些字母标记连接在一起形成一个单词。其次，我们展示了如何在半监督的情况下训练这个生成器，使其能够适应一般的未标记数据，特别是测试时图像。据我们所知，这是第一次使用未标记数据来训练手写文本合成框架。最后，我们提供了经验证据，证明训练词汇对HTR训练的重要性不亚于丰富的风格。这一事实强调了我们的方法相对于那些只扭曲和操纵训练图像的方法的优势。

##### W-Net: One-Shot Arbitrary-Style Chinese Character Generation with Deep Neural Networks

1 Introduction Chinese is a special language with both messaging functions and artistic values. On the other hand, Chinese contains thousands of different categories or over 10,000 different characters among which 3,755 characters, defined as level-1 characters, are commonly used. Given a limited number of Chinese characters or even one single character with a specific style (e.g., a personalized hand-writing calligraphy or a stylistic printing font), it is interesting to mimic automatically many other characters with the same specific style. This topic is very difficult and rarely studied simply because of the large category number of different Chinese characters with various styles. 

This problem is even harder due to the **unique nature of Chinese characters among which each is a combination of various strokes and radicals with diverse interactive structures.** 

Despite these challenges, there are recently a few proposals relevant to the above-mentioned generation task. For example, in [13], strokes are represented by time-series writing evenly-thick trajectories. Then it is sent to the Recurrent Neural Network based generator. In [6], font feature reconstruction for standardized character extraction is achieved based on an additional network to assist the one-to-one image-to-image translation framework. Over 700 pre-selected training images are needed in this framework. In the Zi2Zi [12] model, a one-to-many mapping is achieved with only a single model by the fixed Gaussian-noise based categorical embedding with over 2,000 training examples per style. There are several main limitations in the above approaches. On one hand, the performance of these methods usually relies heavily on a large number of samples with a specific style. In the case of a few-shot or even one-shot generation, these methods would fail to work. On the other hand, these methods may not be able to transfer to a new style which has not been seen during training. Such drawbacks may hence present them from being used practically. In this paper, aiming to generate Chinese characters when even given one shot sample with a specific arbitrary style (seen or unseen in training), we propose a novel deep model named W-Net as a generalized style transformation framework. This framework better solves the above-mentioned drawbacks and could be easily used in practice. Particularly, inherent from the U-Net framework [9] for the one-to-one image-to-image translation task [4], the proposed W-Net employs two parallel convolution-based encoders to extract style and content information respectively. The generated image will be obtained by the deconvolution-based decoder by using the encoded information. Short-cut connections [9] and multiple residual blocks [2] are set to deal with the gradient vanishing problem and balance information from both encoders to the decoder. The training of the W-Net follows an adversarial manner. Inspired by the recently proposed Wasserstein Generative Adversarial Network (W-GAN) framework with gradient penalty [1], an independent discriminator1 (D) are employed to assist the W-Net (G) learning. As a methodological guidance, only one-shot arbitrary-style Chinese character generation is demonstrated in this paper, as examples given in Figure 1. However, the W-Net framework can be extended to a variety of related topics on one-shot arbitrary-style image generation. With such a proposal, the data synthesizing tasks with few samples available can be fulfilled much more readily and effectively than previous approaches in the literature

中文是一种既有信息功能又有艺术价值的特殊语言。另一方面，汉语包含数千个不同的类别或10000多个不同的字符，其中3755个字符是常用的一级字符。给定有限数量的汉字，甚至一个具有特定风格的单个汉字（例如，个性化手写书法或风格印刷字体），自动模拟具有相同特定风格的许多其他字符是很有趣的。由于汉字种类繁多，风格各异，这一课题难度很大，研究较少。这个问题更难解决，因为汉字的独特性质，其中每一个汉字都是各种笔画和部首的组合，具有不同的交互结构。尽管存在这些挑战，但最近仍有一些与上述发电任务相关的建议。例如，在[13]中，笔划由时间序列表示，书写的是均匀粗的轨迹。然后将其发送到基于递归神经网络的生成器。在[6]中，基于附加网络实现了用于标准化字符提取的字体特征重建，以辅助一对一图像到图像的翻译框架。在这个框架中，需要700多个预先选择的训练图像。在Zi2Zi[12]模型中，通过固定的基于高斯噪声的分类嵌入，仅用单个模型实现一对多映射，每个样式有2000多个训练示例。上述方法有几个主要局限性。一方面，这些方法的性能通常严重依赖于具有特定风格的大量样本。在几次甚至一次拍摄的情况下，这些方法将无法工作。另一方面，这些方法可能无法转换为训练期间从未见过的新风格。因此，这些缺点可能导致它们无法实际使用。在本文中，我们提出了一种新的深度模型W-Net，作为一个广义的风格转换框架，以期在给定具有特定任意风格（在训练中可见或不可见）的一次性样本时生成汉字。该框架较好地解决了上述缺点，易于在实践中使用。特别是，从用于一对一图像到图像翻译任务的U-Net框架[9]中固有的[4]，所提出的W-Net使用两个基于并行卷积的编码器分别提取样式和内容信息。生成的图像将由基于反褶积的解码器利用编码信息获得。设置了短连接[9]和多个剩余块[2]，以处理梯度消失问题，并平衡从两个编码器到解码器的信息。W-Net的训练采用对抗方式。受最近提出的带有梯度惩罚的Wasserstein生成性对抗网络（W-GAN）框架[1]的启发，采用了一个独立的鉴别器1（D）来辅助W-Net（G）学习。作为一种方法学指导，本文仅演示了一次性任意样式汉字生成，如图1所示。然而，W-Net框架可以扩展到关于一次性任意样式图像生成的各种相关主题。有了这样一种方案，可以比文献中以前的方法更容易、更有效地完成具有少量可用样本的数据合成任务

##### DCFont: An End-To-End Deep Chinese Font Generation System

INTRODUCTION Making a complete Chinese font library is a time-consuming task. Unlike the English font library that contains only 26 alphabets, the frequently used character set GB2312 is composed of 6763 Chinese characters. Furthermore, the complicated structure and diverse shape of Chinese characters markedly increase the difficulty. Majority of current commercial font generation procedures heavily rely on human design and adjustment, leading to low efficiency and high costs. Similarly, building a personalized Chinese handwriting font library is also a difficult mission for ordinary people since it is hard to write out such huge amounts of complicated characters correctly in a consistent handwriting style. Up to now, many attempts have been made to reduce manual work and increase the level of automation. One intuitive solution is to reuse the strokes or radicals contained in the characters designed/written by the user and then assemble them properly to generate other characters. However, human interventions are always required for this type of methods [Zhou et al. 2011; Zong and Zhu 2014] due to the fact that perfect automatic radical/stroke extraction is almost impossible in real applications. More recently, [Lian et al. 2016] solved this problem by proposing a style learning based synthesizing scheme in which human intervention is no longer required. In last few years, a large amount of research works [Gatys et al. 2015; Johnson et al. 2016] on image style transfer via deep neural networks have been reported, which aim to combine the content of one image with the style of another. With the advent of Generative Adversarial Networks (GANs) [Goodfellow et al. 2014], handwriting synthesis became plausible without exploring any domain knowledge of characters. Recently, [Isola et al. 2016] proposed a new general image-to-image translation framework, “pix2pix”, based on the U-net architecture and conditional GAN, which is well suited to solve the image mapping problems. Possibly inspired by the success of deep learning in generative tasks, several researchers have intended to synthesize Chinese handwritings by using deep neural networks. [Tian 2016] adopted a trickling down CNN structure “Rewrite” to generate Chinese characters. The method is able to produce standard printing fonts but performs badly for handwriting data. More recently, “zi2zi” [Tian 2017] (meaning characters to characters) was proposed based on the “pix2pix” framework [Isola et al. 2016] by adding the category embedding to the generator and discriminator, which results in good synthesizing performance in some specific font styles. [Lyu et al. 2017] regarded Chinese calligraphy synthesis as an image-toimage translation problem, which uses an auto-encoder network to supervise the generator. However, in addition to the poor-quality synthesis results for many characters, the training data size is 6000 in each style that is also too large to be applied in real applications. In this paper, we propose DCFont, an end-to-end deep Chinese font generation system. As shown in Figure 1, it is capable of generating the complete GB2312 font library with 6763 Chinese characters in a specific handwriting style through learning on a small subset consisting of 775 or even less characters. Different against other existing approaches, our system aims to generate high-quality Chinese handwriting fonts automatically without any human intervention and structure information of characters during both online and offline periods. Experiments show that high-quality synthesis results can be obtained by our system and the proposed method clearly outperforms other state-of-the-art approaches.

制作一个完整的中文字体库是一项耗时的任务。与仅包含26个字母的英文字体库不同，常用字符集GB2312由6763个汉字组成。此外，汉字复杂的结构和多样的形状明显增加了难度。当前大多数商业字体生成程序严重依赖于人工设计和调整，导致效率低下和成本高昂。同样，对于普通人来说，建立一个个性化的中文手写字体库也是一项艰巨的任务，因为很难以一致的手写风格正确地写出如此大量的复杂字符。到目前为止，已经作出了许多努力来减少手工工作和提高自动化水平。一个直观的解决方案是重用用户设计/书写的字符中包含的笔划或部首，然后将它们正确组合以生成其他字符。然而，这种方法总是需要人为干预[Zhou等人2011；Zong和Zhu 2014]，因为在实际应用中几乎不可能实现完美的自动根治/中风提取。最近，[Lian et al.2016]提出了一种基于风格学习的合成方案，不再需要人工干预，从而解决了这个问题。在过去几年中，已经报道了大量关于通过深度神经网络进行图像风格转换的研究工作[Gatys et al.2015；Johnson et al.2016]，其目的是将一幅图像的内容与另一幅图像的风格结合起来。随着生成性对抗网络（GANs）的出现【古德费罗等人，2014年】，手写合成在不探索任何字符领域知识的情况下变得可行。最近，[Isola et al.2016]提出了一个新的通用图像到图像转换框架“pix2pix”，该框架基于U-net架构和条件GAN，非常适合解决图像映射问题。可能是受到生成性任务中深度学习成功的启发，一些研究人员打算利用深度神经网络合成中文手写体。[Tian 2016]采用了一种点滴式CNN结构“重写”来生成汉字。该方法能够生成标准打印字体，但对手写数据的性能较差。最近，通过在生成器和鉴别器中添加类别嵌入，在“pix2pix”框架[Isola等人2016]的基础上提出了“zi2zi”[Tian 2017]（字符对字符的含义），从而在某些特定字体样式中获得了良好的合成性能。[Lyu et al.2017]将中国书法合成视为图像到图像的翻译问题，使用自动编码器网络来监控生成器。然而，除了许多字符的合成结果质量较差外，每种样式的训练数据量都是6000，这对于实际应用来说也太大了。本文提出了一种端到端的深汉字字体生成系统DCFont。如图1所示，通过对775个或更少字符组成的小子集的学习，它能够生成包含6763个特定手写风格汉字的完整GB2312字体库。与其他现有方法不同，我们的系统旨在在线和离线期间自动生成高质量的中文手写字体，而无需任何人为干预和字符结构信息。实验表明，我们的系统可以获得高质量的合成结果，并且所提出的方法明显优于其他最先进的方法。



##### DG-Font: Deformable Generative Networks for Unsupervised Font Generation

Introduction Every day, people consume a massive amount of texts for information transfer and storage. As the representation of texts, the font is closely related to our daily life. Font generation is critical in many applications, e.g., font library creation, personalized handwriting, historical handwriting imitation, and data augmentation for optical character recognition and handwriting identification. Traditional font library creating methods heavily rely on expert designers by drawing each glyph individually, which is especially expensive and labor-intensive for logographic languages such as Chinese (more than 60,000 characters), Japanese (more than 50,000 characters), and Korean (11,172 characters). Recently, the development of convolutional neural networks enables automatic font generation without human experts. There have been some attempts to explore font generation and achieve promising results. [49, 1, 18] utilize deep neural networks to generate entire sets of letters for certain alphabet languages. Two notable projects, “Rewrite" [40] and “zi2zi" [61], generate logographic language characters by learning a mapping from one style to another with thousands of paired characters. After that, EMD [58] and SAVAE [44] design neural networks to separate the content and style representation, which can extend to generate character of new styles or contents. However, these methods are in supervised learning and required a large amount of paired training samples. Some other methods exploit auxiliary annotations (e.g., strokes, radicals) to facilitate high-quality font generation. For example, [30] utilizes labels for each stroke to generate glyphs by writing trajectories synthesis. [26] employ the radical decomposition (e.g., radicals or sub-glyphs) of characters to achieve font generation for certain logographic language. DM-Font [7] and its improved version LF-Font [39] propose disentanglement strategies to disentangle complex glyph structures, which help capture local details in rich text design. However, these methods rely on prior knowledge and can only apply to specific writing systems. Some labels such as the stroke skeleton can be estimated by algorithms, but the estimation error would decrease the generated quality. Also, these methods still require thousands of paired data and annotated labels for training. Recently, there are some attempts [19, 9] for unsupervised font generation. [9] introduces a novel module that transfers the features across sequential DenseNet blocks [23]. [19] proposes a fast skeleton extraction method to obtain the skeleton of characters, and then utilize the extracted skeleton to facilitate font generation. For the problem of image-to-image translation, a series of works in unsupervised learning have been proposed by combining adversarial training [32, 54] with consistent constraints [59, 47, 3]. FUNIT [33] maps an image of a source class to an analogous image of a target class by leveraging a few target class images. They extract the style feature of the target class images and employ adaptive instance normalization (AdaIN) [25] to combine the content and the style features. However, these image-to-image translation methods cannot be directly applied to font generation tasks. Although consistent constraints preserve the structure of a content image, they still encounter some problems for font generation (e.g., blurry, missing some strokes). Also, they usually define the style as the set of textures and colors. The AdaIN-based methods transfer style by aligning feature statics, which tends to transform texture and color, which is not suitable to transform local style patterns (e.g., geometric deformation) for the font. Moreover, [9, 19] achieve unsupervised font generation by learning a mapping between two fonts directly, they also ignore the geometric deformation for the font. To learn the mapping across geometry variations, [20] introduces a discriminator with dilated convolutions as well as a multi-scale perceptual loss that is able to represent error in the underlying shape of objects. [52] disentangles image space into a Cartesian product of the appearance and the geometry latent spaces. Compelled by the above observations, we propose a novel deformable generative model for unsupervised font generation (DG-Font). The proposed method is designed to deform and transform the character of one font to another by leveraging the provided images of the target font. The proposed DG-Font separates style and content respectively and then mix two domain representations to generate target characters. We introduce a feature deformation skip connection (FDSC) which predicts pairs of displacement maps and employs the predicted maps to apply deformable convolution to the low-level feature maps from the content encoder. The outputs of FDSC are fed into a mixer to generate the final results. To distinguish different styles, we train our model with a multi-task discriminator, which ensures that each style can be discriminated independently. In addition, another two reconstruction losses are adopted to constrain the domain-invariant characteristics between generated images and content images. The feature deformation skip connection (FDSC) module is used to transform the low-level feature of content images, which preserves the pattern of character (e.g., strokes and radicals). Different from the image-to-image translation problem that defines style as a set of texture and color, the style of font is basically defined as geometric transformation, stroke thickness, tips, and joined-up writing pattern. For two fonts with the same content, they usually have correspondence for each stroke. Taking advantage of the spatial relationship of fonts, the feature deformation skip connection (FDSC) is used to conduct spatial deformation, which effectively ensures the generated image to have complete structures. Extensive experiments demonstrate that our model achieves comparable results to the state-of-the-art font generation methods. Besides, results show that our model is able to extend to generate unseen style character

导言每天，人们都要消耗大量的文本来传输和存储信息。字体作为文本的表现形式，与我们的日常生活息息相关。字体生成在许多应用中至关重要，例如字体库创建、个性化手写、历史手写模拟以及光学字符识别和手写识别的数据扩充。传统的字体库创建方法严重依赖于专家设计师单独绘制每个字形，这对于中文（超过60000个字符）、日文（超过50000个字符）和韩文（11172个字符）等标志语言来说尤其昂贵且耗费大量人力。最近，卷积神经网络的发展使得无需人工专家就能自动生成字体。已经有人尝试探索字体生成，并取得了有希望的结果。[49,1,18]利用深度神经网络为某些字母表语言生成整套字母。两个著名的项目“Rewrite”[40]和“zi2zi”[61]通过学习数千个成对字符从一种样式到另一种样式的映射来生成标识语言字符。之后，EMD[58]和SAVAE[44]设计神经网络以分离内容和样式表示，可以扩展以生成新样式或内容的字符。但是，这些方法是在监督学习中，需要大量成对的训练样本。其他一些方法利用辅助注释（例如笔划、部首）促进高质量字体生成。例如，[30]利用每个笔划的标签，通过书写轨迹合成生成字形。[26]利用字符的部首分解（例如部首或子字形）来实现特定标识语言的字体生成。DM font[7]及其改进版本LF font[39]提出解纠缠策略来解纠缠复杂的字形结构，这有助于在富文本设计中捕获局部细节。然而，这些方法依赖于先验知识，只能应用于特定的书写系统。一些标签（如笔划骨架）可以通过算法进行估计，但估计误差会减少生成的时间ed质量。此外，这些方法仍然需要数千对数据和带注释的标签进行训练。最近，有一些尝试[19,9]用于无监督字体生成。[9]引入了一个新模块，该模块跨顺序DenseNet块传输特征[23]。[19]提出了一种快速骨架提取方法，以获取字符的骨架，然后利用提取的骨架方便字体生成。针对图像到图像的翻译问题，通过将对抗训练[32,54]与一致约束[59,47,3]相结合，提出了一系列无监督学习的工作.FUNIT[33]利用几个目标类图像将源类图像映射到目标类的类似图像。它们提取目标类图像的样式特征，并采用自适应实例规范化（AdaIN）[25]组合内容和样式功能。但是，这些图像到图像的转换方法不能直接应用于字体生成任务。尽管一致的约束保留了内容图像的结构，但它们在字体生成时仍会遇到一些问题（例如，模糊、缺少某些笔划）。此外，他们通常将样式定义为纹理和颜色的集合。基于AdaIN的方法通过对齐特征静态来传递样式，该方法倾向于变换纹理和颜色，这不适合变换字体的局部样式模式（例如，几何变形）。此外，[9,19]通过直接学习两种字体之间的映射来实现无监督的字体生成，它们也会忽略字体的几何变形。要学习跨几何变化的映射，[20]介绍了一种具有扩展卷积和多尺度感知损失的鉴别器，该鉴别器能够表示物体基本形状中的错误。[52]将图像空间分解为外观和几何潜在空间的笛卡尔积。基于上述观察结果，我们提出了一种新的无监督字体生成的可变形生成模型（DG font）。建议的方法旨在通过利用提供的目标字体图像，将一种字体的字符变形并转换为另一种。建议的DG字体分别分离样式和内容，然后混合两种域表示来生成目标字符。我们引入了特征变形跳过连接（FDSC）它预测成对的位移贴图，并使用预测的贴图对来自内容编码器的低级特征贴图应用可变形卷积。FDSC的输出被送入混合器以生成最终结果。为了区分不同的样式，我们使用多任务鉴别器训练我们的模型，以确保每个样式另外，采用另外两种重建损失来约束生成图像和内容图像之间的域不变特征，利用特征变形跳连接（FDSC）模块对内容图像的底层特征进行变换，保留了字符的模式（例如笔划和部首）。与将样式定义为纹理和颜色集的图像到图像的翻译问题不同，字体样式基本上定义为几何变换、笔划厚度、笔尖和连接的书写模式。对于具有相同内容的两种字体，它们通常每个笔划都对应。利用空间r字体的关系，特征变形跳过连接（FDSC）用于进行空间变形，有效地确保生成的图像具有完整的结构。大量的实验表明，我们的模型达到了与最先进的字体生成方法相当的结果。此外，结果表明，我们的模型能够扩展以生成看不见的样式字符

##### Drawing and Recognizing Chinese Characters with Recurrent Neural Network

I. INTRODUCTION Reading and writing are among the most important and fundamental skills of human beings. Automatic recognition (or reading) of handwritten characters has been studied for a long time [1] and obtained great achievements during the past decades [2], [3]. However, the automatic drawing (or writing) of characters has not been studied as much, until the recent advances based on recurrent neural network for generating sequences [4]. In the development of human intelligence, the skills of reading and writing are mutual complementary. Therefore, for the purpose of machine intelligence, it would be interesting to handle them in unified framework. Chinese characters constitute the oldest continuously used system of writing in the world. Moreover, Chinese characters have been widely used (modified or extended) in many Asian countries such as China, Japan, Korea, and so on. There are more than tens of thousands of different Chinese characters. Most of them can be well recognized by most people, however, nowadays, it is becoming more and more difficult for people to write them correctly, due to the overuse of keyboard or touchscreen based input methods. Compared with reading, writing of Chinese characters is gradually becoming a forgotten or missing skill. For the task of automatic recognition of handwritten Chinese characters, there are two main categories of approaches: online and offline methods. With the success of deep learning [5], [6], the convolutional neural network (CNN) [7] has been widely applied for handwriting recognition. The strong priori knowledge of convolution makes the CNN a powerful tool for image classification. Since the offline characters are naturally represented as scanned images, it is natural and works well to apply CNNs to the task of offline recognition [8], [9], [10], [11]. However, in order to apply CNNs to online characters, the online handwriting trajectory should firstly be transformed to some image-like representations, such as the AMAP [12], the path signature maps [13] or the directional feature maps [14]. During the data acquisition of online handwriting, the pentip movements (xy-coordinates) and pen states (down or up) are automatically stored as (variable-length) sequential data. Instead of transforming them into image-like representations, we choose to deal with the raw sequential data in order to exploit the richer information it carries. In this paper, different from the traditional approaches based on CNNs, we propose to use recurrent neural networks (RNN) combined with bidirectional long short term memory (LSTM) [15], [16] and gated recurrent unit (GRU) [17] for online handwritten Chinese character recognition. RNN is shown to be very effective for English handwriting recognition [18]. For Chinese character recognition, compared with the CNN-based approaches, our method is fully end-to-end and does not require any domainspecific knowledge. State-of-the-art performance has been achieved by our method on the ICDAR-2013 competition database [19]. To the best of our knowledge, this is the first work on using RNNs for end-to-end online handwritten Chinese character recognition. Besides the recognition (reading) task, this paper also considers the automatic drawing of Chinese characters (writing task). Under the recurrent neural network framework, a conditional generative model is used to model the distribution of Chinese handwriting, allowing the model to generate new handwritten characters by sampling from the probability distribution associated with the RNN. The study of generative models is an important and active research topic in the deep learning field [6]. Many useful generative models have been proposed such as NADE [20], variational auto-encoder [21], DRAW [22], and so on. To better model the generating process, the generative adversarial network (GAN) [23] simultaneously train a generator to capture the data distribution and a discriminator to distinguish real and generated samples in a min-max optimization framework. Under this framework, high-quality images can be generated with the LAPGAN [24] and DCGAN [25] models, which are extensions of the original GAN. Recently, it was shown by [26] that realistically-looking Chinese characters can be generated with DCGAN. However, the generated characters are offline images which ignore the handwriting dynamics (temporal order and trajectory). To automatically generate the online (dynamic) handwriting trajectory, the recurrent neural network (RNN) with LSTM was shown to be very effective for English online handwriting generation [4]. The contribution of this paper is to study how to extend and adapt this technique for Chinese character generation, considering the difference between English and Chinese handwriting habits and the large number of categories for Chinese characters. As shown by [27], fake and regularwritten Chinese characters can be generated under the LSTMRNN framework. However, a more interesting and challenging problem is the generating of real (readable) and cursive handwritten Chinese characters. To reach this goal, we propose a conditional RNN-based generative model (equipped with GRUs or LSTMs) to automatically draw human-readable cursive Chinese characters. The character embedding is jointly trained with the generative model. Therefore, given a character class, different samples (belonging to the given class but with different writing styles) can be automatically generated by the RNN model conditioned on the embedding. In this paper, the tasks of automatically drawing and recognizing Chinese characters are completed both with RNNs, seen as either generative or discriminative models. Therefore, to verify the quality of the generated characters, we can feed them into the pre-trained discriminative RNN model to see whether they can be correctly classified or not. It is found that most of the generated characters can be automatically recognized with high accuracy. This verifies the effectiveness of the proposed method in generating real and cursive Chinese characters. The rest of this paper is organized as follows. Section II introduces the representation of online handwritten Chinese characters. Section III describes the discriminative RNN model for end-to-end recognition of handwritten Chinese characters Section IV reports the experimental results on the ICDAR2013 competition database. Section V details the generative RNN model for drawing recognizable Chinese characters. Section VI shows the examples and analyses of the generated characters. At last, Section VII draws the concluding remarks.

引言阅读和写作是人类最重要和最基本的技能之一。手写字符的自动识别（或读取）已经研究了很长时间[1]，并在过去的几十年中取得了巨大的成就[2]，[3]。然而，字符的自动绘制（或书写）还没有得到足够的研究，直到最近基于递归神经网络生成序列的进展[4]。在人类智力的发展过程中，阅读和写作技能是相辅相成的。因此，为了机器智能的目的，在统一的框架中处理它们是很有趣的。汉字是世界上最古老的持续使用的书写系统。此外，汉字在中国、日本、韩国等许多亚洲国家被广泛使用（修改或扩展）。有成千上万个不同的汉字。大多数人都能很好地识别它们，然而，如今，由于键盘或触摸屏输入法的过度使用，人们越来越难以正确书写它们。与阅读相比，汉字书写正逐渐成为一种被遗忘或缺失的技能。对于手写体汉字的自动识别，主要有两类方法：在线和离线方法。随着深度学习[5]、[6]的成功，卷积神经网络（CNN）[7]已广泛应用于手写识别。强大的卷积先验知识使CNN成为图像分类的有力工具。由于脱机字符自然地表示为扫描图像，因此将CNN应用于脱机识别任务是自然的，并且效果良好[8]、[9]、[10]、[11]。然而，为了将CNN应用于在线字符，应首先将在线手写轨迹转换为一些类似图像的表示，如AMAP[12]、路径签名映射[13]或方向特征映射[14]。在线手写数据采集期间，奔腾运动（xy坐标）和笔状态（向下或向上）自动存储为（可变长度）顺序数据。我们选择处理原始序列数据，而不是将它们转换为类似图像的表示形式，以便利用它所携带的丰富信息。在本文中，与传统的基于CNN的方法不同，我们提出将递归神经网络（RNN）与双向长短时记忆（LSTM）[15]、[16]和选通递归单元（GRU）[17]相结合，用于在线手写汉字识别。RNN被证明对英语手写识别非常有效[18]。对于汉字识别，与基于CNN的方法相比，我们的方法完全是端到端的，不需要任何特定领域的知识。通过我们在ICDAR-2013竞赛数据库上的方法，达到了最先进的性能[19]。据我们所知，这是第一个使用RNN进行端到端联机手写汉字识别的工作。除了识别（阅读）任务外，本文还考虑了汉字的自动绘制（书写任务）。在递归神经网络框架下，使用条件生成模型对中文手写体的分布进行建模，允许该模型通过从与RNN相关的概率分布中采样来生成新的手写体字符。生成模型的研究是深度学习领域一个重要而活跃的研究课题[6]。已经提出了许多有用的生成模型，如NADE[20]、变分自动编码器[21]、DRAW[22]等。为了更好地模拟生成过程，生成性对抗网络（GAN）[23]同时训练生成器捕获数据分布，并训练鉴别器在最小-最大优化框架中区分真实样本和生成样本。在这个框架下，可以使用拉普根[24]和DCGAN[25]模型生成高质量图像，这是原始GAN的扩展。最近，文献[26]表明，使用DCGAN可以生成逼真的汉字。然而，生成的字符是脱机图像，忽略了手写动态（时间顺序和轨迹）。为了自动生成在线（动态）手写轨迹，使用LSTM的递归神经网络（RNN）对于英语在线手写生成非常有效[4]。本文的贡献在于，考虑到英汉书写习惯的差异以及汉字的种类繁多，研究如何在汉字生成中扩展和适应这种技术。如[27]所示，可以在LSTMRNN框架下生成伪汉字和正规汉字。然而，一个更有趣和更具挑战性的问题是生成真实（可读）和草书手写汉字。为了达到这个目标，我们提出了一个基于条件RNN的生成模型（配备GRU或LSTM）来自动绘制人类可读的草书汉字。字符嵌入与生成模型联合训练。因此，给定一个字符类，RNN模型可以根据嵌入情况自动生成不同的样本（属于给定类但具有不同的书写风格）。在本文中，自动绘制和识别汉字的任务都是通过RNN来完成的，RNN被视为生成模型或判别模型。因此，为了验证生成的字符的质量，我们可以将它们输入预训练的判别RNN模型中，以查看它们是否能够正确分类。研究发现，大部分生成的字符都能以较高的精度自动识别。这验证了该方法在生成真实汉字和草书汉字方面的有效性。本文的其余部分组织如下。第二节介绍了联机手写汉字的表示。第三节介绍了手写体汉字端到端识别的判别式RNN模型。第四节报告了ICDAR2013竞赛数据库的实验结果。第五节详细介绍了可识别汉字的生成RNN模型。第六节展示了生成字符的示例和分析。最后，第七节得出结论。



##### Typography with Decor: Intelligent Text Style Transfer

 Introduction Artistic text, or styled text, is a kind of art wildly used in design and media. As shown in Fig. 1, with text effects such as color, texture, shading, and extra decorative elements, artistic text becomes more visually pleasing and can vividly convey more semantic information. Traditionally, it needs complex manual operations to migrate text effects to other raw text, which is time-consuming especially when a bunch of text is to be processed. In this work, we propose a novel framework for transferring given text effects to arbitrary glyphs. Text style transfer is a sub-topic of image style transfer. Although the task of image style transfer [8, 4, 12, 28, 6] have been wildly studied for years, text style transfer was not explored until recently. Yang et al.[23, 24] first explored this problem and designed a patch-based text effect transfer model. Due to the neglect of many important attributes such as decorative elements, directions, regular structures, etc., it fails on many kinds of text styles. On the other hand, Azadi et al.[1] proposed a deep-based model, which is able to stylize capital English alphabets given a few shots. However, it can only generate images with a limited resolution of 64 ⇥ 64 and is hard to be applied to texts other than the 26 alphabets. Moreover, all these methods have assumed that the styles are uniform within or outside the text. Thus, exquisite decorative elements, which are commonly used in artistic text design, are ignored. These decorations are usually drastically different from the basal text effects and can make the text more visually impressive and more information expressed. Treating decorative elements and basal text effects as a whole style will seriously degrade the visual quality of the stylization results, as shown in Fig. 1. To address this problem, in this paper, we propose a novel framework for text style transfer and pay special attention to decorative elements. The key idea is to detect, separate and recombine these important embellishments. First, we train a segmentation network to detect the decorative elements in the styled text. For training our segmentation network, we use synthetic data and further propose a domain-adaptation scheme so that the framework works well on real data. Then, based on the segmentation results, we are able to separate the decorative elements from basal text effects and design a text style transfer network to infer the basal text effects for the target text. To adapt our network to arbitrary text effects, a novel one-shot fine-tuning scheme is proposed, which empowers our network to extend to a new style with only one example required. Finally, cues for spatial distributions and element diversities are carefully characterized to jointly determine the layout of the decorative elements, which are then adaptively integrated onto the target text. Furthermore, to train the above models, we build a new dataset containing 59k professionally-designed styled texts with various text effects and fonts, and collect four thousand decorative elements and one thousand in-thewild artistic texts from the web. In summary, the contributions of this work are threefold: • We define a new problem of text style transfer with decorative elements, and propose a novel framework to solve the problem. The scheme of separation and recombination of the basal text effects and the decorative elements empowers our method to adapt to different styles and glyphs. • We train networks for effective decor detection and text effects transfer. Two corresponding novel training strategies are proposed to make the networks robust to arbitrary text styles. We propose a structureaware decor recomposition method to determine the decor layout, which produces professional artistic typography. • We introduce a new dataset containing thousands of styled text and decorative elements to support the training of our model

1.引言艺术文本，或称风格文本，是一种广泛应用于设计和媒体的艺术。如图1所示，通过颜色、纹理、阴影和额外的装饰元素等文本效果，艺术文本变得更加视觉愉悦，能够生动地传达更多的语义信息。传统上，将文本效果迁移到其他原始文本需要复杂的手动操作，这非常耗时，尤其是在处理一组文本时。在这项工作中，我们提出了一个新的框架，用于将给定的文本效果转换为任意字形。文本样式转换是图像样式转换的一个子主题。尽管图像风格转移的任务[8,4,12,28,6]已经被广泛研究了多年，但直到最近文本风格转移才被探索。Yang等人[23,24]首先探索了这个问题，并设计了一个基于补丁的文本效果迁移模型。由于忽略了许多重要属性，如装饰元素、方向、规则结构等，它在许多文本样式上都失败了。另一方面，Azadi等人[1]提出了一种基于深度的模型，该模型能够在几张照片的情况下将大写英文字母样式化。但是，它只能生成分辨率为64的有限图像⇥ 64，很难应用于26个字母以外的文本。此外，所有这些方法都假定样式在文本内部或外部是一致的。因此，艺术文本设计中常用的精美装饰元素被忽略了。这些装饰通常与基本文本效果截然不同，可以使文本在视觉上更令人印象深刻，表达更多信息。如图1所示，将装饰元素和基本文本效果视为一个整体样式将严重降低样式化结果的视觉质量。为了解决这个问题，本文提出了一个新的文本风格转换框架，并特别关注装饰元素。关键思想是检测、分离和重新组合这些重要的装饰。首先，我们训练一个分割网络来检测样式文本中的装饰元素。为了训练我们的分割网络，我们使用了合成数据，并进一步提出了一种域自适应方案，使得该框架能够很好地处理真实数据。然后，基于分割结果，我们可以将装饰元素从基本文本效果中分离出来，并设计一个文本风格转换网络来推断目标文本的基本文本效果。为了使我们的网络适应任意文本效果，提出了一种新的一次性微调方案，该方案使我们的网络能够扩展到一种新的风格，只需要一个示例。最后，对空间分布和元素多样性的线索进行仔细描述，共同确定装饰元素的布局，然后将其自适应地整合到目标文本中。此外，为了训练上述模型，我们构建了一个新的数据集，包含59k个专业设计的风格文本，具有各种文本效果和字体，并从web上收集了4000个装饰元素和1000个野生艺术文本。综上所述，这项工作的贡献有三个方面：•我们定义了一个新的文本风格转移问题，并提出了一个新的框架来解决这个问题。基本文本效果和装饰元素的分离和重组方案使我们的方法能够适应不同的风格和字形我们训练网络进行有效的花色检测和文本效果传输。提出了两种相应的训练策略，使网络对任意文本样式具有鲁棒性。我们提出了一种结构感知的装饰重新组合方法来确定装饰布局，从而产生专业的艺术排版我们引入了一个包含数千个样式文本和装饰元素的新数据集，以支持模型的训练

##### Handwritten Chinese Font Generation with Collaborative Stroke Refinement

Introduction Chinese language consists of more than 8000 characters, among which about 3700 are frequently used. Designing a new Chinese font involves considerable tedious manual efforts which are often prohibitive. A desirable solution is to automatically complete the rest of vocabulary given an initial set of manually designed samples, i.e. Chinese font synthesis. Inspired by the recent progress of neural style transfer [6, 9, 10, 18, 21, 26], several attempts have been recently made to model font synthesis as an image-to-image translation problem [7, 19, 24]. Unlike neural style transfer, font transfer is a low fault-tolerant task because any misplacement of strokes may change the semantics of the characters [7]. So far, two main streams of approaches have been explored. One addresses the problem with a bottleneck CNN structure [1, 2, 7, 15, 12] and the other proposes to disentangle representation of characters into content and style [19, 24]. While promising results have been shown in font synthesis, most existing methods require an impracticable large-size initial set to train a model. For example, [1, 2, 7, 15] require 3000 paired characters for training supervision, which requires huge labor resources. Several recent methods [12, 19, 24] explore learning with less paired characters; however, these methods heavily depend on extra labels or other pre-trained networks. Table 1 provides a comparative summary of the above methods. Most of the above methods focus on printed font synthesis. Compared with printed typeface synthesis, handwritten font synthesis is a much more challenging task. First, its strokes are thinner especially in the joints of strokes. Handwriting fonts are also associated with irregular structures and are hard to be modeled. So far there is no satisfying solution for handwritten font synthesis. This paper aims to synthesize more realistic handwritten fonts with fewer training samples. There are two main challenges: (i) handwritten characters are usually associated with thin strokes of few information and complex structure which are error prone during deformation; (see Figure 1) and (ii) thousands of characters with various shapes are needed to synthesize based on a few manually designed characters (see Figure 2). To solve the first issue, we propose collaborative stroke refinement that using collaborative training strategy to recover the missing or broken strokes. Collaborative stroke refinement uses an auxiliary branch to generate the syntheses with various stroke weights, guiding the dominating branch to capture characters with various stroke weights. To solve the second issue, we fully exploit the content-reuse phenomenon in Chinese characters; that is, the same radicals may present in various characters at various locations. Based on this observation, we propose online zoom-augmentation, which synthesizes a complicated character by a series of elementary components. The proposed networks can focus on learning varieties of locations and ratios of those elementary components. This significantly reduces the size of training samples. To make the proposed networks easy to capture the deformation from source to target, we further propose adaptive pre-deformation, which learns the size and scale deformation to standardize characters. This allows the proposed networks focus on learning high-level style deformation. Combining the above methods, we proposed an endto-end model (as shown in Fig.3), generating high-quality characters with only 750 training samples. No pre-trained network, or labels is needed. We verify the performance of the proposed model on several Chinese fonts including both handwritten and printed fonts. The results demonstrate the significant superiority of our proposed method over the state-of-the-art Chinese font generation models. The main contributions are summarized as follows. • We propose an end-to-end model to synthesize handwritten Chinese font given only 750 training samples; • We propose collaborative stroke refinement, handling the thin issue; online zoom-augmentation, enabling learning with fewer training samples; and adaptive predeformation, standardizing and aligning the characters; • Experiments show that the proposed networks are far superior to baselines in respect of visual effects, RMSE metric and user study.

中文由8000多个字符组成，其中约3700个是常用的。设计一种新的中文字体需要大量繁琐的手工操作，这通常是令人望而却步的。一个理想的解决方案是，给定一组手动设计的初始样本，即中文字体合成，自动完成剩余的词汇表。受神经风格转换的最新进展[6,9,10,18,21,26]的启发，最近有人尝试将字体合成建模为图像到图像的翻译问题[7,19,24]。与神经风格转换不同，字体转换是一项容错性较低的任务，因为笔划的任何错位都可能改变字符的语义[7]。到目前为止，已经探索了两种主要的方法。一个解决了瓶颈CNN结构的问题[1,2,7,15,12]，另一个建议将字符表示分离为内容和样式[19,24]。虽然在字体合成方面已经取得了令人鼓舞的结果，但大多数现有的方法都需要一个不切实际的大尺寸初始集来训练模型。例如，[1,2,7,15]需要3000个成对字符用于培训监督，这需要大量人力资源。最近的几种方法[12,19,24]探索了较少配对字符的学习；然而，这些方法严重依赖于额外的标签或其他预先训练的网络。表1提供了上述方法的比较摘要。上述大多数方法侧重于打印字体合成。与印刷字体合成相比，手写字体合成是一项更具挑战性的任务。首先，它的笔划较细，尤其是在笔划的关节处。手写字体也与不规则结构相关，很难建模。到目前为止，还没有令人满意的手写字体合成解决方案。本文旨在用较少的训练样本合成更真实的手写字体。主要存在两个挑战：（i）手写字符通常与信息量少、结构复杂的细笔划相关，在变形过程中容易出错；（参见图1）和（ii）基于几个手动设计的字符，需要合成数千个不同形状的字符（参见图2）。为了解决第一个问题，我们提出了协作笔划细化，即使用协作训练策略来恢复丢失或中断的笔划。协作笔划细化使用辅助分支生成具有不同笔划权重的合成，引导主导分支捕获具有不同笔划权重的角色。为了解决第二个问题，我们充分利用了汉字中的内容重用现象；也就是说，相同的部首可能在不同的位置以不同的字符出现。基于这一观察，我们提出了在线变焦增强，它通过一系列基本组件合成一个复杂的字符。所提出的网络可以专注于学习这些基本组件的各种位置和比率。这大大减少了训练样本的大小。为了使所提出的网络易于捕获从源到目标的变形，我们进一步提出了自适应预变形，即通过学习变形的大小和尺度来标准化字符。这使得建议的网络专注于学习高级风格变形。结合上述方法，我们提出了端到端模型（如图3所示），仅需750个训练样本即可生成高质量的字符。不需要预先培训的网络或标签。我们在几种中文字体（包括手写字体和印刷字体）上验证了该模型的性能。实验结果表明，与目前最先进的中文字体生成模型相比，该方法具有明显的优越性。主要贡献总结如下：我们提出了一个端到端的模型，只需750个训练样本就可以合成手写体中文字体我们提出了协作笔划细化，处理细化问题；在线缩放增强，使用较少的训练样本进行学习；以及自适应预变形、标准化和对齐字符；•实验表明，所提出的网络在视觉效果、RMSE度量和用户学习方面都远远优于基线。

##### Separating Style and Content for Generalized Style Transfer

. Introduction In recent years, style transfer, as an interesting application of deep neural networks (DNNs), has increasingly attracted attention among the research community. Existing studies either apply an iterative optimization mechanism [8] or directly learn a feed-forward generator network to force the output image to be with target style and target contents [12, 23]. A set of losses are accordingly proposed for the transfer network, such as the pix-wise loss [10], the perceptual loss [12, 27], and the histogram loss [25]. Recently, several variations of generative adversarial networks (GANs) [14, 28] are introduced by adding a discriminator to the style transfer network which incorporates adversarial loss with transfer loss to generate better images. However, these studies aim to explicitly learn the transformation from a certain source style to a given target style, and the learned model is thus not generalizable to new styles, i.e. retraining is needed for transformations of new styles which is timeconsuming. In this paper, we propose a novel generalized style transfer network which can extend well to new styles or contents. Different from existing supervised style transfer methods, where an individual transfer network is built for each pair of style transfer, the proposed network represents each style or content with a small set of reference images and attempts to learn separate representations for styles and contents. Then, to generate an image of a given style-content combination is simply to mix the corresponding two representations. This learning framework allows simultaneous style transfer among multiple styles and can be deemed as a special ‘multi-task’ learning scenario. Through separated style and content representations, the network is able to generate images of all style-content combination given the corresponding reference sets, and is therefore expected to generalize well to new styles and contents. To our best knowledge, the study most resembles to ours is the bilinear model proposed by Tenenbaum and Freeman [22], which obtained independent style and content representations through matrix decomposition. However, it usually requires an exhaustive enumeration of examples for accurate decomposition of new styles and contents, which may not be readily available for some styles/contents. As shown in Figure 1, the proposed style transfer network, denoted as EMD thereafter, consists of a style encoder, a content encoder, a mixer, and a decoder. Given a set of reference images, the style/content encoder leverages the conditional dependence of styles and contents to learn style/content representations. The mixer then combines the corresponding style and content representations using a bilinear model. The decoder finally generates the target images based on the combined representations. Each training example for the proposed network is provided as a triplet <RSi , RCj , Iij>, where Iij is the target image of style Si and content Cj. RSi and RCj are respectively the style and content reference sets, each consisting of r random images of the corresponding style Si and content Cj. The entire network is trained end-to-end with a weighted L1 loss measuring the difference between the generated images and the target images. As it is difficult to validate the decomposition of style and content for images, we here use the character typeface transfer as a special case of style transfer to validate the proposed method. Extensive experiment results have demonstrated the effectiveness and robustness of our method for style transfer. The main contributions of our study are summarized as follows. • We propose a generalized style transfer network which is able to generate images of any unseen style/contenti decomposes an image into separate style and content representations, taking advantages of the conditional dependence of contents and styles. • This learning framework allows simultaneous style transfer among multiple styles and can be deemed as a special ‘multi-task’ learning scenario.

. 引言近年来，风格转换作为深度神经网络（DNN）的一个有趣应用，越来越受到研究界的关注。现有研究要么应用迭代优化机制[8]，要么直接学习前馈生成器网络，以强制输出图像具有目标样式和目标内容[12,23]。因此，针对传输网络提出了一组损耗，例如pix损耗[10]、感知损耗[12,27]和直方图损耗[25]。最近，通过在样式传输网络中添加鉴别器，引入了生成性对抗网络（GAN）[14,28]的几种变体，该鉴别器将对抗性损失与传输损失结合起来，以生成更好的图像。然而，这些研究的目的是明确地学习从某个源风格到给定目标风格的转换，因此所学的模型不能概括为新风格，即需要重新培训以转换新风格，这需要时间。在本文中，我们提出了一种新的广义风格传递网络，它可以很好地扩展到新的风格或内容。与现有的监督风格传递方法不同，在监督风格传递方法中，为每对风格传递建立单独的传递网络，所提出的网络用一组小的参考图像表示每种风格或内容，并尝试学习风格和内容的单独表示。然后，要生成给定样式内容组合的图像，只需混合相应的两种表示。这种学习框架允许在多种风格之间同时进行风格转换，可以被视为一种特殊的“多任务”学习场景。通过分离的样式和内容表示，网络能够生成给定相应参考集的所有样式-内容组合的图像，因此有望很好地推广到新样式和内容。据我们所知，与我们最相似的研究是Tenenbaum和Freeman[22]提出的双线性模型，该模型通过矩阵分解获得独立的风格和内容表示。但是，它通常需要详尽列举示例，以便准确分解新的样式和内容，而对于某些样式/内容，这些示例可能并不容易获得。如图1所示，提议的样式传输网络（此后称为EMD）由样式编码器、内容编码器、混音器和解码器组成。给定一组参考图像，样式/内容编码器利用样式和内容的条件依赖性来学习样式/内容表示。然后，混合器使用双线性模型组合相应的样式和内容表示。解码器最终根据组合表示生成目标图像。建议网络的每个训练示例都作为三元组提供，其中Iij是样式Si和内容Cj的目标图像。RSi和RCj分别是风格和内容参考集，每个参考集由相应风格Si和内容Cj的r个随机图像组成。整个网络通过测量生成图像和目标图像之间差异的加权L1损失进行端到端训练。由于很难验证图像的样式和内容分解，我们在这里使用字符-字体转换作为样式转换的一种特殊情况来验证所提出的方法。大量的实验结果证明了我们的风格转换方法的有效性和鲁棒性。我们研究的主要贡献总结如下：我们提出了一个广义的风格传递网络，它能够在给定一小组参考图像的情况下生成任何看不见的风格/内容的图像该网络利用内容和样式的条件依赖性，将图像分解为单独的样式和内容表示这种学习框架允许在多种风格之间同时进行风格转换，可以被视为一种特殊的“多任务”学习场景。



![image-20211019135943442](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20211019135943442.png)



##### Coconditional Autoencoding Adversarial Networks for Chinese Font Feature Learning

1 Introduction The Chinese writing system and through it a sense of a common literature and history is the very fabric that held China together for millennia. **But unlike phonetic writing system which have very limited number of letters such as English, Chinese has a huge amount of ideographic characters(more than 80000). On the other hand, most Chinese characters have more complex shapes and structures than other symbolic characters.** That is why designing a new Chinese font is such an expensive and difficult work, it needs a group of type designers and calligraphers working together for years for a typerface covering official character set like GB-18030, which contains 27533 unique characters. Due to this fact, we could seldomly see independent artists working on Chinese typerface design, which also leads to the status quo that there existed fewer well-designed Chinese digital fonts than alphabetic ones. To alleviate the labor intensive part in this design process, various automatic synthesizing approaches have been proposed these years, among them, machine learing method is a promising solution. In this work, we break the Chinese font learing task down into 2 subtasks (the encoding part and the adversarial), and fufill each with 2 convolutional networks. The encoding networks are designed to disentangle content and style features separately, with a pairwise substitution optimization we force the networks to capture lattent embeddings of feature from different domains. The overall process of our proposed framwork is shown in Fig. 1. The main contributions of this work are: • We propose a novel model which can disentangle each Chinese glyph image into content and style representations automatically, and with those two kinds of feature embedding, the model can generate specified glyph in clear appearance. • We incoporate the adversarial netwroks with the auto-encoders, and propose an adversarial way to train the auto-encoders. • We demonstrate the potention of our proposed framwork to be generalized to new fonts and other characters beyond the training sets.

1导言中国的写作体系，通过它，一种共同的文学和历史感，正是这种结构将中国维系了数千年。但与英语等字母数量非常有限的拼音书写系统不同，汉语有大量的表意文字（超过80000个）。另一方面，大多数汉字的形状和结构比其他符号汉字更复杂。这就是为什么设计一种新的中文字体是如此昂贵和困难的工作，它需要一组字体设计师和书法家多年来共同工作，以获得一个涵盖官方字符集（如GB-18030）的字体表面，该字体包含27533个独特字符。由于这一事实，我们很少能看到独立的艺术家从事中文字体设计，这也导致了设计良好的中文数字字体少于字母字体的现状。为了减轻设计过程中的劳动密集部分，近年来提出了各种自动综合方法，其中机器学习方法是一种很有前途的解决方案。在这项工作中，我们将汉字字体学习任务分解为两个子任务（编码部分和敌对部分），并用两个卷积网络填充每个子任务。编码网络被设计为分别分离内容和样式特征，通过成对替换优化，我们强制网络捕获不同领域特征的Latent嵌入。我们提出的框架的整体过程如图1所示。本工作的主要贡献是：•我们提出了一种新的模型，该模型可以自动将每个汉字字形图像分解为内容和样式表示，并且通过这两种特征嵌入，该模型可以生成外观清晰的特定字形我们将对抗性netwroks与自动编码器结合起来，并提出一种对抗性的方法来训练自动编码器我们展示了我们提出的框架的潜力，可以推广到新的字体和训练集以外的其他字符。

##### TET-GAN: Text Effects Transfer via Stylization and Destylization

Introduction Text effects are additional style features for text, such as colors, outlines, shadows, reflections, glows and textures. Rendering text in the style specified by the example stylized text is referred to as text effects transfer. Applying visual effects to text is very common yet important in graphic design. However, manually rendering text effects is labor intensive and requires great skills beyond normal users. In this work, we propose a neural network architecture that automatically synthesizes high-quality text effects on arbitrary glyphs. The success of the pioneering Neural Style Transfer (Gatys, Ecker, and Bethge 2016) has sparked a research boom of deep-based image stylization. The key idea behind it is to match the global feature distributions between the style image and the generated image (Li et al. 2017a) by minimizing the difference of Gram matrices (Gatys, Ecker, and Bethge 2015). However, this global statistics representation for general styles does not apply to the text effects. Text effects are highly structured along the glyph and cannot be simply characterized as the mean, variance or other global statistics (Li et al. 2017a; Dumoulin, Shlens, and Kudlur 2016; Huang and Belongie 2017; Li et al. 2017c) of the texture features. Instead, the effects should be learned with the corresponding glyphs. For this reason, we develop a new text effects dataset, driven by which we show our network can learn to properly rearrange textures to fit new glyphs. From a perspective of texture rearrangement, modelling the style of text effects using local patches seems to be more suitable than global statistics. Valuable efforts have been devoted to patch-based style transfer (Li and Wand 2016a; Chen and Schmidt 2016; Yang et al. 2017). The recent work of (Yang et al. 2017) is the first study of text effects transfer, where textures are rearranged to correlated positions on text skeleton. However, matching patches in the pixel domain, this method fails to find proper patches if the target and example glyphs differ a lot. The recent deep-based methods (Li and Wand 2016a; Chen and Schmidt 2016) address this issue by matching glyphs in the feature domain, but they use a greedy optimization, causing the disorder of the texture in the spatial distribution. To solve this problem, we introduce a novel distribution-aware data augmentation strategy to constrain the spatial distribution of textures. To handle a particular style, researchers have looked at style modeling from images rather than using general statistics or patches, which refers to image-to-image translation (Isola et al. 2017). Early attempts (Isola et al. 2017; Zhu et al. 2017) train generative adversarial networks (GAN) to map images from two domains, which is limited to only two styles. StarGAN (Choi et al. 2018) employs one-hot vectors to handle multiple pre-defined styles, but requires expensive data collection and retraining to handle new styles. We improve these approaches by designing a novel Texture Effects Transfer GAN (TET-GAN), which characterizes glyphs and styles separately. By disentangling and recombining glyph and visual effects features, we show that our network can simultaneously support stylization and destylization on a variety of text effects as shown in Fig. 1. In addition, having learned to rearrange textures based on glyphs, a trained network can easily be extended to new user-specified text effects. In this paper, we propose a novel approach for text effects transfer with three distinctive aspects. First, we develop a novel TET-GAN built upon encoder-decoder architectures. The encoders are trained to disentangle content and style features in the text effects images. Stylization is implemented by recombining these two features while destylization by solely decoding content features. The task of destylization to completely remove styles guides the network to precisely extract the content feature, which in turn helps the network better capture its spatial relationship with the style feature in the task of stylization. Second, in terms of data, we develop a new text effects dataset with 53,568 image pairs to facilitate training and further study. In addition, we propose a distribution-aware data augmentation strategy to impose a distribution constraint (Yang et al. 2017) for text effects. Driven by the data, our network learns to rearrange visual effects according to the glyph structure and its correlated position on the glyph as a professional designer does. Finally, we propose a self-stylization training scheme for one-shot learning. Leveraging the skills that have been learned from our dataset, the network only needs to additionally learn to reconstruct the texture details of one example, and then it can generate the new style on any glyph. In summary, our contributions are threefold: • We raise a novel TET-GAN to disentangle and recombine glyphs and visual effects for text effects transfer. The explicit content and style representations enable effective stylization and destylization on multiple text effects. • We introduce a new dataset containing thousands of professionally designed text effects images, and propose a distribution-aware data augmentation strategy for distribution-aware style transfer. • We propose a novel self-stylization training scheme that requires only a few or even one example to learn a new style upon a trained network.

简介文字效果是文字的附加样式功能，例如颜色、轮廓、阴影、反射、发光和纹理。以示例样式化文本指定的样式呈现文本称为文本效果传输。在平面设计中，将视觉效果应用于文本是非常常见但重要的。但是，手动渲染文本效果需要耗费大量人力，并且需要比普通用户更高的技能。在这项工作中，我们提出了一种神经网络架构，可以自动合成任意字形上的高质量文本效果。开创性的神经风格转换（Gatys、Ecker和Bethge 2016）的成功引发了深层图像风格化的研究热潮。其背后的关键思想是通过最小化Gram矩阵的差异来匹配样式图像和生成图像之间的全局特征分布（Li et al.2017a）（Gatys、Ecker和Bethge 2015）。但是，常规样式的此全局统计表示不适用于文字效果。文字效果沿字形高度结构化，不能简单地描述为纹理特征的平均值、方差或其他全局统计（Li等人2017a；Dumoulin、Shlens和Kudlur 2016；Huang和Belongie 2017；Li等人2017c）。相反，应使用相应的图示符学习效果。出于这个原因，我们开发了一个新的文本效果数据集，通过该数据集，我们展示了我们的网络可以学习正确地重新排列纹理以适应新的字形。从纹理重排的角度来看，使用局部补丁对文本效果的样式进行建模似乎比全局统计更合适。在基于补丁的风格转换方面做出了宝贵的努力（Li和Wand 2016a；Chen和Schmidt 2016；Yang等人，2017）。最近的工作（Yang et al.2017）是首次对文本效果转移的研究，其中纹理被重新排列到文本骨架上的相关位置。然而，在像素域中匹配面片时，如果目标和示例图示符差异很大，该方法无法找到合适的面片。最近的基于深度的方法（Li和Wand 2016a；Chen和Schmidt 2016）通过在特征域中匹配轮廓来解决这个问题，但它们使用贪婪优化，导致纹理在空间分布上的无序。为了解决这个问题，我们引入了一种新的分布感知数据增强策略来约束纹理的空间分布。为了处理特定的风格，研究人员从图像中研究了风格建模，而不是使用一般的统计数据或补丁，即图像到图像的转换（Isola et al.2017）。早期尝试（Isola et al.2017；Zhu et al.2017）训练生成性对抗网络（GAN）从两个领域映射图像，这仅限于两种风格。StarGAN（Choi et al.2018）使用一个热向量来处理多个预定义样式，但需要昂贵的数据收集和再培训来处理新样式。我们通过设计一种新颖的纹理效果转移GAN（TET-GAN）来改进这些方法，该GAN分别表征字形和样式。通过分离和重组字形和视觉效果功能，我们表明我们的网络可以同时支持各种文本效果的样式化和非样式化，如图1所示。此外，在学习了基于字形重新排列纹理之后，经过训练的网络可以轻松地扩展到新的用户指定的文本效果。本文从三个方面提出了一种新的文本效果迁移方法。首先，我们开发了一种基于编解码器结构的新型TET-GAN。编码器经过训练，可以分离文本效果图像中的内容和样式特征。样式化是通过重新组合这两个特性来实现的，而非样式化是通过单独解码内容特性来实现的。完全删除样式的非样式化任务引导网络精确提取内容特征，这反过来有助于网络在样式化任务中更好地捕获其与样式特征的空间关系。其次，在数据方面，我们开发了一个新的文本效果数据集，包含53568对图像，以便于训练和进一步研究。此外，我们提出了一种分布感知数据增强策略，以对文本效果施加分布约束（Yang等人，2017）。在数据的驱动下，我们的网络学习根据字形结构及其在字形上的相关位置重新排列视觉效果，就像专业设计师所做的那样。最后，我们提出了一个一次性学习的自我风格化训练方案。利用从我们的数据集中学习的技能，网络只需另外学习重建一个示例的纹理细节，然后就可以在任何字形上生成新样式。总之，我们的贡献有三个方面：•我们提出了一种新的TET-GAN，用于分离和重新组合字形和视觉效果，以进行文本效果传输。明确的内容和样式表示可以对多个文本效果进行有效的样式化和非样式化我们引入了一个包含数千幅专业设计的文本效果图像的新数据集，并提出了一种用于分布感知风格转换的分布感知数据增强策略我们提出了一种新颖的自我风格化训练方案，只需要几个甚至一个例子就可以在训练过的网络上学习新的风格。

##### Controllable Artistic Text Style Transfer via Shape-Matching GAN

Introduction Artistic text style transfer aims to render text in the style specified by a reference image, which is widely desired in many visual creation tasks such as poster and advertisement design. Depending on the reference image, text can be stylized either by making analogy of existing well-designed text effects [28], or by imitating the visual features from more general free-form style images [30]: the latter provides more flexibility and creativity. For general style images as reference, since text is significantly different from and more structured than natural images, more attention should be paid to its stroke shape in the stylization of text. For example, one needs to manipulate the stylistic degree or shape deformations of a glyph to resemble the style subject flames in Fig. 1(b). Meanwhile, the glyph legibility needs to be maintained so that the stylized text is still recognizable. Such a delicate balance is subjective and hard to attain automatically. Therefore, a practical tool allowing users to control the stylistic degree of the glyph is of great value. Further, as users are prone to trying various settings before obtaining desired effects, real-time response to online adjustment is important. In the literature, some efforts have been devoted to addressing fast scale-controllable style transfer. They trained fast feed-forward networks, with the main focus on the scale of textures like the texture strength [2], or the size of texture patterns [17]. Up to our best knowledge, there has been no work discussing the real-time control of glyph deformations, which is rather crucial for text style transfer. In view of the above, we are motivated to investigate a new problem of fast controllable artistic text style transfer from a single style image. We aim at the real-time adjustment for the stylistic degree of the glyph in terms of shape deformations. It can allow users to navigate around different forms of the rendered text and select the most desired one, as illustrated in Fig. 1(b)(c). The challenges of fast controllable artistic text style transfer lie in two aspects. On one hand, in contrast to well-defined scales such as the texture strength that can be straightforwardly modelled by hyper-parameters, the glyph deformation degree is subjective, neither clearly defined nor easy to parameterize. On the other hand, there does not exist a large-scale paired training set with both source text images and the corresponding results stylized (deformed) in different degrees. Usually, only one reference image is available for a certain style. It is thus also not straightforward to train data-driven models to learn multi-scale glyph stylization. In this work, we propose a novel Shape-Matching GAN to address these challenges. Our key idea is a bidirectional shape matching strategy to establish the shape mapping between source styles and target glyphs through both backward and forward transfers. We first show that the glyph deformation can be modelled as a coarse-to-fine shape mapping of the style image, where the deformation degree is controlled by the coarse level. Based on this idea, we develop a sketch module that simplifies the style image to various coarse levels by backward transferring the shape features from the text to the style image. Resulting coarsefine image pairs provide a robust multi-scale shape mapping for data-driven learning. With this obtained data, we build a scale-controllable module, Controllable ResBlock, that empowers the network to learn to characterize and infer the style features on a continuous scale from the mapping. Eventually, we can forward transfer the features of any specified scale to target glyphs to achieve scale-controllable style transfer. In summary, our contributions are threefold: • We investigate the new problem of fast controllable artistic text style transfer, in terms of glyph deformations, and propose a novel bidirectional shape matching framework to solve it. • We develop a sketch module to match the shape from the style to the glyph, which transforms a single style image to paired training data at various scales and thus enables learning robust glyph-style mappings. • We present Shape-Matching GAN to transfer text styles, with a scale-controllable module designed to allow for adjusting the stylistic degree of the glyph with a continuous parameter as user input and generating diversified artistic text in real-time.

引言艺术文本风格转换旨在以参考图像指定的风格呈现文本，这在海报和广告设计等许多视觉创作任务中都是广泛需要的。根据参考图像的不同，文本可以通过类比现有精心设计的文本效果[28]或通过模仿更一般的自由形式图像的视觉特征[30]进行样式化：后者提供了更大的灵活性和创造性。对于一般风格的图像作为参考，由于文本与自然图像显著不同，并且比自然图像更加结构化，因此在文本的风格化过程中，应更多地关注其笔划形状。例如，需要操纵字形的风格程度或形状变形，以类似于图1（b）中的风格主题火焰。同时，需要保持字形的易读性，以便样式化文本仍然可以识别。这种微妙的平衡是主观的，很难自动达到。因此，一个实用的工具，允许用户控制字形的风格程度是非常有价值的。此外，由于用户在获得预期效果之前容易尝试各种设置，因此对在线调整的实时响应非常重要。在文献中，一些研究致力于解决快速规模可控风格转换问题。他们训练快速前馈网络，主要关注纹理的规模，如纹理强度[2]，或纹理图案的大小[17]。据我们所知，还没有讨论实时控制字形变形的工作，这对于文本样式转换非常关键。鉴于此，我们致力于研究一个新的问题，即从单一风格图像快速可控的艺术文本风格转换。我们的目标是实时调整字形在形状变形方面的风格程度。如图1（b）（c）所示，它可以允许用户在不同形式的渲染文本周围导航并选择最想要的文本。快速可控的艺术文本风格转换的挑战在于两个方面。一方面，与定义良好的比例（如纹理强度）不同，纹理强度可以通过超参数直接建模，字形变形程度是主观的，既不明确定义，也不容易参数化。另一方面，不存在同时具有源文本图像和不同程度样式化（变形）的相应结果的大规模成对训练集。通常，对于某个样式，只有一个参考图像可用。因此，训练数据驱动模型来学习多尺度图示符样式化也不是一件容易的事情。在这项工作中，我们提出了一种新颖的形状匹配GAN来解决这些挑战。我们的关键思想是一种双向形状匹配策略，通过向后和向前传输在源样式和目标图示符之间建立形状映射。我们首先表明，字形变形可以建模为样式图像的从粗到细的形状映射，其中变形程度由粗级别控制。基于这一思想，我们开发了一个草图模块，通过将形状特征从文本向后转移到样式图像，将样式图像简化为各种粗略级别。由此产生的粗糙精细图像对为数据驱动学习提供了一个健壮的多尺度形状映射。利用这些获得的数据，我们构建了一个规模可控模块可控ResBlock，该模块使网络能够从映射中学习描述和推断连续规模上的风格特征。最终，我们可以将任何指定比例的特征向前传输到目标图示符，以实现比例可控的样式传输。综上所述，我们的贡献有三个方面：•我们从字形变形的角度研究了快速可控艺术文本风格转换的新问题，并提出了一种新的双向形状匹配框架来解决这个问题我们开发了一个草图模块来匹配从样式到字形的形状，该模块将单个样式图像转换为不同比例的成对训练数据，从而能够学习稳健的字形样式映射我们提出了形状匹配GAN来传输文本样式，并设计了一个比例可控模块，以允许使用连续参数作为用户输入来调整字形的风格程度，并实时生成多样化的艺术文本。

##### Pyramid Embedded Generative Adversarial Network for Automated Font Generation

INTRODUCTION Font, as one basic element, has been widely used in various aspects of the art and design. However, the design of font is a very time-consuming task. During the first stage of developing a font bank, calligrapher needs to write a large amount of characters as templates . And for the second stage, font designers will spend quite a long time to digitize these character templates and manually create all other characters that do not exist in the templates. Particularly, it takes even more time to design one Chinese font which has a much larger dictionary compared with English or Latin that includes only tens of letters. Therefore, a more efficient way to automatically generate a font bank is desperately needed. There are some attempts to synthesize Chinese characters automatically. The most typical method is based on stroke extraction [1]. In this kind of methods, font generation procedure is divided into stroke extraction and recombination of isolated strokes. Effective stroke extraction from character image plays a crucial role to determine the performance of the font generation. However, the current stroke extraction algorithms can not always work well due to the complexity and diversity of Chinese characters. Generative adversarial network (GAN) [2] is increasingly being used for image generation. The original GAN has one generator and one discriminator. The generator can generate realistic images and try to fool the discriminator. The discriminator is used to classify real images and generated images. The generator and discriminator can be trained alternately by one minmax policy [2]. In [3], conditional GAN is proposed by feeding an auxiliary information such as image class labels together with original input of GAN to generator and discriminator. In [4], pix2pix is proposed to transform images from one style to another based on conditional GAN. Similar to pix2pix, zi2zi [5] is also built on conditional GAN and it is the first attempt to automatically generate font images. There is another type of method which is not based on adversarial learning and also can be used for image generation. In [6], one convolutional network which is denoted as cascaded refinement network (CRN) is proposed to synthesize photographic images conditioned on pixel-wise semantic layouts. The backbone of CRN is one feedforward convolutional network that consists of several cascaded refinement modules. Each module operates at a given resolution, which is doubled between consecutive modules. This design feeds low level information of input image to different layers, preserves much more detailed information during training and results in highresolution images generation. However, the input and output of CRN are supposed to have identical layout. Thus, it cannot handle the image transformation. In this work, we treat the task of font generation as image to image transformation and propose a new method for automated font generation. The contributions of this paper are as follows: 1) We propose PEGAN by embedding multi-scale pyramid of refinement information into U-Net [7]. This design can enable generator preserve much more detailed information and is beneficial for model performance. 2) We deploy four loss functions in training procedure: adversarial loss, pixel-wise loss, category loss and perceptual loss. The adversarial loss is beneficial to generate images with fine details. The pixel loss indicates the pixel space distance between real and generated images, while perceptual loss measures the discrepancy between them from perceptual aspect. The category loss, which is important for pre-training, enables the model to learn from multiple styles simultaneously. 3) We build a character set for evaluation based on stroke number and frequency of use. We conduct both qualitative and quantitative measurements to evaluate the performance of proposed font generation model. The organization of the rest of paper is as follows. In the next Section, we review recent methods for font generation. In Section III, the framework of PEGAN is introduced, as well as loss functions and implementation details. Section IV shows experimental results, where we report our results using Microsoft HeiTi to generate HuaKang font. Also, we deploy PEGAN performing small font banks extension. Finally, we conclude the paper in Section V.

引言字体作为一种基本元素，已广泛应用于艺术设计的各个方面。然而，字体的设计是一项非常耗时的任务。在开发字体库的第一阶段，书法家需要编写大量的字符作为模板。对于第二阶段，字体设计师将花费相当长的时间数字化这些字符模板，并手动创建模板中不存在的所有其他字符。特别是，设计一种中文字体需要更多的时间，因为它的字典要比英文或拉丁文大得多，而英文或拉丁文只包含几十个字母。因此，迫切需要一种更有效的自动生成字体库的方法。有人尝试自动合成汉字。最典型的方法是基于笔划提取[1]。在这种方法中，字体生成过程分为笔划提取和孤立笔划的重组。从字符图像中有效地提取笔划对字体生成的性能起着至关重要的作用。然而，由于汉字的复杂性和多样性，现有的笔划提取算法往往不能很好地工作。生成性对抗网络（GAN）[2]越来越多地用于图像生成。原始GAN具有一个发生器和一个鉴别器。生成器可以生成真实的图像并试图愚弄鉴别器。鉴别器用于对真实图像和生成的图像进行分类。生成器和鉴别器可以通过一个最小-最大策略交替训练[2]。在[3]中，通过将辅助信息（如图像类别标签）与GAN的原始输入一起提供给生成器和鉴别器，提出了条件GAN。在[4]中，pix2pix被提议基于条件GAN将图像从一种样式转换为另一种样式。与pix2pix类似，zi2zi[5]也是基于条件GAN构建的，它是第一次尝试自动生成字体图像。还有一种方法不是基于对抗性学习，也可以用于图像生成。在文献[6]中，提出了一种卷积网络，称为级联细化网络（CRN），用于合成基于像素语义布局的摄影图像。CRN的主干是一个由多个级联求精模块组成的前馈卷积网络。每个模块都以给定的分辨率运行，在连续模块之间分辨率加倍。该设计将输入图像的低级信息反馈到不同的层，在训练过程中保留更详细的信息，并生成高分辨率图像。然而，CRN的输入和输出应该具有相同的布局。因此，它无法处理图像变换。在这项工作中，我们将字体生成任务视为图像到图像的转换，并提出了一种新的字体自动生成方法。本文的贡献如下：1）我们通过将多尺度金字塔细化信息嵌入到U-Net[7]中，提出了PEGAN。这种设计可以使生成器保存更详细的信息，并有利于模型性能。2） 我们在训练过程中部署了四个损失函数：对抗损失、像素损失、类别损失和感知损失。对抗性损失有利于生成具有精细细节的图像。像素损失表示真实图像和生成图像之间的像素空间距离，而感知损失则从感知角度衡量它们之间的差异。类别丢失对于预训练非常重要，它使模型能够同时从多个样式中学习。3） 我们建立了一个基于笔划数和使用频率的字符集进行评估。我们进行定性和定量测量来评估所提出的字体生成模型的性能。本文其余部分的组织结构如下。在下一节中，我们将回顾字体生成的最新方法。第三节介绍了PEGAN的框架、功能和实现细节。第四部分展示了实验结果，我们报告了使用Microsoft HeiTi生成华康字体的结果。此外，我们还部署了一个小型字体库扩展。最后，我们在第五部分对论文进行总结。

##### Auto-Encoder Guided GAN for Chinese Calligraphy Synthesis

INTRODUCTION Chinese calligraphy is a very unique visual art and an important manifestation of Chinese ancient culture which is popular with many people in the world. Writing a pleasing calligraphy work is so difficult that it always takes the writer many years to learn from the famous calligraphers’ facsimiles. Is there a way to synthesize calligraphy with specified style expediently? We will explore an effective and efficient approach for calligraphy synthesis in this paper. Automatic calligraphy synthesis is a very challenging problem due to the following reasons: 1) Various Chinese calligraphy styles. A Chinese character usually has thousands of calligraphy styles which vary from the shapes of component and the styles of strokes; 2) Deformations between the standard font image and calligraphy image. The standard font image and calligraphy image for the same character are only similar in relative layout of radicals of character but different in the layout and style of strokes. Recently, there are some attempts [27], [25] to synthesize calligraphy automatically, which first extract strokes from some known calligraphy characters and then some strokes are selected and assembled into a new calligraphy character. The above mentioned methods are largely dependent on the effect of strokes extraction. However, the stroke extraction technology does not always work well when the Chinese character is too complex or the character is written in a cursive style (Fig. 1(b)) where the strokes are hard to separate and have to be extracted artificially [26]. Considering there are some shortcomings in stroke assemble based methods, we treat the calligraphy generation as an image-to-image translation problem and propose a new method which can generate calligraphy with a specified style from a standard Chinese font (i.e. Hei Font) directly without extracting strokes of characters. Over the past few years, many network architectures have been proposed and applied to different image-to-image tasks. However, those networks are all designed to handle the pixel-to-pixel problems, such as semantic segmentation, and poor performance is achieved when there are deformations between the input and target images (Fig. 1(c)). To overcome these problems, **we propose a deep neural network based model which consists of two subnets.** The first one is an encoder-decoder network acting as image transfer, which encodes an input standard font image to a feature representation and then decodes the feature representation to a calligraphy image with specified style. The encoder-decoder network with similar architecture has been used in [9] and show that the feature representation is likely to compress the image content. This network architecture is sufficient to reconstruct an image. But considering that in our task the input images and output images only have the same relative layout among radicals but are different in the layout and style of strokes, it is hard for an encoder-decoder network to yield vivid calligraphy images. So besides the transfer who captures the layout of input standard font image, we also use another encoder-decoder network acting as autoencoder which inputs and reconstructs calligraphy images to guide the transfer to learn the detailed stroke information from autoencoder’s low level features. Finally, we train the two subnets together with reconstruct loss and adversarial loss to make the output look real. In summary, the contributions of this paper are two aspects: Firstly, we propose a neural network based method which can end-to-end synthesize calligraphy images with specified style from standard Chinese font images. Compared to some baseline methods, our approach achieves the best results with more realistic details. Secondly, we establish a large-scale dataset for Chinese calligraphy synthesis collected from the Internet. The dataset composes of 4 calligraphy styles and each style contains about 7000 calligraphy images.

中国书法是一门非常独特的视觉艺术，是中国古代文化的重要表现形式，深受世界各国人民的喜爱。写一件令人愉快的书法作品是如此的困难，以至于作家总是要花很多年的时间去学习著名书法家的摹本。有没有一种方法可以方便地合成特定风格的书法？本文将探索一种有效的书法合成方法。自动书法合成是一个非常具有挑战性的问题，原因如下：1）中国书法风格多样。一个汉字通常有上千种书法风格，这些风格因构件形状和笔画风格而异；2） 标准字体图像和书法图像之间的变形。同一个汉字的标准字体图像和书法图像，只是在字根的相对布局上相似，但在笔画的布局和风格上不同。最近，有一些尝试[27]、[25]自动合成书法，首先从一些已知的书法字符中提取笔画，然后选择一些笔画并组合成一个新的书法字符。上述方法在很大程度上取决于笔划提取的效果。然而，当汉字过于复杂或以草书形式书写（图1（b））时，笔划提取技术并不总是能够很好地工作，其中笔划很难分离，必须人工提取[26]。考虑到基于笔画组合的方法存在一些不足，我们将书法生成视为一个图像到图像的翻译问题，并提出了一种新的方法，可以直接从标准汉字字体（即黑字体）生成具有特定风格的书法，而无需提取字符笔画。在过去的几年中，许多网络架构被提出并应用于不同的图像到图像任务。然而，这些网络都被设计用于处理像素到像素的问题，例如语义分割，并且当输入图像和目标图像之间存在变形时，实现了较差的性能（图1（c））。为了克服这些问题，我们提出了一种基于深度神经网络的模型，该模型由两个子网组成。第一种是作为图像传输的编码器-解码器网络，它将输入的标准字体图像编码为特征表示，然后将特征表示解码为具有指定样式的书法图像。[9]中使用了具有类似结构的编码器-解码器网络，并表明特征表示可能会压缩图像内容。这种网络架构足以重建图像。但是，考虑到在我们的任务中，输入图像和输出图像在部首之间只有相同的相对布局，但笔画的布局和风格不同，编码器-解码器网络很难生成生动的书法图像。因此，除了捕获输入标准字体图像布局的传输外，我们还使用另一个编码器-解码器网络作为自动编码器，输入并重建书法图像，以指导传输，从自动编码器的低级特征中学习详细的笔划信息。最后，我们对这两个子网进行训练，并结合重构损耗和对抗损耗使输出看起来真实。综上所述，本文的贡献有两个方面：首先，我们提出了一种基于神经网络的方法，可以从标准汉字字体图像中端到端合成具有特定风格的书法图像。与一些基线方法相比，我们的方法在细节更真实的情况下获得了最好的结果。其次，我们建立了一个从互联网上收集的大规模书法综合数据集。该数据集由4种书法风格组成，每种风格包含约7000幅书法图像。

##### Generating Handwritten Chinese Characters using CycleGAN

Introduction Chinese characters have been used continually over three millennia by more than a quarter of the world’s population [24]. The handwriting of Chinese character has long been one of the most fundamental skills in education, employment, communication, and everyday life in East Asia. For a long time, good handwriting or calligraphy has been considered not only as an artistic expression of language, but also as the supreme visual art as a means of self-expression and cultivation. As an example, Figure 1 shows some of our generated calligraphic characters. These aesthetically pleasing calligraphic works usually needs years of dedication and practice. In contrast to phonological languages that have very limited number of letters such as English, Chinese has more than 80, 000 logographic characters. Therefore, it is more challenging to design a personalized Chinese font than phonological languages. For example, only 26 letters need to be designed for a personalized English font while for Chinese at least 3, 000 most commonly used characters need to be designed. To meet the demands of designing personalized Chinese font, methods that can automatically generate characters with personalized handwritten style based on a relatively small set of training characters are needed. Although handwritten Chinese character generation is not as widely studied as character recognition, there are still approaches proposed for handwritten Chinese character generation. Most previous works rely on the hierarchical representation of simple strokes [27, 26, 17]. They decompose Chinese characters into strokes and then combine strokes to mimic the personalized writing style. As a result, such methods only focus on local representations of the characters rather than the overall style as a whole, and thus need to adjust the shapes, sizes, and positions of the strokes for every new character. In contrast, zi2zi [25] learns to transform fonts using pix2pix [9] with paired character images as the training data. However, in the task of generating handwritten Chinese characters, it is difficult to obtain a large set of paired training examples since it is infeasible to ask the user to write a large number of characters. The handwriting samples are also often isolated from a user’s writing, without knowing the true labels of the characters. Further, even the same character written by a user varies every time, which makes it more important to learn the overall style instead of mimicking every single character. Therefore, it is more appropriate to use unpaired Chinese characters instead of paired data for the handwritten Chinese generation problem. Figure 2 provides an example of paired and unpaired training data. Paired training data contain images of the same character in both fonts, while unpaired data do not necessarily contain the same set of characters. In this work, we formulate the Chinese handwritten character generation as a problem that learns a mapping from an existing printed font to a personalized handwritten style. We further propose a method based on unpaired image-toimage translation to solve this problem. Our main contributions are: • We propose to generate Chinese characters in a personalized handwritten style using DenseNet CycleGAN. • We propose content accuracy and style discrepancy as the evaluation metrics to assess the quality of the generated characters. • We demonstrate the efficacy of our proposed method on both CASIA dataset [16] and our newly introduced Lanting calligraphy dataset.

简介3000多年来，超过四分之一的世界人口一直在使用汉字[24]。汉字书写一直是东亚地区教育、就业、交流和日常生活中最基本的技能之一。长期以来，好的书法不仅被视为语言的艺术表现，而且被视为自我表达和修养的最高视觉艺术。例如，图1显示了我们生成的一些书法字符。这些优美的书法作品通常需要多年的奉献和实践。与英语等字母数量非常有限的语音语言相比，汉语有8万多个符号字符。因此，设计个性化的汉字字体比语音语言更具挑战性。例如，个性化的英文字体只需要设计26个字母，而中文字体至少需要设计3000个最常用的字符。为了满足个性化汉字字体设计的需要，需要基于相对较小的训练字符集自动生成具有个性化手写风格的字符的方法。尽管手写体汉字生成的研究不如字符识别广泛，但仍有一些方法被提出用于手写体汉字生成。以前的大多数作品都依赖于简单笔画的层次表示[27,26,17]。他们将汉字分解成笔画，然后结合笔画来模仿个性化的书写风格。因此，这些方法只关注字符的局部表示，而不是整体样式，因此需要为每个新字符调整笔划的形状、大小和位置。相反，zi2zi[25]学习使用pix2pix[9]和成对字符图像作为训练数据来转换字体。然而，在生成手写汉字的任务中，由于要求用户书写大量字符是不可行的，因此很难获得大量成对的训练示例。手写样本通常也与用户的书写分离，而不知道字符的真实标签。此外，即使是用户编写的同一个字符每次都会发生变化，这使得学习整体风格而不是模仿每个字符变得更加重要。因此，对于手写体中文生成问题，使用不成对的汉字而不是成对的数据更合适。图2提供了成对和非成对训练数据的示例。成对的训练数据包含两种字体中相同字符的图像，而未成对的数据不一定包含相同的字符集。在这项工作中，我们将中文手写字符生成描述为一个从现有打印字体到个性化手写样式的映射问题。针对这一问题，我们进一步提出了一种基于非成对图像到图像的转换方法。我们的主要贡献是：•我们建议使用DenseNet CycleGAN生成个性化手写风格的汉字。•我们建议将内容准确性和风格差异作为评估标准，以评估生成字符的质量我们在CASIA数据集[16]和新引入的Lanting书法数据集上证明了我们提出的方法的有效性。



##### Visualizing Adapted Knowledge in Domain Transfer



Domain transfer or domain adaptation **aims to bridge** the distribution **gap between** source and target domains. **Many existing works study the unsupervised domain adaptation (UDA) problem,许多现有的工作研究了---问题** where the target domain is unlabeled [27, 6, 46, 1, 11]. **In this process**, we **are interested in what knowledge neural networks 我们对---感兴趣** learn and adapt. Essentially, we should visualize the knowledge difference between models: a source model trained on the source domain, and a target model learned through UDA for the target domain. **We aim to** portray the knowledge difference with image generation. **给定 Given** a translated image and its original version, **we feed the two images to** the source and the target model, respectively. **It is desired that** differences between image pairs can compensate for the knowledge difference between models, **leading to** similar outputs from the two branches (two images fed to two different models). Achieving this, we could also say that the image pair represent the knowledge difference. This visualization problem is very challenging and heretofore yet to be studied in the literature. It focuses on a relatively understudied field in transfer learning, where we distill knowledge differences from models and embed it in generated images. A related line of works, traditional image translation, generates images in the desired style utilizing content images and style images [7, 13, 48], and is applied in pixel-level alignment methods for UDA [26, 2, 44, 11]. However, relying on images from both domains to indicate the style difference, such works cannot faithfully portray the knowledge difference between source and target models, and are unable to help us understand the adaptation process. In this paper, we propose a source-free image translation (SFIT) approach, where we translate target images to the source style without using source images. The exclusion of source images prevents the system from relying on image pairs for style difference indication, and ensures that the system only learns from the two models. Specifically, we feed translated source-style images to the source model and original target images to the target model, and force similar outputs from these two branches by updating the generator network. To this end, we use the traditional knowledge distillation loss and a novel relationship preserving loss, which maintains relative channel-wise relationships between feature maps. We show that the proposed relationship preserving loss also helps to bridge the domain gap while changing the image style, further explaining the proposed method from a domain adaptation point of view. Some results of our method are shown in Fig. 1. We observe that even under the source-free setting, knowledge from the two models can still power the style transfer from the target style to the source style (SFIT decreases color saturation and whitens background to mimic the unseen source style). On several benchmarks [19, 36, 39, 38], we show that generated images from the proposed SFIT approach significantly decrease the performance gap between the two models, suggesting a successful distillation of adapted knowledge. Moreover, we find SFIT transfers the image style at varying degrees, when we use different UDA methods on the same dataset. This further verifies that the SFIT visualizations are faithful to the models and that different UDA methods can address varying degrees of style differences. For applications, we show that generated images can serve as an additional cue and enable further tuning of target models. This also falls into a demanding setting of UDA, source-free domain adaptation (SFDA) [17, 20, 24], where the system has no access to source images.

域转移或域适配旨在弥合源域和目标域之间的分布差距。许多现有的工作研究了无监督域自适应（UDA）问题，其中目标域是未标记的[27,6,46,1,11]。在这个过程中，我们对神经网络学习和适应的知识感兴趣。本质上，我们应该将模型之间的知识差异可视化：源域上训练的源模型和目标域上通过UDA学习的目标模型。我们的目标是通过图像生成来描述知识差异。给定一个翻译后的图像及其原始版本，我们分别将这两个图像提供给源模型和目标模型。希望图像对之间的差异能够补偿模型之间的知识差异，从而导致两个分支（两个图像馈送到两个不同的模型）的类似输出。为了实现这一点，我们还可以说图像对代表了知识差异。这个可视化问题非常具有挑战性，迄今为止还没有在文献中进行研究。它关注于迁移学习中一个相对未被研究的领域，我们从模型中提取知识差异并将其嵌入生成的图像中。一系列相关的工作，传统图像翻译，利用内容图像和样式图像生成所需样式的图像[7,13,48]，并应用于UDA的像素级对齐方法[26,2,44,11]。然而，依靠这两个领域的图像来表示风格差异，这些作品无法忠实地描述源模型和目标模型之间的知识差异，也无法帮助我们理解适应过程。在本文中，我们提出了一种源代码自由图像翻译（SFIT）方法，在不使用源代码图像的情况下，将目标图像翻译为源代码样式。排除源图像可防止系统依赖图像对进行样式差异指示，并确保系统仅从两个模型中学习。具体来说，我们将转换后的源样式图像提供给源模型，将原始目标图像提供给目标模型，并通过更新生成器网络强制这两个分支提供类似的输出。为此，我们使用了传统的知识提取损失和一种新的关系保持损失，它保持了特征映射之间的相对通道关系。我们表明，在改变图像样式的同时，所提出的关系保持损失也有助于缩小域间距，从域适应的角度进一步解释了所提出的方法。我们的方法的一些结果如图1所示。我们观察到，即使在无源设置下，来自两个模型的知识仍然可以推动从目标样式到源样式的样式转换（SFIT降低颜色饱和度并使背景变白以模仿看不见的源样式）。在几个基准[19,36,39,38]上，我们表明，从提议的SFIT方法生成的图像显著减少了两个模型之间的性能差距，表明成功地提取了适应的知识。此外，当我们在同一数据集上使用不同的UDA方法时，我们发现SFIT在不同程度上传递了图像样式。这进一步验证了SFIT可视化对模型的忠实性，以及不同的UDA方法可以解决不同程度的风格差异。对于应用程序，我们表明生成的图像可以作为额外的线索，并支持进一步调整目标模型。这也属于UDA的严格设置，即源代码自由域适配（SFDA）[17、20、24]，其中系统无法访问源图像。

##### Image Style Transfer Using Convolutional Neural Networks

Introduction Transferring the style from one image onto another can be considered a problem of texture transfer. In texture transfer the goal is to synthesise a texture from a source image while constraining the texture synthesis in order to preserve the semantic content of a target image. For texture synthesis there exist a large range of powerful non-parametric algorithms that can synthesise photorealistic natural textures by resampling the pixels of a given source texture [7, 30, 8, 20]. Most previous texture transfer algorithms rely on these nonparametric methods for texture synthesis while using different ways to preserve the structure of the target image. For instance, Efros and Freeman introduce a correspondence map that includes features of the target image such as image intensity to constrain the texture synthesis procedure [8]. Hertzman et al. use image analogies to transfer the texture from an already stylised image onto a target image[13]. Ashikhmin focuses on transferring the high-frequency texture information while preserving the coarse scale of the target image [1]. Lee et al. improve this algorithm by additionally informing the texture transfer with edge orientation information [22]. Although these algorithms achieve remarkable results, they all suffer from the same fundamental limitation: they use only low-level image features of the target image to inform the texture transfer. Ideally, however, a style transfer algorithm should be able to extract the semantic image content from the target image (e.g. the objects and the general scenery) and then inform a texture transfer procedure to render the semantic content of the target image in the style of the source image. Therefore, a fundamental prerequisite is to find image representations that independently model variations in the semantic image content and the style in which it is presented. Such factorised representations were previously achieved only for controlled subsets of natural images such as faces under different illumination conditions and characters in different font styles [29] or handwritten digits and house numbers [17]. To generally separate content from style in natural images is still an extremely difficult problem. However, the recent advance of Deep Convolutional Neural Networks [18] has produced powerful computer vision systems that learn to extract high-level semantic information from natural images. It was shown that Convolutional Neural Networks trained with sufficient labeled data on specific tasks such as object recognition learn to extract high-level image content in generic feature representations that generalise across datasets [6] and even to other visual information processing tasks [19, 4, 2, 9, 23], including texture recognition [5] and artistic style classification [15]. In this work we show how the generic feature representations learned by high-performing Convolutional Neural Networks can be used to independently process and manipulate the content and the style of natural images. We introduce A Neural Algorithm of Artistic Style, a new algorithm to perform image style transfer. Conceptually, it is a texture transfer algorithm that constrains a texture synthesis method by feature representations from state-of-the-art Convolutional Neural Networks. Since the texture model is also based on deep image representations, the style transfer method elegantly reduces to an optimisation problem within a single neural network. New images are generated by performing a pre-image search to match feature representations of example images. This general approach has been used before in the context of texture synthesis [12, 25, 10] and to improve the understanding of deep image representations [27, 24]. In fact, our style transfer algorithm combines a parametric texture model based on Convolutional Neural Networks [10] with a method to invert their image representations [24].

引言将样式从一个图像转移到另一个图像可以被视为纹理转移的问题。在纹理传输中，目标是从源图像合成纹理，同时约束纹理合成，以保留目标图像的语义内容。对于纹理合成，存在大量功能强大的非参数算法，可以通过对给定源纹理的像素重新采样来合成照片级真实感自然纹理[7,30,8,20]。大多数以前的纹理传输算法依赖于这些非参数方法进行纹理合成，同时使用不同的方法来保持目标图像的结构。例如，Efros和Freeman引入了一个对应贴图，其中包括目标图像的特征，如图像强度，以约束纹理合成过程[8]。Hertzman等人使用图像类比将纹理从已经定型的图像转移到目标图像上[13]。Ashikhmin专注于传输高频纹理信息，同时保留目标图像的粗略比例[1]。Lee等人通过向纹理转移添加边缘方向信息来改进该算法[22]。尽管这些算法取得了显著的效果，但它们都受到了相同的基本限制：它们仅使用目标图像的低级图像特征来通知纹理传输。然而，理想情况下，样式转换算法应该能够从目标图像（例如，对象和一般场景）中提取语义图像内容，然后通知纹理转换程序以源图像的样式呈现目标图像的语义内容。因此，一个基本的先决条件是找到能够独立模拟语义图像内容和呈现方式变化的图像表示。这种因式分解表示以前仅用于自然图像的受控子集，如不同照明条件下的人脸和不同字体样式的字符[29]或手写数字和门牌号[17]。在自然图像中，通常将内容与风格分离仍然是一个极其困难的问题。然而，深度卷积神经网络[18]的最新进展产生了强大的计算机视觉系统，可以学习从自然图像中提取高级语义信息。结果表明，在特定任务（如对象识别）上使用足够的标记数据训练的卷积神经网络可以学习在通用特征表示中提取高级图像内容，这些特征表示可在数据集[6]甚至其他视觉信息处理任务[19,4,2,9,23]中推广，包括纹理识别[5]和艺术风格分类[15]。在这项工作中，我们展示了如何使用高性能卷积神经网络学习的通用特征表示来独立处理和操纵自然图像的内容和样式。我们介绍了一种艺术风格的神经算法，一种执行图像风格转换的新算法。从概念上讲，它是一种纹理传递算法，通过来自最先进的卷积神经网络的特征表示来约束纹理合成方法。由于纹理模型也基于深度图像表示，因此样式转换方法优雅地简化为单个神经网络中的优化问题。通过执行图像前搜索以匹配示例图像的特征表示来生成新图像。在纹理合成[12,25,10]的背景下，这种通用方法曾被用于提高对深度图像表示的理解[27,24]。事实上，我们的风格转换算法将基于卷积神经网络的参数化纹理模型[10]与反转其图像表示的方法相结合[24]。



在深度学习时代，光学字符识别（OCR）系统的性能有了显著的提高。手写文本识别（HTR）尤其如此，每个作者都有自己独特的风格，而印刷文本的设计差异较小。也就是说，与其他任务一样，基于深度学习的HTR受到培训示例数量的限制。收集数据是一项具有挑战性且成本高昂的任务，更重要的是，接下来的标记任务，我们在此重点介绍。减少数据注释负担的一种可能方法是半监督学习。与完全监督方法相比，半监督方法除了使用标记数据外，还使用一些未标记样本来提高性能。因此，此类方法可适用于测试期间的不可见图像。我们提出了ScrabbleGAN，一种半监督的方法来合成手写文本图像，它在风格和词汇方面都是通用的。ScrabbleGAN依赖于一种新的生成模型，该模型可以生成任意长度的单词图像。我们将展示如何以半监督的方式操作我们的方法，并享受上述好处，例如相对于最先进的监督HTR的性能提升。此外，我们的生成器可以操纵生成的文本样式。这允许我们改变，例如，文本是否是草书，或者笔划有多细。

摘要目前最先进的脱机手写文本识别系统倾向于使用神经网络，因此需要对大量带注释的数据进行训练。为了部分满足这一要求，我们提出了一种基于生成对抗网络（GAN）的系统来生成手写单词的合成图像。

受在指定空间位置生成指定类别component相关文章的启发，设计了一个网络以bigGAN为骨干网用来扩充手写汉字数据集。



我们还通过添加用于文本识别的辅助网络来修改标准GAN。然后以对抗性损失和CTC损失的平衡组合对系统进行训练。





Fig. 2, illustrates the proposed TSB architecture. We formulate training in a self-supervised manner where we do not have target style supervision and only use the original style image; our framework is designed to supervise itself in seeking photo-realistic results. We do, however, assume to have the ground truth content of each word box when training (the text appearing in it). **During inference,** we take a single source style image and new content (i.e., a character string), and generate a new image in the source style with the target content. **We denote the** input style image **along with** its context by Is,c ∈ RH×W ×3 and its implicit style as s. The content string is denoted by c. As we explain in Sec. 3.1, we represent c using a synthetically rendered image, Iˆ s,c. Our entire framework consists of seven networks. We use a style encoder (Fs) and content encoder, (Fc), to convert the input style and content images to latent representations es and ec respectively (Sec. 3.1). Given es, our style mapping network, M , is used to produce multi-scale style representations, ws,i, which are then processed by a stylized text generator network, G (Sec. 3.2). Finally, our framework is trained using multiple losses which involve a style loss computed using a typeface classifier pre-trained on a limited set of synthetic fonts, C (Sec. 4.1), a content loss, which uses a pre-trained OCR recognition model, R (Sec. 4.2), and a discriminator, D (Sec. 4.4).

图2示出了所提议的TSB架构。我们以自我监督的方式制定培训，没有目标式监督，只使用原始式形象；我们的框架设计用于监督自身寻求照片真实感结果。然而，我们确实假设在训练时每个单词框都有基本的真实内容（其中出现的文本）。在推理过程中，我们获取单个源样式的图像和新内容（即字符串），并使用目标内容生成源样式的新图像。我们用Is，c表示输入样式图像及其上下文∈ RH×W×3及其作为s的隐式风格。内容字符串由c表示。正如我们在第二节中所解释的。3.1，我们使用合成渲染图像Iˆs，c表示c。我们的整个框架由七个网络组成。我们使用样式编码器（Fs）和内容编码器（Fc），将输入样式和内容图像分别转换为潜在表示形式es和ec（第3.1节）。给定es，我们的样式映射网络M用于生成多比例样式表示ws，i，然后由样式化文本生成器网络G处理（第3.2节）。最后，我们的框架使用多个损失进行训练，包括使用在有限的合成字体集上预先训练的字体分类器C（第4.1节）计算的样式损失、使用预先训练的OCR识别模型R（第4.2节）和鉴别器D（第4.4节）计算的内容损失。

Here, we present the details of our model. The model mainly consists of three modules of a Content Recognition Network C, a Style Inference Network S and a Character Generation Network G as illustrated in Fig. 2. The whole process can be separated as two stages — inference and generation. In the inference stage, we first learn to disentangle the latent features into content-related and style-related components based on the Content Recognition Network and Style Inference Network respectively. In the generation stage, we take the content and style vectors as inputs via a deconvolutional network, such that the stylized characters are well reconstructed with the style feature that is appropriately captured in the inference stage. The training process is executed by an intercross pair-wise way (See Sec. 2.3) for a reliable disentanglement.

在这里，我们展示了我们模型的细节。该模型主要由内容识别网络C、风格推理网络S和字符生成网络G三个模块组成，如图2所示。整个过程可分为推理和生成两个阶段。在推理阶段，我们首先学习分别基于内容识别网络和风格推理网络将潜在特征分解为与内容相关和与风格相关的成分。在生成阶段，我们通过反褶积网络将内容和风格向量作为输入，这样，风格化的角色就可以很好地利用在推理阶段适当捕获的风格特征进行重构。通过交叉成对方式（见第2.3节）执行训练过程，以实现可靠的解纠缠。



Architecture. Figure 2 shows the architecture of the proposed method. Given h, we render an image x through a given Chinese font, and then we encode x through an image encoder Ei to generate an image feature vector vi. At the same time, we consult a dictionary T to obtain h’s component sequence c to generate a component feature vector vc through a component encoder Ec. We convert the style label s of the reference image y to a one-hot vector vs. We concatenate vc, vi, and vs as an input feature vector used by an image decoder G to generate a calligraphy character image ˆ y.

图2显示了所提出方法的体系结构。给定h，我们通过给定的中文字体呈现图像x，然后通过图像编码器Ei对x进行编码以生成图像特征向量vi。同时，我们查阅字典T以获得h的分量序列c，通过分量编码器Ec生成分量特征向量vc。我们将参考图像y的样式标签s转换为一个热向量vs。我们连接vc、vi和vs作为输入特征向量，由图像解码器G用于生成书法字符图像y。
