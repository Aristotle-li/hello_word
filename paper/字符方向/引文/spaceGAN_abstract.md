### Abstract：



##### RD-GAN: Few/Zero-Shot Chinese Character Style Transfer via Radical Decomposition and Rendering

Abstract. Style transfer has attracted much interest owing to its various applications. Compared with English character or general artistic style transfer, Chinese character style transfer remains a challenge owing to the large size of the vocabulary(70224 characters in GB180102005) and the complexity of the structure. Recently some GAN-based methods were proposed for style transfer; however, they treated Chinese characters as a whole, ignoring the structures and radicals that compose characters. In this paper, a novel radical decomposition-andrendering-based GAN(RD-GAN) is proposed **to utilize the radical-level compositions** of Chinese characters and achieves few-shot/zero-shot Chinese character style transfer. The RD-GAN consists of three components: a radical extraction module (REM), radical rendering module (RRM), and multi-level discriminator (MLD). Experiments demonstrate that our method has a powerful few-shot/zero-shot generalization ability by using the radical-level compositions of Chinese characters.

摘要风格转换因其广泛的应用而引起了人们的广泛兴趣。与英文字符或一般艺术风格的转换相比，汉字的风格转换仍然是一个挑战，因为汉字的词汇量很大（GB180102005中有70224个字符），并且结构复杂。最近提出了一些基于GAN的风格转换方法；然而，他们将汉字视为一个整体，忽略了构成汉字的结构和部首。本文提出了一种新的基于部首分解和描述的GAN（RD-GAN），利用汉字的部首级成分，实现了少镜头/零镜头的汉字风格转换。RD-GAN由三个组件组成：根提取模块（REM）、根渲染模块（RRM）和多级鉴别器（MLD）。实验表明，该方法利用汉字的部首级成分，具有较强的少镜头/零镜头泛化能力。

##### SCFont: Structure-Guided Chinese Font Generation via Deep Stacked Networks

Abstract Automatic generation of Chinese fonts that consist of large numbers of glyphs with complicated structures is now still a challenging and ongoing problem in areas of AI and Computer Graphics (CG). Traditional CG-based methods typically rely heavily on manual interventions, while recentlypopularized deep learning-based end-to-end approaches often obtain synthesis results with incorrect structures and/or serious artifacts. To address those problems, this paper proposes a structure-guided Chinese font generation system, SCFont, by using deep stacked networks. The key idea is to integrate the domain knowledge of Chinese characters with deep generative networks to ensure that high-quality glyphs with correct structures can be synthesized. More specifically, we first apply a CNN model to learn how to transfer the writing trajectories with separated strokes in the reference font style into those in the target style. Then, we train another CNN model learning how to recover shape details on the contour for synthesized writing trajectories. Experimental results validate the superiority of the proposed SCFont compared to the state of the art in both visual and quantitative assessments.

摘要在人工智能和计算机图形学领域，由大量复杂结构的字形组成的汉字字体的自动生成仍然是一个具有挑战性的问题。传统的基于CG的方法通常严重依赖于人工干预，而最近普及的基于深度学习的端到端方法通常会获得结构不正确和/或严重伪影的合成结果。为了解决这些问题，本文提出了一种基于深度堆叠网络的结构导向汉字字体生成系统SCFont。其关键思想是将汉字领域知识与深层生成网络相结合，以确保合成具有正确结构的高质量字形。更具体地说，我们首先应用CNN模型来学习如何将参考字体样式中带有分隔笔划的书写轨迹转换为目标样式中的笔划轨迹。然后，我们训练另一个CNN模型，学习如何恢复合成书写轨迹轮廓上的形状细节。实验结果验证了所提出的SCFont与现有技术相比在视觉和定量评估方面的优越性。

##### Multi-scale multi-class conditional generative adversarial network for handwritten character generation

Abstract Handwritten character generation is a popular research topic with various applications. Various methods have been proposed in the literatures which are based on methods such as pattern recognition, machine learning, deep learning or others. However, seldom method could generate realistic and natural handwritten characters with a built-in determination mechanism to enhance the quality of generated image and make the observers unable to tell whether they are written by a person. To address these problems, in this paper, we proposed a novel generative adversarial network, multi-scale multi-class generative adversarial network (MSMC-CGAN). It is a neural network based on conditional generative adversarial network (CGAN), and it is designed for realistic multi-scale character generation. MSMC-CGAN combines the global and partial image information as condition, and the condition can also help us togenerate multi-class handwritten characters. Our model is designed with unique neural network structures, image features and training method. To validate the performance of our model, we utilized it in Chinese handwriting generation, and an evaluation method called mean opinion score (MOS) was used. The MOS results show that MSMC-CGAN achieved good performance.

手写体字符生成是一个具有广泛应用的热门研究课题。基于模式识别、机器学习、深度学习等方法，文献中提出了各种方法。然而，很少有方法能够通过内置的判断机制生成真实自然的手写字符，以提高生成图像的质量，并使观察者无法判断这些字符是否由人书写。针对这些问题，本文提出了一种新的生成性对抗网络——多尺度多类生成性对抗网络（MSMC-CGAN）。它是一种基于条件生成对抗网络（CGAN）的神经网络，设计用于真实的多尺度角色生成。MSMC-CGAN将全局和局部图像信息结合起来作为条件，该条件还可以帮助我们生成多类手写字符。我们的模型设计了独特的神经网络结构、图像特征和训练方法。为了验证该模型的性能，我们将其应用于中文笔迹生成，并使用了一种称为平均意见分数（MOS）的评估方法。MOS结果表明，MSMC-CGAN具有良好的性能。

##### Learning to Write Stylized Chinese Characters by Reading a Handful of Examples

Abstract Automatically writing stylized characters is an attractive yet challenging task, especially for Chinese characters with complex shapes and structures. Most current methods are restricted to generate stylized characters already present in the training set, but require to retrain the model when generating characters of new styles. In this paper, we develop a novel framework of Style-Aware Variational Auto-Encoder (SA-VAE), which disentangles the content-relevant and style-relevant components of a Chinese character feature with a novel intercross pair-wise optimization method. In this case, our method can generate Chinese characters flexibly by reading a few examples. Experiments demonstrate that our method has a powerful oneshot/few-shot generalization ability by inferring the style representation, which is the first attempt to learn to write new-style Chinese characters by observing only one or a few examples. 1 

摘要自动书写程式化汉字是一项极具吸引力但又极具挑战性的任务，特别是对于形状和结构复杂的汉字而言。大多数当前方法仅限于生成训练集中已存在的样式化字符，但在生成新样式的字符时需要重新训练模型。在本文中，我们开发了一种新的样式感知变分自动编码器（SA-VAE）框架，该框架采用一种新的交叉成对优化方法来分离汉字特征的内容相关和样式相关组件。在这种情况下，我们的方法可以通过阅读一些示例灵活地生成汉字。实验结果表明，该方法通过推断汉字的风格表征，具有很强的单次/少次泛化能力，这是第一次尝试通过观察一个或几个例子来学习书写新的汉字。

##### CalliGAN: Style and Structure-aware Chinese Calligraphy Character Generator

Abstract Chinese calligraphy is the writing of Chinese characters as an art form performed with brushes so Chinese characters are rich of shapes and details. Recent studies show that Chinese characters can be generated through image-toimage translation for multiple styles using a single model. We propose a novel method of this approach by incorporating Chinese characters’ component information into its model. We also propose an improved network to convert characters to their embedding space. Experiments show that the proposed method generates high-quality Chinese calligraphy characters over state-of-the-art methods measured through numerical evaluations and human subject studies.

摘要中国书法是用毛笔书写汉字的一种艺术形式，汉字具有丰富的形状和细节。最近的研究表明，汉字可以通过使用单个模型对多种样式进行图像到图像的翻译来生成。我们提出了一种新的方法，将汉字的成分信息融入到模型中。我们还提出了一种改进的网络来将字符转换为其嵌入空间。实验表明，该方法比通过数值评估和人体实验测量的最新方法生成高质量的中国书法字符。

##### ScrabbleGAN: Semi-Supervised Varying Length Handwritten Text Generation

Abstract Optical character recognition (OCR) systems performance have improved significantly in the deep learning era. This is especially true for handwritten text recognition (HTR), where each author has a unique style, unlike printed text, where the variation is smaller by design. That said, deep learning based HTR is limited, as in every other task, by the number of training examples. Gathering data is a challenging and costly task, and even more so, the labeling task that follows, of which we focus here. One possible approach to reduce the burden of data annotation is semisupervised learning. Semi supervised methods use, in addition to labeled data, some unlabeled samples to improve performance, compared to fully supervised ones. Consequently, such methods may adapt to unseen images during test time. We present ScrabbleGAN, a semi-supervised approach to synthesize handwritten text images that are versatile both in style and lexicon. ScrabbleGAN relies on a novel generative model which can generate images of words with an arbitrary length. We show how to operate our approach in a semi-supervised manner, enjoying the aforementioned benefits such as performance boost over state of the art supervised HTR. Furthermore, our generator can manipulate the resulting text style. This allows us to change, for instance, whether the text is cursive, or how thin is the pen stroke.

摘要在深度学习时代，光学字符识别（OCR）系统的性能有了显著的提高。手写文本识别（HTR）尤其如此，每个作者都有自己独特的风格，而印刷文本的设计差异较小。也就是说，与其他任务一样，基于深度学习的HTR受到培训示例数量的限制。收集数据是一项具有挑战性且成本高昂的任务，更重要的是，接下来的标记任务，我们在此重点介绍。减少数据注释负担的一种可能方法是半监督学习。与完全监督方法相比，半监督方法除了使用标记数据外，还使用一些未标记样本来提高性能。因此，此类方法可适用于测试期间的不可见图像。我们提出了ScrabbleGAN，一种半监督的方法来合成手写文本图像，它在风格和词汇方面都是通用的。ScrabbleGAN依赖于一种新的生成模型，该模型可以生成任意长度的单词图像。我们将展示如何以半监督的方式操作我们的方法，并享受上述好处，例如相对于最先进的监督HTR的性能提升。此外，我们的生成器可以操纵生成的文本样式。这允许我们改变，例如，文本是否是草书，或者笔划有多细。

##### DG-Font: Deformable Generative Networks for Unsupervised Font Generation

Abstract Font generation is a challenging problem especially for some writing systems that consist of a large number of characters and has attracted a lot of attention in recent years. However, existing methods for font generation are often in supervised learning. They require a large number of paired data, which is labor-intensive and expensive to collect. Besides, common image-to-image translation models often define style as the set of textures and colors, which cannot be directly applied to font generation. To address these problems, we propose novel deformable generative networks for unsupervised font generation (DGFont). We introduce a feature deformation skip connection (FDSC) which predicts pairs of displacement maps and employs the predicted maps to apply deformable convolution to the low-level feature maps from the content encoder. The outputs of FDSC are fed into a mixer to generate the final results. Taking advantage of FDSC, the mixer outputs a high-quality character with a complete structure. To further improve the quality of generated images, we use three deformable convolution layers in the content encoder to learn style-invariant feature representations. Experiments demonstrate that our model generates characters in higher quality than state-of-art methods. The source code is available at https://github.com/ecnuycxie/DG-Font.

摘要字体生成是一个具有挑战性的问题，特别是对于一些由大量字符组成的书写系统，近年来引起了人们的广泛关注。然而，现有的字体生成方法往往是监督学习。它们需要大量成对的数据，这是劳动密集型的，而且收集成本很高。此外，常见的图像到图像转换模型通常将样式定义为纹理和颜色的集合，这不能直接应用于字体生成。为了解决这些问题，我们提出了一种新的用于无监督字体生成（DGFont）的可变形生成网络。我们引入了一种特征变形跳跃连接（FDSC），它预测位移贴图对，并利用预测的贴图对来自内容编码器的低级特征贴图应用可变形卷积。FDSC的输出被送入混合器以产生最终结果。利用FDSC，混频器输出结构完整的高质量字符。为了进一步提高生成图像的质量，我们在内容编码器中使用三个可变形卷积层来学习样式不变的特征表示。实验表明，我们的模型生成的字符质量高于最先进的方法。源代码可在https://github.com/ecnuycxie/DG-Font.

##### Multiple Heads are Better than One: Few-shot Font Generation with Multiple Localized Experts

Abstract A few-shot font generation (FFG) method has to satisfy two objectives: the generated images should preserve the underlying global structure of the target character and present the diverse local reference style. Existing FFG methods aim to disentangle content and style either by extracting a universal representation style or extracting multiple component-wise style representations. However, previous methods either fail to capture diverse local styles or cannot be generalized to a character with unseen components, e.g., unseen language systems. To mitigate the issues, we propose a novel FFG method, named Multiple Localized Experts Few-shot Font Generation Network (MXFont). MX-Font extracts multiple style features not explicitly conditioned on component labels, but automatically by multiple experts to represent different local concepts, e.g., left-side sub-glyph. Owing to the multiple experts, MX-Font can capture diverse local concepts and show the generalizability to unseen languages. During training, we utilize component labels as weak supervision to guide each expert to be specialized for different local concepts. We formulate the component assign problem to each expert as the graph matching problem, and solve it by the Hungarian algorithm. We also employ the independence loss and the content-style adversarial loss to impose the contentstyle disentanglement. In our experiments, MX-Font outperforms previous state-of-the-art FFG methods in the Chinese generation and cross-lingual, e.g., Chinese to Korean, generation. Source code is available at https://github. com/clovaai/mxfont.

摘要少镜头字体生成（FFG）方法必须满足两个目标：生成的图像应保持目标字符的潜在全局结构，并呈现不同的局部参考样式。现有的FFG方法旨在通过提取通用表示样式或提取多个组件式样式表示来分离内容和样式。然而，以前的方法要么不能捕获不同的本地样式，要么不能推广到具有看不见的组件（例如，看不见的语言系统）的字符。为了缓解这些问题，我们提出了一种新的FFG方法，称为多本地化专家少镜头字体生成网络（MXFont）。MX Font提取多个样式特征，这些特征不受组件标签的明确限制，但由多个专家自动表示不同的局部概念，例如左侧子图示符。由于有多名专家，MX字体可以捕捉不同的本地概念，并显示出对未知语言的通用性。在培训期间，我们利用组件标签作为弱监督，指导每位专家专门处理不同的本地概念。我们将每个专家的组件分配问题描述为图匹配问题，并使用匈牙利算法进行求解。我们还采用了独立性损失和内容风格对抗性损失来实施内容风格分离。在我们的实验中，MX字体在中国一代和跨语言（如汉语到韩语）一代中的表现优于以前最先进的FFG方法。源代码可在https://github. com/clovaai/mxfont。

##### DCFont: An End-To-End Deep Chinese Font Generation System

ABSTRACT Building a complete personalized Chinese font library for an ordinary person is a tough task due to the existence of huge amounts of characters with complicated structures. Yet, existing automatic font generation methods still have many drawbacks. To address the problem, this paper proposes an end-to-end learning system, DCFont, to automatically generate the whole GB2312 font library that consists of 6763 Chinese characters from a small number (e.g., 775) of characters written by the user. Our system has two major advantages. On the one hand, the system works in an end-to-end manner, which means that human interventions during offline training and online generating periods are not required. On the other hand, a novel deep neural network architecture is designed to solve the font feature reconstruction and handwriting synthesis problems through adversarial training, which requires fewer input data but obtains more realistic and high-quality synthesis results compared to other deep learning based approaches. Experimental results verify the superiority of our method against the state of the art.

摘要由于汉字数量庞大、结构复杂，为普通人建立一个完整的个性化汉字字库是一项艰巨的任务。然而，现有的自动字体生成方法仍然存在许多缺点。为了解决这个问题，本文提出了一个端到端的学习系统DCFont，它可以从用户编写的少量字符（例如775个）中自动生成由6763个汉字组成的整个GB2312字体库。我们的系统有两大优势。一方面，该系统以端到端的方式工作，这意味着在离线培训和在线生成期间不需要人工干预。另一方面，设计了一种新的深度神经网络结构，通过对抗性训练解决字体特征重构和笔迹合成问题，与其他基于深度学习的方法相比，该结构需要更少的输入数据，但获得更真实、更高质量的合成结果。实验结果验证了我们的方法相对于现有技术的优越性。

##### W-Net: One-Shot Arbitrary-Style Chinese Character Generation with Deep Neural Networks

Abstract. Due to the huge category number, the sophisticated combinations of various strokes and radicals, and the free writing or printing styles, generating Chinese characters with diverse styles is always considered as a difficult task. In this paper, an efficient and generalized deep framework, namely, the W-Net, is introduced for the one-shot arbitrary-style Chinese character generation task. Specifically, given a single character (one-shot) with a specific style (e.g., a printed font or hand-writing style), the proposed W-Net model is capable of learning and generating any arbitrary characters sharing the style similar to the given single character. Such appealing property was rarely seen in the literature. We have compared the proposed W-Net framework to many other competitive methods. Experimental results showed the proposed method is significantly superior in the one-shot setting.

摘要由于种类繁多，各种笔画和部首的复杂组合，以及自由书写或印刷的风格，生成不同风格的汉字一直被认为是一项艰巨的任务。本文介绍了一种高效、通用的深度框架，即W-Net，用于一次性生成任意风格的汉字。具体而言，给定具有特定样式（例如打印字体或手写样式）的单个字符（一次），所提出的W-Net模型能够学习和生成与给定单个字符具有相似样式的任意字符。这种吸引人的特性在文献中很少见到。我们将所提出的W-Net框架与许多其他竞争方法进行了比较。实验结果表明，所提出的方法在一次性设置中具有明显的优越性。



##### Multi-Content GAN for Few-Shot Font Style Transfer

Abstract In this work, we focus on the challenge of taking partial observations of highly-stylized text and generalizing the observations to generate unobserved glyphs in the ornamented typeface. To generate a set of multi-content images following a consistent style from very few examples, we propose an endto-end stacked conditional GAN model considering content along channels and style along network layers. Our proposed network transfers the style of given glyphs to the contents of unseen ones, capturing highly stylized fonts found in the real-world such as those on movie posters or infographics. We seek to transfer both the typographic stylization (ex. serifs and ears) as well as the textual stylization (ex. color gradients and effects.) We base our experiments on our collected data set including 10,000 fonts with different styles and demonstrate effective generalization from a very small number of observed glyphs

摘要在这项工作中，我们将重点放在对高度程式化的文本进行部分观察，并对观察结果进行概括，从而在装饰字体中生成未被观察到的字形。为了从很少的示例中生成一组风格一致的多内容图像，我们提出了一种端到端的堆叠条件GAN模型，该模型考虑了沿通道的内容和沿网络层的风格。我们提议的网络将给定字形的样式转换为不可见字形的内容，捕获现实世界中的高度样式化字体，如电影海报或信息图形上的字体。我们试图转移排版风格（例如衬线和耳朵）以及文本风格（例如颜色渐变和效果）。我们的实验基于我们收集的数据集，包括10000种不同风格的字体，并通过观察到的极少量字形证明有效的泛化



##### Adversarial Generation of Handwritten Text Images Conditioned on Sequences

Abstract—State-of-the-art offline handwriting text recognition systems tend to use neural networks and therefore require a large amount of annotated data to be trained. In order to partially satisfy this requirement, we propose a system based on Generative Adversarial Networks (GAN) to produce synthetic images of handwritten words. We use bidirectional LSTM recurrent layers to get an embedding of the word to be rendered, and we feed it to the generator network. We also modify the standard GAN by adding an auxiliary network for text recognition. The system is then trained with a balanced combination of an adversarial loss and a CTC loss. Together, these extensions to GAN enable to control the textual content of the generated word images. We obtain realistic images on both French and Arabic datasets, and we show that integrating these synthetic images into the existing training data of a text recognition system can slightly enhance its performance.

摘要目前最先进的脱机手写文本识别系统倾向于使用神经网络，因此需要对大量带注释的数据进行训练。为了部分满足这一要求，我们提出了一种基于生成对抗网络（GAN）的系统来生成手写单词的合成图像。我们使用双向LSTM递归层来获得要渲染的单词的嵌入，并将其馈送到生成器网络。我们还通过添加用于文本识别的辅助网络来修改标准GAN。然后以对抗性损失和CTC损失的平衡组合对系统进行训练。总之，这些对GAN的扩展能够控制生成的单词图像的文本内容。我们在法语和阿拉伯语数据集上都获得了真实的图像，并且我们表明，将这些合成图像集成到文本识别系统的现有训练数据中可以略微提高其性能。



