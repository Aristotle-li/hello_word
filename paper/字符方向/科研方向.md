



ICIP、ICDAR、SPL、ICPR、



1、更加真实的部首级编码手写文本生成，用于数据增广提高手写文本的识别准确率，

研究难点

分类数量多、风格差异多、形近字多(GB2312-80标准)

针对脱机手写汉字识别中形近字难以识别问题,从两个方面进行改进。方法一,使用卷积神经网络加中心损失的方法对相似手写汉字进行识别。引入度量学习中中心损失函数到卷积神经网络,使用交叉熵损失及中心损失作为卷积神经网络的联合损失,使模型学习到更加具有鉴别能力的特征,减小同类样本之间的距离,增加不同类样本之间的距离。方法二,使用卷积神经网络加支持向量机的方式对相似手写汉字进行识别。将卷积神经网络当作一个特征提取器,使用卷积神经网络全连接层输出的特征向量训练支持向量机分类器,识别相似手写汉字。实验结果表明,使用联合损失函数的方法,及卷积神经网络加支持向量机的方式相对于单独使用卷积神经网络的方式,平均识别准确率能够提升2.68%,1.59%。

2、

3、二线小会

汉字有两万多个字符，而英文只有26个字母，对于汉字的生成若每个汉字独立的建模生成，则意味着要生成两万多类的目标，对于生成器而言是十分困难的，再加上很多汉字仅有细微的差别，要进行准确的生成更加困难，例如壁和璧这两个字仅仅是底部有些许不同，但是对于ocr来说十分容易错误识别，目前主流的汉字生成都是基于autoencode结构进行风格迁移，或者字体的改变的工作，该方法

优点：可以比较准确地生成目标汉字

缺点：对于没见过的汉字无法生成、对于不存在的汉字也无法生成

由autoencode结构将汉字解耦为内容和风格启发，我们将汉字拆解为component和structure两部分，通过将7000多常用汉字拆解为1000多component和30多structure，

优点：

* 使得生成器生成的类别大大减少，生成复杂字的细节更清晰。
* 不同作者的大量手写文本获取成本很高，可以很方便的扩充风格不同手写数据集。
* 可以生成自由组合的艺术字体，具备一定的商业价值。
* 可以生成不存在的汉字数据集，用来训练带有纠正能力的ocr。autoencode结构需要获得不存在字的海量图片才能得到大量风格各异的样本。



传统的非auto encode 的GAN对于生成汉字这种类别数目巨大，且空间结构复杂的特殊图像存在笔画缺失，断裂等现象。一个容易想到的思路就是讲汉字拆分开分别生成，通过这种思路，不仅可以降低生成目标的种类，还可以将需要生成的目标分别生成大大降低了难度。在查阅文章时发现将汉字拆分为拓扑结构和stroke或者component的汉字合成思路早已存在，。。。。。描述传统方法。。。。。。，区别传统汉字生成基于字根和部首，在深度时代，GAN对于简单的单字符生成能力表现很好，对于多component组成的生僻字由于样本数少，空间结构复杂的原因生成的效果不好，所以对于我们工作的汉字拆分并不是完全基于字根和部首，例如交和父 并没有将交拆为包含父的两个字符，因为GAN对于交生成效果很好，ocr对于这种字的识别效果也很好，我们的拆分原则是可以对容易混淆的字通过部件的拓扑结构的重新组合，生成汉字数据库中没有的字符，例如。。。手写字体中对于ocr 来说 驽和鹜是难以分辨的困难样本，对于这种情况一个基本思路是orc可以学到更多这两个字带有缓冲的边界，而不是仅仅学到类似SVM中的分割线，发生一点扰动





基于GAN的风格迁移任务一个基本思路是采用autoencode结构分别抽取两幅图片或者汉字图片的内容和风格，送到生成器进行合成。我们的工作是基于GAN的汉字生成合成，理论上可以采用auto encode 结构分别抽取汉字图片 日 和 月 的内容以及 好 的左右拓扑结构生成明这个字，但是

对于目前主流的字体生成任务，普遍采用auto encode 结构而言，高分辨率部分的跨链代表字符的底层的更加细节的信息，而低分辨率的部分更多提供的是字符结构的信息，一些工作通过引入更多的笔画或者偏旁等信息，可以实现更少样本的生成，这也很容易理解，如果总样本量更少，引入的先验信息越多生成的字符效果越好。但是这种结构对于手写文本的大规模扩充却无能为力，一个人的手写文本不仅具有字体的特征，还有整体偏上偏下和字与字之间习惯性距离大小不等等特征，所以对于手写文本的扩充是一项十分困难的任务，手写文本的扩充需要字符级别的编码，而文本行图片对于采用auto encode 结构无法生成字符级别的编码。

所有的基于auto encode 的文章，引入了笔画或结构骨架或部首，局部特征，全局特征等等，都是为了使用更少的字体样本，生成内容和风格更准确的目标字体，而对于生成风格多样的手写文本行却无能为力。





当前的场景文字识别和手写文字识别研究都在致力于提升识别的准确率，甚至可以将写错的单词和汉字通过上下文的联系，通过基于RNN结构的方法将其识别为应该正确的书写或打印的字，但是打印的标准字体和手写字体同样是可能存在错误的，特别是手写汉字或英文单词，在科技飞速发展的今天，人们的书写技能急速下滑，提笔忘字的现象十分常见，即便写出来的对应的文本也很有汉字可能是错的，对于这个问题在提高OCR识别准确率的同时，让其带有纠错或者拒识的能力是十分有必要的。



1855

scrabbleGAN：生成的模块使用了普通卷积，加入no first-layer看结果

没有使用反卷积生成，使用三维卷积！！！

ocr（内容监督器）：在scrabbleGAN中为了避免crnn的隐式建模的问题，放弃了采用rnn，直接采用cnn得到feature sequence，直接作出预测，相当于减弱了orc部件的隐事建模的能力。但是隐式建模的问题依然严重存在，特别是对与生成文字较为复杂的结构的网络，orc的隐式建模的缺陷明显存在，问题在于crnn是基于现实中文字识别提出的网络，对与现实中已经存在的文字，尽管拓扑结构各有不同但是人类已经默认该字符是符合规范的，所以对与ocr任务来说强大的隐式建模能力是及其有好处的，因为不需要考虑该字符是否符合人类的书写习惯，简而言之orc任务面对的是合理存在的一串字符，只需要识别正确即可，而生成任务对于存在的合理性还要加一个判断。所以对与所有直接将ocr迁移到生成模型中的任务都会存在这个问题。对与汉字文字生成这一特定任务，我们的思路是将汉字拆解成偏旁部首进行监督，这样可以有效避免由于汉字空间结构中某一部件的改变而引起的整体的变形。

使用两套字典和两套encode、decode结构。

Dict1：用来编码汉字，每个汉字得到唯一的数字编码text_encode1，embedding成8160维度向量，经过G，D，得到real/fake ，计算loss

Dict2：用来编码汉字，每个汉字得到不定长的拆字数字编码text_encode2，real/fake送入R中得到预测的数字编码（编码字典从dict2中获得）和text_encode2计算CTC_loss

为什么不使用text_encode2来生成文本，因为text_encode2的拆字缺失了空间的信息，经过实验发现，使用text_encode2会使得生成器发生混乱，例如回和吕两个字，text_encode2编码是一致的，送入同时送入生成器会使得性能下降，而对R这一部分可以使用text_encode2，例如回和吕两个字，若其中一个口生成效果不好，loss的反向传播会使得生成器学习到该如何降低loss。

同时使用整体和部分的监督信息，可能会更加好



而ocr本身就已经对于形状十分相似的两个字的识别准确率就十分低下，再加上ocr要在7000多汉字中计算概率最相似的汉字，字形相似的汉字数目很多，识别准确率进一步下降，使得GAN生成的较为复杂的汉字在内容上距离gt总是比较远，这是因为此时面对空间结构复杂的汉字，ocr作为内容监督器已经无法发挥足够好的作用，从而导致断笔，缺笔的问题。

此前的工作大多基于auto-encode架构+L1loss，生成的汉字效果十分不错，但是这一类结构的输入时图片，仍然需要encode对图片中汉字的空间结构进行细致的编码才可以生成比较好的汉字结构，由于这种结构大多只能生成单个汉字，对于手写的或者艺术类的连笔字就无能为力了。此前有工作对汉字的先验结构进行细致条件编码后作为生成器的条件，但是工作量巨大。

本篇工作反其道而行之，只需要对汉字进行简单的拆字，使得ocr可以进行更细节的监督就可以达到更好的效果。

这种方式能够取得更好的效果，主要有如下几个因素：

* 一些比较复杂的字体，样本数量十分稀少，目前的生成模型都是以单个汉字作为整体去生成，对于样本数量稀少的汉字生成效果十分不理想，例如土恒这个字，生成起来十分困难，而对该汉字进行左右结构的分别监督生成，就会有很好的性能。缺点是对于复杂的上下结构你任然无能为力。封土
* 另一个好处：每个基本字对应的样本变多了，能学到更一般的表达，提高泛化性能
* 还有一个好处：ocr的预测空间变小了，监督更加精确，例如 僵 和 缰等类似的结构，原本ocr对每个字进行预测，使得生成的样本总是无法学习到要在形旁上靠近gt，



由人类书写汉字的方式启发：人类在书写汉字的时候记忆的往往不是细致到笔画的结构，而是component的组合，例如肋这个字，人类能记住这个字，并且能书写下来是因为这个字在人类的记忆和书写习惯中这个字是有月和力组成的，而且是左右结构。这样一个汉字就自然书写而成了。由此启发，我们对汉字进行component编码和结构编码。

产生了一些问题，例如频繁出现在左右和上下结构中的部件，比如非、垂等字符，当单独出现的时候，效果不好，我们为其加入了带有attention机制的maskloss，使其能关注到需要关注的汉字部件区域，而不会相互干扰。





关键：编码

1、笔画+结构+ID编码生成

2、若1可以，且效果基本收敛，将部首结构编码送入一个网络，可学习的编码，在生成器的中间层cat部首和结构信息





D也可以对内容做一下监督



v1：加入linear，降维到128，汉字失败

V1.1：加入linear，降维到256，汉字失败

V2.0：在G中加入可训练的embedding，汉字失败

V2.1：

* 在G中加入可训练的embedding，降维到128，训练原始英文数据集===失败

* 去掉liner，使用one-hot+乘法===成功

* 去掉liner，使用one-hot-2和cat([z,y])===训练失败。

* 去掉liner，使用one-hot+cat([z,y])===失败

* 去掉liner，使用one-hot-32和cat([z,y])==成功，猜测，one-hot-k的表达能力是关键，每个字符使用多少位编码

* 加上liner(81,128)，使用one-hot-32和cat([z,y])==成功，但是拟合不太好，D可以拟合，但是ocr无法拟合，分析是可训练的内容编码和噪声混合，使得内容训练无法拟合。

* 放弃one-hot，加入embedding(81,81)，使用乘法，修改代码scrabbleGAN，246行，区别是从81维一位是1，其他是0，变成了可学习的81维向量==失败，使用了乘法，内容编码和噪声彻底混合，使得风格和内容都无法拟合

* 放弃one-hot，加入embedding(81,81)，使用cat，修改代码scrabbleGAN，246行，区别是从[1,0,0....0]，变成了可学习的81维向量== 成功，按理说embedding和one-hoe+liner同样会失败，可是embedding却成功了，而且比原始的scrabbleGAN和fix_embedding效果更好！！！

  ：猜测是embedding后的向量更容易学到字符和字符之间的相关程度，比如中和甲，嵌入后的向量会比较相似，基于此猜测，使用基于笔画的编码方式会不会取得更好的效果？？？？

* 放弃one-hot，加入embedding(81,128)，使用cat，修改代码scrabbleGAN，246行，区别是从[1,0,0....0]，变成了可学习的81维向量== 成功

V3.0：

* 放弃one-hot，加入固定权重的embedding(81,81)，使用cat，修改代码scrabbleGAN，246行，区别是从81维一位是1，其他是0，变成了可学习的81维向量== 成功，说明在cat噪声之前，固定单个字符的嵌入，有助于收敛
* 放弃one-hot，加入固定权重的embedding(81,128)，使用cat，修改代码scrabbleGAN，246行== 成功
* 放弃one-hot，加入固定权重的embedding(81,8160)，使用cat，修改代码scrabbleGAN，246行== 成功



1000-character_128：

* one_hot+cat==失败
* embedding1000-8160+cat==失败

依然是中文不拟合

* embedding1000-8160+cat+D+D_loss and D_loss_fake>0.1，G=1e-3 D=1e-5 ==卡在局部极小，不再更新D
* fix_embedding1000-8160+cat+D+D_loss >0.1,and D_loss_fake>0.02，G=1e-3 D=1e-5 ==卡在局部极小，不再更新D
* fix_embedding1000-8160+cat+D+D_loss >0.4，G=1e-3 D=1e-5 ==



* embedding1000-8160+cat+D+D_loss>0.1，G=1e-4 D=1e-5 ==卡在局部极小，不再更新D



1000-character_32：

* embedding1000-8160+cat+== 失败，D_loss=0，判别器太强了



cat之前学习编码还是cat之后学习编码？哪个更好？或者不用编码，直接固定那个好？

1、去掉h = liner(z)，embedding(81，8192)，z = embedding(32,8192)，再cat

1、去掉h = liner(z)，fix_embedding(81，8192)，z = embedding(32,8192)，再cat





对原始scrabbleGAN修改

1、做可训练的embedding==失败

2、

书法GAN

1、去掉可训练的embedding：也可以

2、去掉可训练的embedding，去掉D的labels_emded：也可以





大方向：

文本图像生成：GAN、VAE

生成+OCR

### 小目标：

* 检测到笔迹后提取风格，生成其它内容风格相同的手写汉字（带有连笔的更真实的汉语句子）
* 还可以将风格继续解耦，笔迹的结构，粗细，倾斜，不同字的大小不一致等。比如生成相似而又不同风格文字的生成
* 甚至笔迹的风格迁移和融合，比如多人书写笔迹的融合

Scrabble GAN+textstylebrush+zi2ziz+刘师兄

1、跑通scrabbleGAN，看代码结构，文章，SAGAN，技巧

2、结合textstylebrush+zi2ziz+刘师兄修改代码，把英文的生成改成汉字。网络结构textstylebrush为主干，G替换为scrabbleGAN中的逐字符生成。



## 单字符生成

风格迁移、可控

字形：楷体，宋体，隶书，书法！！！墓碑，碑文，颜真卿，毛笔字

纹理（比如艺术字）：颜色，光影，倾斜，

==将汉字的偏旁部首建模成图的node，将偏旁部首的关系建模成edge，全连接图，为每个偏旁部首编码作为node的特征==

人为什么能识别出一段草书的每一个字，是因为不仅语言中字与字之间的相关的，字的偏旁部首也是相关的。

## 序列生成

1、做数据增广：目的内容准确，风格越多越好。无监督的、半监督的

my idea：

* G:transformerGAN
* D、R合并，使用triplet loss
* 聚合的attention机制，local+hierarchical



2、风格迁移：1万字手写体学出来某人的风格，不仅要学单字符的风格，还要学字符之间的相关笔迹



3、小样本：只有几个字体，几段话的手写体，如何大规模生成，把笔画拆分提取风格



4、自己的idea：LSTM、RNN、transformer等等去预处理序列，再按照单字符的方法去处理。



方向：边看边coding

paper：看开源代码的paper

code：模块



11月：CVPR直接冲



https://github.com/kaonashi-tyc/zi2zi
https://github.com/amzn/convolutional-handwriting-gan
这是单字符生成和序列字符生成的两份工作，效果都很好，可以直接在此基础上改



## 情感迁移





## 文字纹理效果  

 参考：http://www.cnblogs.com/xiaohuochai/p/7498757.html

1、渐变色纹理

```
<style>
.box-with-text { background-image: linear-gradient(135deg,hsl(50, 100%, 70%), hsl(320, 100%, 50%)); -webkit-text-fill-color: transparent; -webkit-background-clip: text; background-size: cover;font:bolder 100px/100px Impact;position:absolute;}
</style>
<div class="box-with-text">match</div>
```

![img](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/img993883-20171102175244498-1898550956.png)

2、背景图片纹理

```
<style>
.box-with-text { background-image: url(http://7xpdkf.com1.z0.glb.clouddn.com/runjs/img/leaf.jpg); -webkit-text-fill-color: transparent; -webkit-background-clip: text; background-size: cover;font:bolder 100px/100px Impact;position:absolute;}
</style>
<div class="box-with-text">match</div>
```

 

![img](https://images2017.cnblogs.com/blog/993883/201711/993883-20171102175321420-268303406.png)

3、gif背景图片纹理

```
<style>
.box-with-text { background: url(http://7xpdkf.com1.z0.glb.clouddn.com/runjs/img/fire.gif) 0 -130px /cover; -webkit-text-fill-color: transparent; -webkit-background-clip: text; font:bolder 100px/100px Impact;position:absolute;}
</style>
<div class="box-with-text">match</div>
```

![img](https://images2017.cnblogs.com/blog/993883/201711/993883-20171102175548857-389928413.png)

 

4、通过[animation](http://www.cnblogs.com/xiaohuochai/p/5391663.html)移动背景的位置和尺寸来添加动画

```
<style>
@keyframes stripes {100% {background-position: 0 -50px;}}
.box-with-text {animation: stripes 2s linear infinite;background:linear-gradient(crimson 50%, #aaa 50%) 0 0/ 100% 50px ; -webkit-text-fill-color: transparent; -webkit-background-clip: text; font:bolder 100px/100px Impact;position:absolute;}
</style>
<div class="box-with-text">match</div>
```

![img](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimg993883-20171102175538873-812140223.png)

 



资料

[CVPR 2020 论文大盘点-文本图像篇](https://bbs.cvmart.net/articles/2778)

[自然场景文本检测识别技术综述](https://zhuanlan.zhihu.com/p/38655369)

[解耦表示学习](https://www.aminer.cn/topic/601fbf1392c7f9be21266c41?f=cs)

[CVF Sponsored Conferences](https://openaccess.thecvf.com/menu)

训练GAN技巧

[ICCV 2017：训练GAN的16个技巧，2400+星（PPT](https://blog.csdn.net/amds123/article/details/78388247)

[谱归一化_SN-GANs](https://zhuanlan.zhihu.com/p/65549312)

[Lipschitz Continuity](https://kaizhao.net//posts/spectral-norm)

[GAN--提升GAN训练的技巧汇总 ](http://www.dataguru.cn/article-14458-1.html)

[GAN训练技巧汇总，这个博主还有其他文章不错](https://www.cnblogs.com/qizhou/p/13742186.html) 

[本文译自medium文章Keep Calm and train a GAN. Pitfalls and Tips on training Generative Adversarial Networks](https://zhuanlan.zhihu.com/p/74663048)

[Keep Calm and train a GAN. Pitfalls and Tips on training Generative Adversarial Networks](https://medium.com/@utk.is.here/keep-calm-and-train-a-gan-pitfalls-and-tips-on-training-generative-adversarial-networks-edd529764aa9)

[GAN训练心得](https://blog.csdn.net/q1242027878/article/details/84452902)



R:

[详解CTC](https://zhuanlan.zhihu.com/p/42719047)

[带你看懂CTC算法](https://zhuanlan.zhihu.com/p/161186907)

[Sequence ModelingWith CTC](https://distill.pub/2017/ctc/)

[CRNN——pytorch + wrap_ctc编译，实现pytorch](http://www.cxyzjd.com/article/ft_sunshine/90300938)

[OCR：CRNN+CTC开源加详细解析](https://www.ycc.idv.tw/crnn-ctc.html)

[加入attention的crnn ](https://blog.csdn.net/u014453898/article/details/104784212)

[一文读懂CRNN+CTC文字识别](https://zhuanlan.zhihu.com/p/43534801)

[CTPN结合CNN与LSTM深度网络，能有效的检测出复杂场景的横向分布的文字](https://zhuanlan.zhihu.com/p/34757009)

[CRNN](https://blog.csdn.net/rabbithui/article/details/78831299)

G/D:

[Transformer构建一个GAN](https://finance.sina.com.cn/tech/2021-02-17/doc-ikftssap6178219.shtml)

[SAGAN](https://www.oukohou.wang/2019/07/03/SAGAN/)

[GraphGAN](https://zhuanlan.zhihu.com/p/47622003)

[ScrabbleGAN](https://towardsdatascience.com/scrabblegan-adversarial-generation-of-handwritten-text-images-628f8edcfeed)

[BigGAN简介](https://xiaosean5408.medium.com/biggan%E7%B0%A1%E4%BB%8B-large-scale-gan-training-for-high-fidelity-natural-image-synthesis-df349a5f811c)

[BigGAN 简介 BigGAN 大生成对抗网络](https://machinelearningmastery.com/a-gentle-introduction-to-the-biggan/)

[史上最强GAN图像生成器—BigGAN](https://zhuanlan.zhihu.com/p/46581611)

[【必读】生成对抗网络GAN论文TOP10 ](https://www.sohu.com/a/303754825_756411)

github开源代码：

[cw2vec](https://github.com/ShelsonCao/cw2vec)

[ TransGA  ](https://github.com/Aristotle-li/TransGAN)

[rewrite](https://github.com/kaonashi-tyc/Rewrite)

[zi2zi](https://github.com/kaonashi-tyc/zi2zi)

[ScrabbleGAN](https://github.com/amzn/convolutional-handwriting-gan)

[My Text in Your Handwriting](http://visual.cs.ucl.ac.uk/pubs/handwriting/)

GAN模型量化评价：

[FID](https://www.jiqizhixin.com/articles/2019-10-14-13)

[FID](https://machinelearningmastery.com/how-to-implement-the-frechet-inception-distance-fid-from-scratch/)

汉字编码：

[cw2vec](https://bamtercelboo.github.io/2018/05/11/cw2vec/)

[Glyce](https://blog.csdn.net/cskywit/article/details/86712070)

[深入解析Glyce](https://zhuanlan.zhihu.com/p/56141067)

[cw2vec](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736364&idx=4&sn=919d7b0f7fdda88443c3a4b246c83c1e&scene=0#wechat_redirect&rd2werd=1#wechat_redirect)

解耦：

[最新最全论文合集——解耦表示学习](https://blog.csdn.net/AI_Conf/article/details/113765458?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-6.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-6.control)

[基于解耦表示的Image-to-Image Translation](https://zhuanlan.zhihu.com/p/70402066)

相关：

[基于GAN的字体风格迁移](https://www.sohu.com/a/227138454_500659)



目的：

目前所有的手写文字生成的同一字符都是一样的，但是书法中的汉字不是如此，会针对不同文字组成词语，成语的不同，位置不同表现出不同的写法和意境。

生成汉字的连笔字，学习书法中的意境！！！

1、手写汉字数据集的扩充：

行草字体的生成有很多实现，但是真实人在手写汉字的时候，字与字之间会有连笔，字和字并不是孤立的存在的，会: 18254734032和前后产生关联，而汉字的字形相比于字母更加丰富且含有丰富的语义信息，结合多种汉字编码方式解决内容空间的问题。使用逐字符生成的方式，使得既保持了字与字之间的关联，使得整篇文章的生成成为了可能。

小目标：四个字成语，诗词，古文风格的生成，各种字体，毛笔字，艺术字，书法生成，单字符的生成割裂了字与字之间的关联，使得书法失去了美感，书法中不同文字的大小样式并不相同，同一文字临近词语的不同导致写法也不同，其大小比例每位书法家都有自己的对美的理解。

1step：G，类条件GAN

2step：creat_dataset

3step：D、R

1、ccbn

2、mdb文件

3、base_options  ： 文件的命名规则

4、attention

5、初始化：networks：apply ，

6、strLabConvert：初始化将字母表编码成数字，然后索引单词每个字母的数字，返回一组数字list和单词的长度，这里是重点要改的地方！！！

7、mode.set_input：读取label，执行netconvert.encode，将label转化为encode后的one-hot向量。这里也要改

8、G的前向传播，取lable汉字不需要大小写改变

9、scrabbleGAN的G只处理了数据，得到z和编码后的单词对应各个类别(既各个字母)的one-hot向量作为类条件c，送入bigGAN的G

10、bigGAN的G

11、实际的z2、z3、z4并没有使用

12、R只用lable的数据训练，不使用没有lable的真实图像，也不用生成的图像

13、单个汉字的生成由于训练样本太少，难以学出风格多样的书写形式，且十分容易过拟合，对单个汉字进行编码，而不采用汉字级别的注释(既lable)，可以使用规模巨大的目标检测领域的数据集，从而生成风格更加多样化的手写文本，由于我们并不是逐个字符生成的，甚至我们可以生成狂草体，既一句话一笔写成的文体。

2、笔迹模仿

思路：

1、编码，得到生成汉字的embedding：cw2vec、Glyce、bert.encode



![image-20210812103306347](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20210812103306347.png)

<img src="https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgDA5AD4C2D00ABFF033CFDE2D921DD001.jpg" alt="DA5AD4C2D00ABFF033CFDE2D921DD001" style="zoom: 67%;" />

2、结合二者结构，以scrabble为主体，加入textstylebrush风格化编码

textstylebrush：

<img src="https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimg7AA354ACE46DE4ECC2A415677ECEE5DA.jpg" alt="7AA354ACE46DE4ECC2A415677ECEE5DA"  />

scrabble：

![image-20210812103427011](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20210812103427011.png)

![image-20210812104310983](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20210812104310983.png)

zi2zi：

![image-20210812104316653](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20210812104316653.png)

font-GAN

![image-20210812112820438](https://xiaoguciu.oss-cn-beijing.aliyuncs.com/imgimage-20210812112820438.png)



书法GAN的缺点，仍然是单个字体的生成，书法之所以成为书法艺术，不仅仅是单个字符书写的堆砌，还有字与字之间的关联和相互作用才构成了一幅书法的美感，我们所要做的就是学习这种书法家的创造力，单个字体书写只是书法的基本功，想成为书法家还需要有对全局，对所要书写一句话整体的排列布局的把握，才能构成一幅传世的书法，特别是草书的艺术更是如此。



几乎所有的文字生成网络都集中在单个字体的生成领域，风格的提取仅限于构建笔画之间的粗细，拉伸，倾斜等风格，而对于手写字体和艺术字体中，字体之间的关系没有风格化的能力。

笔迹模仿的难点在于个人的书写习惯中，同一个字在不同的语句位置字体的大小和风格不同，整体文字书写的偏移也是不同的，而普通的文字GAN无法做到这类风格的提取。因为这类风格与单个字符的风格同时提取往往会构成矛盾，使得风格提取网络混淆，而且目前的文字生成网络对于任意长文本的生成还没有很好地解决，本文提出了一种莲花-net的网络，通过将要生成的文本逐个字符编码，就像莲花的每个花瓣作为一个encode去分别编码单个字符，在最后一层和中间层分别按照文本顺序concat送到decode生成长文本，通过这种方式，莲花-net可以生成任意长的文本，同时依据风格提取网络对长文本进行风格化，得到最终的艺术字或者草书书法字体。





a.repeat(M，N，Q)，dim=0迭代M次，以此类推

torch.repeat.interleve(y,num,dim)，在dim维度复制num次



 embedding(10001,8160)  

y = (b,n,8160)  z = (b,n,32)

Cat(y,z)

