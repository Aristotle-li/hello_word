![一文详解神经网络结构搜索（NAS）](https://pic3.zhimg.com/v2-5ee088c6cda59cbcc985d592023ecab1_1440w.jpg?source=172ae18b)

# 一文详解神经网络结构搜索（NAS）

[![AI研习社](https://pic2.zhimg.com/v2-778bc39a3033dc22b07795519356ed1a_xs.jpg?source=172ae18b)](https://www.zhihu.com/people/aiyan-xi-she)

[AI研习社](https://www.zhihu.com/people/aiyan-xi-she)

yanxishe.com，专注AI开发者和学术青年求知求职

91 人赞同了该文章

本文作者为东北大学自然语言处理实验室 2018 级研究生胡驰，他的研究方向包括神经网络结构搜索、自然语言处理。雷锋网 AI 科技评论经作者授权发表本文章。

近年来，深度学习的繁荣，尤其是神经网络的发展，颠覆了传统机器学习特征工程的时代，将人工智能的浪潮推到了历史最高点。然而，尽管各种神经网络模型层出不穷，但往往模型性能越高，对超参数的要求也越来越严格，稍有不同就无法复现论文的结果。而网络结构作为一种特殊的超参数，在深度学习整个环节中扮演着举足轻重的角色。在图像分类任务上大放异彩的 ResNet、在机器翻译任务上称霸的 Transformer 等网络结构无一不来自专家的精心设计。这些精细的网络结构的背后是深刻的理论研究和大量广泛的实验，这无疑给人们带来了新的挑战。

### **1. 经典的NAS方法**

正如蒸汽机逐渐被电机取代一般，神经网络结构的设计，正在从手工设计转型为机器自动设计。这一进程的标志事件发生在2016年，Google发表论文Neural Architecture Search with Reinforcement Learning，他们使用强化学习进行神经网络结构搜索（NAS），并在图像分类和语言建模任务上超越了此前手工设计的网络。如图1所示，经典的NAS方法使用RNN作为控制器（controller）产生子网络（child network），再对子网络进行训练和评估，得到其网络性能（如准确率），最后更新控制器的参数。然而，子网络的性能是不可导的，我们无法直接对控制器进行优化，幸好有强化学习这一利器，学者们采用了策略梯度的方法直接更新控制器参数。

![img](https://pic2.zhimg.com/v2-6725787d379af7f059889e55c1534c39_r.jpg)图1 经典NAS方法概览图

经典的NAS方法形式简单，并且取得了令人瞩目的效果，例如:在PTB语言建模任务上，NAS搜索出来的RNN模型击败了当时最先进的RHN网络，在测试集上取得了62.4的PPL（困惑度，越低越好）。然而受限于其离散优化的本质，这类方法有一个致命的缺点：太耗费计算资源了！例如，在CIFAR-10这么一个小数据集上进行搜索就需要800张GPU计算3到4周，受限于当时的深度学习框架，该论文甚至专门提出了基于参数服务器的分布式训练框架。如此巨大的算力需求实在是令人望洋兴叹，那有没有办法加速搜索，让NAS变得亲民呢？首先我们来思考一下NAS为何如此耗时，在NAS中，为了充分挖掘每个子网络的“潜力”，控制器每次采样一个子网络，都要初始化其网络权重从头训练，那每次采样不重新初始化是不是就能大大减少训练时间？为此，后面有人提出了ENAS，即Efficient NAS，顾名思义，其目的就是提高NAS的搜索效率。ENAS将搜索空间表示为一个有向无环图（DAG），其中的任一子图都代表了一个网络结构，每个节点代表了局部的计算，如矩阵乘法，而节点间的有向连接代表了信息的流动。所谓的权重共享，也就是不同的网络结构共享整个有向无环图节点上的参数。如图2所示，其中左边是一个有向无环图，假设红色的连接被控制器选中，我们就可以将其转换为右边的网络结构，其中包含4个计算节点，而输入输出是固定的节点，此外激活函数也是控制器选择出来的。ENAS提出的权重共享，极大地减少了搜索时间，使用一张GTX1080Ti只需10小时就可以完成在CIFAR-10上的搜索。

![img](https://pic2.zhimg.com/v2-9c97575b1f33883098efad94e553fa61_r.jpg)图2 ENAS中的有向无环图和对应的网络结构

### **2. NAS的发展现状**

在NAS、ENAS提出后，相关论文如同雨后春笋一般出现，神经网络结构搜索已经成为了一大研究潮流。虽然方法层出不穷，但基本都包括这三大部分：1. 定义搜索空间； 2. 执行搜索策略采样网络； 3. 对采样的网络进行性能评估。接下来我们从这三个方面介绍NAS的发展现状。

**搜索空间**

搜索空间，即待搜索网络结构的候选集合。搜索空间大致分为全局搜索空间和基于细胞的搜索空间，前者代表搜索整个网络结构，后者只搜索一些小的结构，通过堆叠、拼接的方法组合成完整的大网络。如图3（a）所示，早期的NAS的搜索空间是链式结构，搜索的内容只是网络的层数、每层的类型和对应的超参数。而后受到ResNet等网络的启发，跳跃连接、分支结构也被引入了搜索空间中，如图3（b）所示。搜索空间的复杂程度决定了网络结构的潜力，最近的一些工作表明，精心设计的搜索空间可以大大提高网络性能的下限，换言之，在这些空间里进行随机搜索也能取得不错的效果。目前最先进的方法都得益于其适当的搜索空间，而且几乎都是类似于图4中的细胞结构，既减少了搜索代价，也提高了结构的可迁移性。

![img](https://pic1.zhimg.com/v2-3f1c4c9f4d3cc5061cd325872f84e468_r.jpg)图3 全局搜索空间示意图

![img](https://pic1.zhimg.com/v2-94e852b047fb5345a73ef8c2dd50208c_r.jpg)图4 基于细胞的搜索空间示意图

**搜索策略**

搜索策略，即如何在搜索空间中进行选择，根据方法的不同，搜索策略大致分为三种。

\1. 基于强化学习的方法。强化学习被广泛应用于连续决策建模中，该方法通过智能体（agent）与环境交互，每次agent都会执行一些动作（action），并从环境中获得回馈，强化学习的目标就是让回馈最大化。NAS可以很自然地被建模为一个强化学习任务，最初的NAS使用RNN作为控制器来采样子网络，对子网络训练、评估后使用策略梯度方法更新RNN参数。这种方法简单可操作，易于理解和实现，然而基于策略梯度的优化效率是很低的，而且对子网络的采样优化会带来很大的变异性（策略梯度有时方差很大）。其实这也是无奈之举，RNN只能生成网络描述，因而无法通过模型的准确率直接对其进行优化。同样的策略也适用于各种其他的约束，如网络时延等各项衡量网络好坏的指标。

\2. 基于进化算法的方法。进化算法的由来已久，该方法受生物种群进化启发，通过选择、重组和变异这三种操作实现优化问题的求解。Google在2017年的论文Large-Scale Evolution of Image Classifiers首次将进化算法应用于NAS任务，并在图像分类任务上取得了不错的成绩。该方法首先对网络结构进行编码，维护结构的集合（种群），从种群中挑选结构训练并评估，留下高性能网络而淘汰低性能网络。接下来通过预设定的结构变异操作形成新的候选，通过训练和评估后加入种群中，迭代该过程直到满足终止条件（如达到最大迭代次数或变异后的网络性能不再上升）。后续的论文Regularized Evolution for Image Classifier Architecture Search对这一方法进行了改进，为候选结构引入年代的概念（aging），即将整个种群放在一个队列中，新加入一个元素，就移除掉队首的元素，这样使得进化更趋于年轻化，也取得了网络性能上的突破。

\3. 基于梯度的方法。前面的方法网络空间是离散的，它们都将NAS处理为黑盒优化问题，因而效率不尽人意。如果能将网络空间表示为连续分布，就能通过基于梯度的方法进行优化。CMU和Google的学者在DARTS: Differentiable Architecture Search一文中提出可微分结构搜索方法。该方法与ENAS相同，将网络空间表示为一个有向无环图，其关键是将节点连接和激活函数通过一种巧妙的表示组合成了一个矩阵，其中每个元素代表了连接和激活函数的权重，在搜索时使用了Softmax函数，这样就将搜索空间变成了连续空间，目标函数成为了可微函数。在搜索时，DARTS会遍历全部节点，使用节点上全部连接的加权进行计算，同时优化结构权重和网络权重。搜索结束后，选择权重最大的连接和激活函数，形成最终的网络，DARTS的整个搜索过程如图5所示。

![img](https://pic2.zhimg.com/v2-8cc1c65f92e829dbf2a381e0628bb08d_r.jpg)

图5 DARTS在搜索时不断优化结构权重，最终只保留一个子结构

此外中科大和微软发表的论文Neural Architecture Optimization中提出另一种基于梯度的方法，该方法基于经典的encode-decode框架。首先将网络结构映射到连续空间的表示（embedding），这个空间中的每一个点对应一个网络结构。在这个空间上可以定义准确率的预测函数，以它为目标函数进行基于梯度的优化，这样就可以对embedding进行优化。网络收敛后，再将这个表示映射回网络结构。这些方法与强化学习和进化算法相比，极大提高了搜索效率，在权重共享的情况下，单卡一天就能够完成搜索。

**性能评估**

性能评估，即在目标数据集上评估网络结构的性能好坏。上一节讨论的搜索策略旨在找到某些性能（如准确度）最高的网络，为了引导它们的搜索过程，这些策略需要考虑如何评判给定架构的性能高低。最简单的方法是在训练数据上训练每个子网络并评估其在测试数据上的表现，然而，从头训练这么多结构太过耗时。上面提到过，ENAS、DARTS和NAO都使用了权重共享来代替重新初始化，并大大加速了搜索过程。除此之外，还有别的方法吗？当然是有的，例如评估时使用数据集的一小部分、减少网络参数、训练更少的轮数或者预测网络训练的趋势等，这和充分的训练相比大大加快了速度，然而由于超参数的选择，这样会带来新的问题：我们无法公平地对比网络结构。例如，有的结构在训练早期性能突出，但最终不如其他的结构，这样就会错过最优的网络。

基于One-Shot的结构搜索是目前的主流方法，该方法将搜索空间定义为超级网络（supernet），全部网络结构都被包含其中。这个方法最显著的特征就是在一个过参数化的大网络中进行搜索，交替地训练网络权重和模型权重，最终只保留其中一个子结构，上面提到的DARTS和ENAS就是这一类方法的代表。该类方法的本质其实是对网络结构进行排序，然而不同的网络共享同一权重这一做法虽然大大提高搜索效率，却也带来了严重的偏置。显然，不同的神经网络不可能拥有相同的网络参数，在共享权重时，网络输出必定受到特定的激活函数和连接支配。ENAS和DARTS的搜索结果也反应了这一事实，如图6所示，其中ENAS搜索出来的激活函数全是ReLU和tanh，而DARTS搜索出来激活函数的几乎全是ReLU。此外，DARTS等方法在搜索时计算了全部的连接和激活函数，显存占用量很大，这也是它只能搜索较小的细胞结构的原因。

![img](https://pic2.zhimg.com/v2-05456311a59108565396e8d5db1a3a5d_r.jpg)

图6 ENAS（左）和DARTS（右）在PTB上搜索的RNN模型

最近的一些工作着眼于解决共享权重带来的偏置问题和超级图的高显存占用问题，并将新的搜索目标如网络延时、结构稀疏性引入NAS中。商汤研究院提出的随机神经网络结构搜索（SNAS）通过对NAS进行重新建模，从理论上绕过了基于强化学习的方法在完全延迟奖励中收敛速度慢的问题，直接通过梯度优化NAS的目标函数，保证了结果网络的网络参数可以直接使用。旷视研究院提出的Single Path One-Shot NAS与MIT学者提出的ProxylessNAS类似，都是基于One-Shot的方法，与DARTS相比，它们每次只探索一条或者两条网络路径，大大减少了显存消耗，从而可以搜索更大的网络。其中，SNAS将结构权重表示为一个连续且可分解的分布，而ProxylessNAS将二值化连接引入NAS中。这些方法的涌现还标志着NAS正在朝着多任务、多目标的方向前进。

### **3. NAS未来展望**

目前NAS搜索的网络都是比较简单的节点和激活函数的排列组合，尽管在一些任务上性能表现突出，但仍离不开繁琐的超参数选择。个人认为未来NAS技术的发展趋势有这几点：

\1. 网络设计自动化：真正做到把数据丢给机器，直接获得最优的模型，而不是依赖众多超参数。谷歌、阿里巴巴等巨头都早已推出了AutoML的云服务产品，实现了随机搜索、进化算法和网格搜索等方法，在一定程度上达到了超参数选择的自动化，但不能为网络设计带来新的灵感。

\2. 多目标搜索：根据不同任务，朝着多目标的方向继续前进。这和模型的应用场景是息息相关的，例如目前手机端的模型一般都是对特定模型进行剪枝或低精度化实现，而ProxylessNAS等方法将网络时延、稀疏性纳入搜索考虑的指标,提出了针对不同平台的结构搜索，使得这些工作逐渐转型为自动化设计。不难预见，未来还会有更多的NAS工作聚焦于其他的搜索目标。

\3. 大规模搜索：直接在大规模数据集上进行搜索，而不仅仅是在几个小型数据集上搜索、强化手工设计的网络。要实现这个目标，需要在搜索空间的定义、搜索策略的制定和性能评估方法的选择上取得关键性的突破。2019年初CMU学者的一篇论文Random Search and Reproducibility for Neural Architecture Search就给此前的各类NAS方法泼了一盆冷水，该论文证明了在使用权重共享的情况下，随机搜索可以打败ENAS、DARTS等一系列强劲的方法。

\4. 拓展应用领域：尽管NAS在图像分类、目标检测和语义分割等图像和视觉领域表现突出，但自然语言处理领域方面的工作寥寥无几，现有的方法主要集中在语言建模任务，而目前Transformer及其变种在语言模型的王者地位依然无人动摇。

**相关资料**

\1. Zoph, B., & Le, Q.V. (2017). Neural Architecture Search with Reinforcement Learning. ArXiv, abs/1611.01578.

\2. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., & Dean, J. (2018). Efficient Neural Architecture Search via Parameter Sharing. ArXiv, abs/1802.03268.

\3. Liu, H., Simonyan, K., & Yang, Y. (2019). DARTS: Differentiable Architecture Search. ArXiv, abs/1806.09055.

\4. Xie, S., Zheng, H., Liu, C., & Lin, L. (2019). SNAS: Stochastic Neural Architecture Search. ArXiv, abs/1812.09926.

\5. Guo, Z., Zhang, X., Mu, H., Heng, W., Liu, Z., Wei, Y., & Sun, J. (2019). Single Path One-Shot Neural Architecture Search with Uniform Sampling. ArXiv, abs/1904.00420.

\6. Cai, H., Zhu, L., & Han, S. (2019). ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware. ArXiv, abs/1812.00332.

\7. Real, E., Aggarwal, A., Huang, Y., & Le, Q.V. (2018). Regularized Evolution for Image Classifier Architecture Search. ArXiv, abs/1802.01548.

\8. Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Tan, J.Y., Le, Q.V., & Kurakin, A. (2017). Large-Scale Evolution of Image Classifiers. ICML.

\9. Wistuba, M., Rawat, A., & Pedapati, T. (2019). A Survey on Neural Architecture Search. ArXiv, abs/1905.01392.

\10. Li, L., & Talwalkar, A. (2019). Random Search and Reproducibility for Neural Architecture Search. ArXiv, abs/1902.07638.

**作者单位信息：**

东北大学自然语言处理实验室由姚天顺教授创建于 1980 年，现由朱靖波教授领导，长期从事计算语言学的相关研究工作，主要包括机器翻译、语言分析、文本挖掘等。

**Github项目推荐 | 不想让路人甲乱入你的镜头？或许它能帮你P掉他们**

本项目的相关论文入选CVPR 2019。

论文标题：《Fast Online Object Tracking and Segmentation: A Unifying Approach》《Deep Video Inpainting》(CVPR 2019)

只需绘制一个边界框，你就可以擦除想要删除的对象。

详情点击：[https://ai.yanxishe.com/page/blogDetail/13917](https://link.zhihu.com/?target=https%3A//ai.yanxishe.com/page/blogDetail/13917%3Ffrom%3Dzhihu)