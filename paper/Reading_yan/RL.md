采用时序差分法的强化学习可以分为两类，

一类是在线控制（On-policy Learning），即一直使用一个策略来更新价值函数和选择新的动作，代表就是Sarsa。

而另一类是离线控制（Off-policy Learning），会使用两个控制策略，一个策略用于选择新的动作，另一个策略用于更新价值函数，代表就是Q-Learning。





监督学习，无监督学习，半监督学习，主动学习的概念

"灼灼其华" 2019-12-18 11:39:17  1570  收藏 3
分类专栏： Machine Learning
版权
1、监督学习（supervised learning）
训练数据既有特征(feature)又有标签(label)，通过训练，让机器可以自己找到特征和标签之间的联系，在面对只有特征没有标签的数据时，可以判断出标签，即生成合适的函数将输入映射到输出。

2、无监督学习（unsupervised learning）
训练样本的标记信息未知，目标是通过对无标记训练样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础，此类学习任务中研究最多、应用最广的是"聚类" (clustering)

3、半监督学习（Semi-Supervised learning,SSL）
训练集同时包含有标记样本数据和未标记样本数据，不需要人工干预，让学习器不依赖外界交互、自动地利用未标记样本来提升学习性能，就是半监督学习。

3.1、自训练模型（纯半监督学习和直推学习的区别）

纯半监督学习与直推学习主要的区别在于学习后要取得好的泛化能力的范围不同。

纯半监督学习是一种归纳学习，可以对测试数据（训练过程中未观察到的数据）进行预测，也就是基于“开放世界”的假设。

直推学习仅仅对于训练数据中的未标记的数据能够进行标记，而模型不具备对测试样本进行泛化的能力，直推学习是基于“封闭世界”的假设。

对其进行数学化，假设我们有如下的数据集：其中训练集为X1+X2,其中X1是已标记的训练数据，而X2是未标记的训练数据，Xtesxt是未知的测试集，对于归纳学习而言，是可以在训练数据中进行学习，然后对Xtesxt进行测试的，而对于直推学习而言，是不能够在Xtesxt中进行测试，也就是只能在训练数据中使得在训练数据中的泛化能力达到最大，却不具有迁移的能力。

在直推学习中，知道测试数据是什么，也就是训练数据中的未标注的数据是最终用于测试的数据，学习的目的是在这些数据中取得最佳的泛化能力。在归纳学习中，不清楚自己的测试数据是什么，是具有可替换性的。

4、主动学习（active learning）
学习器能够主动选择包含信息量大的未标注的样例并将其交由专家进行标注，然后置入训练集进行训练，从而在训练集较小的情况下获得较高的分类正确率，这样可以有效的降低构建高性能分类器的代价。
学习器能够主动的提出一些标注请求，将一些经过筛选的数据交给专家进行标注。这个过程中最重要的是如何筛选数据进行标注。

4.1、主动学习的模型如下： 

A=(C,Q,S,L,U)

其中C指的是一个或者多个分类器，Q指的是一些查询函数，也就是一些查询算法，S指的是监督者，通常指的是专家，L是少量标记的样本，U指的是大量未被标记的额样本。学习者利用少量标记的样本L，通过一些基本的机器学习算法学习一个或者多个机器学习模型C，然后通过查询算法，按照查询算法查询出一批最有用的样本，交给专家，让专家进行标记，最后将新学到的标记数据加入到少量样本中，继续训练模型。一直循环，直到达到一个准则为止。流程如下所示：


4.2、主动学习与半监督学习的联系

二者都利用到了未标注的数据和已标注的数据，然后提高学习能力。只是二者的学习思想不同。

4.3、主动学习与半监督学习的区别

主动学习，在利用未标注数据的时候，是从未标注数据中找到最容易判断错误的样例来交由专家进行标注，这个过程是一个筛选差数据的过程，也是一个互动交互的过程，引入了额外的专家的知识。

半监督学习，尤其是对于自学习模型，对于未标注数据而言，是选择最不容易判断错误的样例来加入到已标注数据中，这个过程，是一个自动的过程，是筛选最好的数据的过程，然后不需要互动，不需要人工干预，基于自身对于未标记数据加以利用，来提高学习模型的泛化性能。

 

# 解说：机器学习、监督学习、非监督学习、强化学习、深度学习、迁移学习

demi 在 周六, 10/12/2019 - 16:35 提交

**机器学习（machine learning）**

机器学习的主要任务：

- 分类（classification）：将实例数据划分到合适的类别中。
- 回归（regression）：主要用于预测数值型数据。

机器学习可以分为三种形式：

- 监督学习（supervised learning）
- 非监督学习（unsupervised learning）
- 强化学习（reinforcement learning）

**监督学习（supervised learning）**

必须确定目标变量的值，以便机器学习算法可以发现特征和目标变量之间的关系。在监督学习中，给定一组数据，我们知道正确的输出结果应该是什么样子，并且知道在输入和输出之间有着一个特定的关系。 (包括：分类和回归)

训练样本 = 特征(feature) + 目标变量(label: 分类-离散值/回归-连续值)

**非监督学习（unsupervised learning）**

无监督学习，即在未加标签的数据中，试图找到隐藏的结构。数据没有类别信息，也没有给定的目标值。

非监督学习包括的类型：

- 聚类：将数据集分成由类似的对象组成多个类。
- 密度估计：通过样本分布的紧密程度，来估计与分组的相似性。

此外，无监督学习还可以减少数据特征的维度，以便我们可以使用二维或三维图形更加直观地展示数据信息。

**强化学习（reinforcement learning）**

强化学习，又称再励学习或者评价学习，也是机器学习的技术之一。

所谓强化学习就是智能系统从环境到行为映射的学习，以使奖励信号（强化信号）函数值最大，由于外部给出的信息很少，强化学习系统必须依靠自身的经历进行自我学习。通过这种学习获取知识，改进行动方案以适应环境。

强化学习最关键的三个因素：

- 状态
- 行为
- 环境奖励。

强化学习和深度学习的主要区别：

- 深度学习的训练样本是有标签的，强化学习的训练是没有标签的，它是通过环境给出的奖惩来学习
- 深度学习的学习过程是静态的，强化学习的学习过程是动态的。这里静态与动态的区别在于是否会与环境进行交互，深度学习是给什么样本就学什么，而强化学习是要和环境进行交互，再通过环境给出的奖惩来学习
- 深度学习解决的更多是感知问题，强化学习解决的主要是决策问题。因此有监督学习更像是五官，而强化学习更像大脑。

应用：

关于深度学习和强化学习的实例，最典型的莫过于谷歌的AlphaGo和AlphaZero两位了。AlphaGo是通过深度卷积神经网络，在训练了大约三千万组人类的下棋数据生成的模型，而AlphaZero使用强化学习的方式，通过自己和自己下棋的方式生成模型。而最终的实验结果也很让人震撼。AlphaGo击败了人类围棋顶尖高手，而AlphaZero击败了AlphaGo。

强化学习实际应用目前还较窄，主要包括AI游戏（如Atari），推荐系统，机器人控制相关（如Ng的无人机飞行）

**传统机器学习**

机器学习（ML）技术在预测中发挥了重要的作用，ML经历了多代的发展，形成了丰富的模型结构，例如：
  \-  线性回归
  \-  逻辑回归
  \-  决策树
  \-  支持向量机
  \-  贝叶斯模型
  \-  正则化模型
  \-  集成模型
  \-  神经网络

这些预测模型中的每一个都基于特定的算法结构，参数都是可调的。

训练预测模型涉及以下步骤：

- 选择一个模型结构（例如逻辑回归，随机森林等）。
- 用训练数据（特征和标签）输入模型。
- 学习算法将输出最优模型（即具有使训练错误最小化的特定参数的模型）。

每种模式都有自己的特点，在一些任务中表现不错，但在其他方面表现不佳。但总的来说，我们可以把它们分成低功耗（简单）模型和高功耗（复杂）模型。选择不同的模型是一个非常棘手的问题。

机器学习的主要障碍是特征工程这个步骤，特征工程要靠手动设计完成，需要大量领域专业知识，因此它成为当今大多数机器学习任务的主要瓶颈。

**深度学习 （deep learning）**

深度学习，也是一种机器学习的技术。最初的深度学习网络是利用神经网络来解决特征层分布的一种学习过程。

DNN（深度神经网络）可以将原始信号（例如RGB像素值）直接作为输入值，而不需要创建任何特定的输入特征。通过多层神经元（这就是为什么它被称为“深度”神经网络），DNN可以“自动”在每一层产生适当的特征，最后提供一个非常好的预测，极大地消除了寻找“特征工程”的麻烦。

DNN也演变成许多不同的网络拓扑结构，例如：

- CNN（卷积神经网络）
- RNN（递归神经网络）
- LSTM（长期短期记忆网络）
- GAN（生成对抗网络）

所有的这些被统称为深度学习（Deep Learning）。

**迁移学习 （transfer learning）**

迁移学习能够将适用于大数据的模型迁移到小数据上，作为小数据模型的训练起点，节约训练神经网络需要的大量计算和时间资源。

例如采用在计算机视觉挑战赛通过ImageNet数据（源数据）集训练出来的AlexNet 模型迁移应用到另一个新的数据集（目标数据集）重新进行训练（微调）。

主要步骤：

① 在源数据上训练一个神经网络。比如在 ImageNet 上训练出的 AlexNet 模型。

② 将模型的输出层改成适合目标数据的大小。
新加载的数据并不需要作1000个类别的分类任务，因此 AlextNet 的最后三层可以针对性按照新的分类问题重新调整。
例如：

- 降低之前训练网络的特征初始学习率，减缓迁移层的学习。
- 降低迁移学习设置过多的迭代训练次数，提高训练效率。
- 减小批（Mini Batch Size）的大小，降低内存使用率。

③ 将输出层的权重初始化成随机值，但其他层的权重保持跟原先训练好的权重一致。

④ 在目标数据集上开始训练。

## 机器学习中的几个概念区分





[![img](https://www.leiphone.com/uploads/new/avatar/05/05/14/82_avatar_pic_100_100.jpg?imageMogr2/thumbnail/!52x52r/gravity/Center/crop/52x52/quality/90)](https://www.yanxishe.com/center/myPage/5051482)

[王立鱼](https://www.yanxishe.com/center/myPage/5051482)

2019年02月13日

本文作者：水之心
原文链接：https://www.jianshu.com/p/429320f81d61

------

**监督学习**

- **监督学习**([supervised learning](https://en.wikipedia.org/wiki/Supervised_learning))：是[机器学习](https://zh.wikipedia.org/wiki/机器学习)中的一种方法，可以由训练数据中学习到或通过建立一个模式（函数 / learning model），并依此模式推测新的实例。训练数据是由输入数据（通常是向量）和预期输出所组成。函数的输出可以是一个连续的值（称为[回归分析](https://zh.wikipedia.org/wiki/迴歸分析)），或是预测一个分类标签（称作[分类](https://zh.wikipedia.org/wiki/分类)）。[[1\]](https://www.yanxishe.com/blogDetail/9988#fn1)

通俗的说就是：监督学习算法训练含有很多特征的带标签（label 或 target）的数据集，来对新的数据集的标签做出预测。我们把需要训练的数据集称为**训练集**（trainset），需要预测的数据集称为**测试集**。

我们可以将其形式化（n 表示样本个数）：
Input: trainset = \{ x_i, \; y_i \}_{i=1}^n
Output: 映射 f: x_i \mapsto y_i ,\;\; i \in \{1, \cdots, n\}
上面的 x_i 表示特征向量（由若干特征组成的向量），y_i 表示标签值。

# 无监督学习

- **无监督学习**([unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning))：已知数据不知道任何标签，按照一定的偏好，训练一个智能算法，将所有的数据映射到多个不同标签的过程。相对于有监督学习，无监督学习是一类比较困难的问题，所谓的按照一定的偏好，是比如特征空间距离最近，等人们认为属于一类的事物应具有的一些特点。举个例子，猪和鸵鸟混杂在一起，算法会测量高度，发现动物们主要集中在两个高度，一类动物身高一米左右，另一类动物身高半米左右，那么算法按照就近原则，75 厘米以上的就是高的那类也就是鸵鸟，矮的那类是第二类也就是猪，当然这里也会出现身材矮小的鸵鸟和身高爆表的猪会被错误的分类。

常见的非监督式学习是[数据聚类](https://zh.wikipedia.org/wiki/数据聚类)。在[人工神经网路](https://zh.wikipedia.org/wiki/人工神经网络)中，[生成对抗网络](https://zh.wikipedia.org/wiki/生成对抗网络)（GAN）、[自组织映射](https://zh.wikipedia.org/wiki/自组织映射)（SOM）和[适应性共振理论](https://www.yanxishe.com/blogDetail/9988)（ART）则是最常用的非监督式学习。

# [强化学习](https://zh.wikipedia.org/wiki/强化学习)

- **强化学习**(reinforcement learning)：智能算法在没有人为指导的情况下，通过不断的试错来提升任务性能的过程。“试错”的意思是还是有一个衡量标准，用棋类游戏举例，我们并不知道棋手下一步棋是对是错，不知道哪步棋是制胜的关键，但是我们知道结果是输还是赢，如果算法这样走最后的结果是胜利，那么算法就学习记忆，如果按照那样走最后输了，那么算法就学习以后不这样走。

> **强化学习**（英语：Reinforcement learning，简称RL）是[机器学习](https://zh.wikipedia.org/wiki/机器学习)中的一个领域，强调如何基于[环境](https://zh.wikipedia.org/wiki/环境)而行动，以取得最大化的预期利益。其灵感来源于心理学中的[行为主义](https://zh.wikipedia.org/wiki/行为主义)理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。这个方法具有普适性，因此在其他许多领域都有研究，例如[博弈论](https://zh.wikipedia.org/wiki/博弈论)、[控制论](https://zh.wikipedia.org/wiki/控制论)、[运筹学](https://zh.wikipedia.org/wiki/运筹学)、[信息论](https://zh.wikipedia.org/wiki/信息论)、仿真优化、[多主体系统学习](https://zh.wikipedia.org/wiki/多主体系统学习)、[群体智能](https://zh.wikipedia.org/wiki/群体智能)、[统计学](https://zh.wikipedia.org/wiki/统计学)以及[遗传算法](https://zh.wikipedia.org/wiki/遗传算法)。在运筹学和控制理论研究的语境下，强化学习被称作“近似动态规划”（approximate dynamic programming，ADP）。在[最优控制](https://zh.wikipedia.org/wiki/最优控制)理论中也有研究这个问题，虽然大部分的研究是关于最优解的存在和特性，并非是学习或者近似方面。在[经济学](https://zh.wikipedia.org/wiki/经济学)和[博弈论](https://zh.wikipedia.org/wiki/博弈论)中，强化学习被用来解释在[有限理性](https://zh.wikipedia.org/wiki/有限理性)的条件下如何出现平衡。

> 在机器学习问题中，环境通常被规范为[马可夫决策过程](https://zh.wikipedia.org/wiki/馬可夫決策過程)（MDP），所以许多强化学习算法在这种情况下使用[动态规划](https://zh.wikipedia.org/wiki/动态规划)技巧。传统的技术和强化学习算法的主要区别是，后者不需要关于MDP的知识，而且针对无法找到确切方法的大规模MDP。

> 强化学习和标准的[监督式学习](https://zh.wikipedia.org/wiki/監督式學習)之间的区别在于，它并不需要出现正确的输入/输出对，也不需要精确校正次优化的行为。强化学习更加专注于在线规划，需要在探索（在未知的领域）和遵从（现有知识）之间找到平衡。强化学习中的“探索-遵从”的交换，在 [Multi-armed bandit](https://zh.wikipedia.org/w/index.php?title=多臂老虎机&action=edit&redlink=1) 问题和有限MDP中研究得最多。

> 因此，强化学习对于包含长期反馈的问题比短期反馈的表现更好。它在许多问题上得到应用，包括[机器人控制](https://www.yanxishe.com/blogDetail/9988)、电梯调度、[电信](https://zh.wikipedia.org/wiki/电信)通讯、[双陆棋](https://zh.wikipedia.org/wiki/双陆棋)和[西洋跳棋](https://zh.wikipedia.org/wiki/西洋跳棋)。[[1\]](https://zh.wikipedia.org/wiki/强化学习#cite_note-1)

> 强化学习的强大能来源于两个方面：使用样本来优化行为，使用函数近似来描述复杂的环境。它们使得强化学习可以使用在以下的复杂环境中：
>
> - 模型的环境已知，且解析解不存在；
> - 仅仅给出环境的模拟模型（模拟优化方法的问题）[[2\]](https://zh.wikipedia.org/wiki/强化学习#cite_note-2)
> - 从环境中获取信息的唯一办法是和它互动。前两个问题可以被考虑为规划问题，而最后一个问题可以被认为是genuine learning问题。使用强化学习的方法，这两种规划问题都可以被转化为[机器学习](https://zh.wikipedia.org/wiki/机器学习)问题。

# 弱监督学习

- **弱监督学习**(weakly supervised learning)： 已知数据和其一一对应的弱标签，训练一个智能算法，将输入数据映射到一组更强的标签的过程。标签的强弱指的是标签蕴含的信息量的多少，比如相对于分割的标签来说，分类的标签就是弱标签，如果我们知道一幅图，告诉你图上有一只猪，然后需要你把猪在哪里，猪和背景的分界在哪里找出来，那么这就是一个已知若标签，去学习强标签的弱监督学习问题。

# 半监督学习

- **半监督学习**(semi supervised learning) ：已知数据和部分数据一一对应的标签，有一部分数据的标签未知，训练一个智能算法，学习已知标签和未知标签的数据，将输入数据映射到标签的过程。半监督通常是一个数据的标注非常困难，比如说医院的检查结果，医生也需要一段时间来判断健康与否，可能只有几组数据知道是健康还是非健康，其他的只有数据不知道是不是健康。那么通过有监督学习和无监督的结合的半监督学习就在这里发挥作用了。

半监督学习(Semi-Supervised Learning，SSL)是模式识别和[机器学习](http://lib.csdn.net/base/2)领域研究的重点问题，是监督学习与[**无监督学习**](http://baike.baidu.com/view/3552442.htm)相结合的一种学习方法。它主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类的问题。主要分为半监督分类，半监督回归，半监督聚类和半监督降维算法。

至于**直推学习，它与半监督学习一样不需要人工干预，**不同的是，直推学习**假设未标记的数据就是最终要用来测试的数据**，学习的目的就是**在这些数据上取得最佳泛化能力**。相对应的，半监督学习在学习时并不知道最终的测试用例是什么。

也就是说，直推学习其实类似于半监督学习的一个子问题，或者说是一个特殊化的半监督学习，所以也有人将其归为半监督学习。

## 纯半监督学习与直推学习的区别：

假设有如下的数据集，其中训练集为 X_L+X_U，测试集为 X_{test}，标记样本数目为 L，未标记样本数目为 U，L \ll U

- 标记样本 (X_L,Y_L)=\{(x_{1:L},\,y_{1:L})\}
- 未标记样本 X_U=\{x_{L+1:N}\}，训练时可用
- 测试样本 X_{test}=\{x_{N+1:}\}，只有在测试时才可以看到

**纯半监督学习**是一种**归纳学习**(inductive learning)，可以对测试样本X_{test} 进行预测。也即纯半监督学习是基于「开放世界」的假设。

**直推学习**是 transductive 学习，仅仅可以对未标记样本 X_U 进行标记，模型不具备对测试样本 X_{test} 进行泛化的能力。直推学习是基于「封闭世界」的假设。

直推学习假设未标记的数据就是最终要用来测试的数据，学习的目的就是在这些数据上取得最佳泛化能力。相对应的，纯半监督学习在学习时并不知道最终的测试用例是什么。

# 主动学习

**主动学习**指的是这样一种学习方法：有的时候，有类标的数据比较稀少而没有类标的数据是相当丰富的，但是对数据进行人工标注又非常昂贵，这时候，学习算法可以主动地提出一些标注请求，将一些经过筛选的数据提交给专家进行标注。这个筛选过程也就是主动学习主要研究的地方了。

## 主动学习与半监督学习的区别

主动学习的主动指的是主动提出标注请求，也就是说还需要一个外在的能够对其进行标注的实体(通常是相关人员)，即主动学习是交互进行的。其目标是使用尽量少的“查询”(query)来获得尽量好的性能。主动学习引入了额外的专家知识，用过与外界的交互来将部分未标记样本转变为有标记样本。

# 多示例学习

- **多示例学习**(multiple instance learning) ：已知包含多个数据的数据包和数据包的标签，训练智能算法，将数据包映射到标签的过程，在有的问题中也同时给出包内每个数据的标签。多事例学习引入了数据包的概念，比如说一段视频由很多张图组成，假如 1000 张，那么我们要判断视频里是否有猪出现，一张一张的标注每一帧是否有猪太耗时，所以人们看一遍说这个视频里有猪或者没猪，那么就得到了多示例学习的数据，1000 帧的数据不是每一个都有猪出现，只要有一帧有猪，那么我们就认为这个包是有猪的，所有的都没有猪，才是没有猪的，从这里面学习哪一段视频（1000 张）有猪哪一段视频没有就是多事例学习的问题。





