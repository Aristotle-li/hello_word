## smooth SCNN、ChebNet、GCN、STGCN

> 题目：Deep Convolutional Networks on Graph-Structured Data
>
> 作者：Yann LeCun  Joan Bruna

### motivation

SCNN提出的方法，对于图结构已知时

推广到图形结构未知的环境中。在这种情况下，学习图结构相当于估计相似度矩阵，其复杂性为O（N2）

每个特征图 g，我们需要卷积核通常被限制为具有较小的空间支持，与输入像素的数量 N 无关，这使模型能够学习与 N 无关的多个参数。 为了恢复类似的学习由于频谱域的复杂性，因此有必要将频谱乘法器的类别限制为对应于局部滤波器的那些。



### idea

1、

平滑核：$\mathcal{K}\in \mathbb{R}^{N\times N0} $

$w_g=\mathcal{K}\tilde{w}_g$

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210608165504101.png" alt="image-20210608165504101" style="zoom:50%;" />

2、

使用分层图聚类进行池化

3、

Graph Construction

* Unsupervised Graph Estimation

  $X=R^{N\times F}$ N是节点个数，F是特征维数

  构建图：计算两个特征之间的距离 $d(i,j)=||X_i-X_j||^2$ ，$X_i$是$X$ 的第$i$ 列，





> 题目：Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering
>
> 来源：NIPS 2016
>
> 作者：Michaël Defferrard Xavier Bresson Pierre Vandergheynst

### motivation

第一代GCN：SCNN存在计算复杂度高和无法保证局部链接的缺点，为了解决这一缺陷，设计了ChebNet。

- $\boldsymbol{f}$的Graph傅里叶变换：$\hat{\boldsymbol{f}} = \boldsymbol{\Phi}^T \boldsymbol{f}$
- 卷积核$\boldsymbol{h}$的Graph傅里叶变换为：$\hat{\boldsymbol{h}}=(\hat{h}_1,…,\hat{h}_n)$，其中，$\hat{h}_k=\langle h, \phi_k \rangle, k=1,2…,n$。实际上，$\hat{\boldsymbol{h}}=\boldsymbol{\Phi}^T \boldsymbol{h}$.
- 求图傅里叶变换向量$\hat{\boldsymbol{f}} \in \mathbb{R}^{N \times 1}$和$\hat{\boldsymbol{h}} \in \mathbb{R}^{N \times 1}$ 的element-wise乘积，等价于将$\hat{\boldsymbol{h}}$组织成对角矩阵的形式，即$diag[\hat{h}(\lambda_k)] \in \mathbb{R}^{N \times N}$，再求$diag[\hat{h}(\lambda_k)] $和$\hat{\boldsymbol{f}}$矩阵乘法。
- 求上述结果的逆傅里叶变换，即左乘$\boldsymbol{\Phi}$。

则：图上的卷积定义为：
$$
(\boldsymbol{f} * \boldsymbol{h})_\mathcal{G}=\boldsymbol{\Phi} \text{diag}[\hat{h}(\lambda_1),…,\hat{h}(\lambda_n)] \boldsymbol{\Phi}^T \boldsymbol{f} \tag{1}
\\为什么\hat h是\lambda 的函数？
\\因为\hat h 就是"频率域"上的滤波器，就是自变量"频率"的函数，假如h=1,那么h就是全通滤波器，f经过傅里叶变换和反傅里叶变换后没有改变
$$



### idea

为了解决上述问题(1、在谱域中定义的滤波器不是自然就是局部的，2、与图傅立叶基的 $O(n^2)$  乘法，平移计算成本很高)，首先回顾一下，**图傅里叶变换**是关于特征值(频率)的函数$F(\lambda_1), …,F(\lambda_n)$, 即，$F(\boldsymbol{\Lambda})$，因此可以将上述卷积核$\boldsymbol{g}_{\theta}$写作$\boldsymbol{g}_{\boldsymbol{\theta}}(\boldsymbol{\Lambda})$。接着，将$\boldsymbol{g}_{\boldsymbol{\theta}}(\boldsymbol{\Lambda})$定义成如下**k阶多项式**形式：
$$
\boldsymbol{g}_{\boldsymbol{\theta^{\prime}}}(\boldsymbol{\Lambda}) \approx \sum_{k=0}^K \theta_k^{\prime} \boldsymbol{\Lambda}^k \\
\boldsymbol{g}_{\boldsymbol{\theta^{\prime}}}(\boldsymbol{\Lambda})类比成频域滤波器，假如=e^x，那么级数就是1+x+\frac{x^2}{2}+\frac{x^3}{3}+\cdots,对应高通滤波器
$$
代入可以得到：
$$
\begin{aligned}
\boldsymbol{g}_{\boldsymbol{\theta^{\prime}}} * \boldsymbol{x} &\approx \boldsymbol{\Phi} \sum_{k=0}^K \theta_k^{\prime} \boldsymbol{\Lambda}^k \boldsymbol{\Phi}^T \boldsymbol{x} \\
&= \sum_{k=0}^K \theta_k^{\prime} (\boldsymbol{\Phi} \boldsymbol{\Lambda}^k \boldsymbol{\Phi}^T) \boldsymbol{x} \\
&= \sum_{k=0}^K \theta_k^{\prime} (\boldsymbol{\Phi} \boldsymbol{\Lambda}\boldsymbol{\Phi}^T)^k \boldsymbol{x} \\
& = \sum_{k=0}^K \theta_k^{\prime} \boldsymbol{L}^k \boldsymbol{x}
\end{aligned}
$$
上述推导第三步应用了特征分解的性质。

上式为第二代的GCN。不需要做特征分解了，直接对拉普拉斯矩阵进行变换。可以事先把$\boldsymbol{L}^{k}$计算出来，这样前向传播的时候，就只需要计算矩阵和相邻的乘法。复杂度为$O(Kn^2)$。如果使用稀疏矩阵（$L$比较稀疏）算法，复杂度为$O(k|E|)$.

那么上式是如何体现localization呢？我们知道，矩阵的$k$次方可以用于求连通性，即1个节点经过$k$步能否到达另一个顶点，矩阵$k$次方结果中对应元素非0的话可达，为0不可达。因此$L$矩阵的$k$次方的含义就是代表$\text{k-hop}$之内的节点。进一步，根据拉普拉斯算子的性质。可以证明，如果两个节点的最短路径大于$K$的话，那么$L^{K}$在相应位置的元素值为0。因此，实际上只利用到了节点的K-Localized信息。

另外，作者提到，可以引入切比雪夫展开式来近似$\boldsymbol{L}^k$，因为**任何k次多项式都可以使用切比雪夫展开式来近似**。(类比泰勒展开式对函数进行近似）。

引入切比雪夫多项式（Chebyshev polynomial) $T_k(x)$的$K$阶截断获得对$\boldsymbol{L}^k$的近似，进而获得对$\boldsymbol{g}_{\theta}(\boldsymbol{\Lambda})$的近似，来降低时间复杂度。
$$
\boldsymbol{g}_{\boldsymbol{\theta^{\prime}}}(\boldsymbol{\Lambda}) \approx \sum_{k=0}^K \theta_k^{\prime} T_k(\tilde{\boldsymbol{\Lambda}})
$$
其中，$\tilde{\boldsymbol{\Lambda}}=\frac{2}{\lambda_{max}}\boldsymbol{\Lambda}-\boldsymbol{I}_n$为经图拉普拉斯矩阵$L$的最大特征值（即谱半径）缩放后的特征向量矩阵（防止连乘爆炸）。$\boldsymbol{\theta}^{\prime} \in \mathbb{R}^{K}$表示一个**切比雪夫向量**，$\theta_k^{\prime}$是第$k$维分量。切比雪夫多项式$T_k(x)$使用递归的方式进行定义：$T_k(x)=2xT_{k-1}(x)-T_{k-2}(x)$，其中$T_0(x)=1,T_1(x)=x$。

此时，可以使用近似的$\boldsymbol{g}_{\boldsymbol{\theta^{\prime}}}$替换原来的$\boldsymbol{g}_{\theta}$，可以得到：
$$
\begin{aligned}
\boldsymbol{g}_{\boldsymbol{\theta^{\prime}}} * \boldsymbol{x} &\approx \boldsymbol{\Phi} \sum_{k=0}^K \theta_k^{\prime} T_k(\tilde{\boldsymbol{\Lambda}}) \boldsymbol{\Phi}^T \boldsymbol{x} \
&\approx \sum_{k=0}^K \theta_k^{\prime} (\boldsymbol{\Phi} T_k(\tilde{\boldsymbol{\Lambda}}) \boldsymbol{\Phi}^T) \boldsymbol{x} \\
&=\sum_{k=0}^K \theta_k^{\prime} T_k(\tilde{\boldsymbol{L}}) \boldsymbol{x}
\end{aligned}
$$
其中，$\tilde{\boldsymbol{L}}=\frac{2}{\lambda_{max}} \boldsymbol{L}- \boldsymbol{I}_n$。

因此有递归式，
$$
\bar x_k =T_k(\tilde L)x = 2\tilde L\bar x_{k-1}-\bar x_{k-2} \in \mathbb R^n  \\
\boldsymbol{y}_{output} = \sigma(\sum_{k=0}^K \theta_k^{\prime} T_k(\tilde{\boldsymbol{L}}) \boldsymbol{x})= \sigma(\sum_{k=0}^K \theta_k^{\prime} \bar x_k)= \sigma([\bar x_0 ,...,\bar x_{K-1} ]\theta)
$$
参数向量$\boldsymbol{\theta}^{\prime} \in \mathbb{R}^{k}$，需要通过反向传播学习。时间复杂度也是$O(K|E|)$。





> 题目：SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS
>
> 来源：ICLR 2017 
>
> 作者：Thomas N. Kipf Max Welling

### motivation

由CNN中$3\times 3$ 的卷积核启发，通过纵向堆叠卷积核来获得大的横向感受野，从而降低计算复杂度。

### idea

第三代的GCN对上式进一步简化，在图上半监督学习场景下，带标签的数据非常少， 为了避免模型过拟合，Kipf等人约束$θ=θ_0=-θ_1$来 降低模型参数，并对权重矩阵做归一化处理，最终得 到如下的一阶图卷积神经网络

- 取$K=1$，此时模型是1阶的first-order proximity。即每层卷积层只考虑了直接邻域，类似CNN中$3\times 3$ 的卷积核。

- 深度加深，宽度减小。即，若要建立多阶 proximity，只需要使用多个卷积层。

- 并加了参数的一些约束，如: $\lambda_{max}\approx2$，引入renormalization trick，大大简化了模型。

  具体推导，首先$K=1,\lambda_{max}=2$代入，

$$
\begin{aligned}
\boldsymbol{g}_{\boldsymbol{\theta^{\prime}}} * \boldsymbol{x} &\approx \theta_0^{\prime} \boldsymbol{x} + \theta_1^{\prime}(\boldsymbol{L}- \boldsymbol{I}_n) \boldsymbol{x} \\
&= \theta_0^{\prime} \boldsymbol{x} - \theta_1^{\prime}(\boldsymbol{D}^{-1/2} \boldsymbol{A} \boldsymbol{D}^{-1/2}) \boldsymbol{x}
\end{aligned}
$$

上述推导利用了归一化拉普拉斯矩阵$\boldsymbol{L}=\boldsymbol{D}^{-1/2}(\boldsymbol{D}-\boldsymbol{A})\boldsymbol{D}^{-1/2}=\boldsymbol{I_n}-\boldsymbol{D}^{-1/2} \boldsymbol{A} \boldsymbol{D}^{-1/2}$。此时只有两个参数，即每个卷积核只有2个参数，$\boldsymbol{W}$是邻接矩阵。

进一步简化，假设$\theta_0^{\prime}=-\theta_1^{\prime}$，则此时单个通道的单个卷积核参数只有1个$\theta$：
$$
\boldsymbol{g}_{\boldsymbol{\theta^{\prime}}} * \boldsymbol{x} = \theta(\boldsymbol{I_n} + \boldsymbol{D}^{-1/2} \boldsymbol{W} \boldsymbol{D}^{-1/2}) \boldsymbol{x}
$$

$\boldsymbol{I_n} + \boldsymbol{D}^{-1/2} \boldsymbol{W} \boldsymbol{D}^{-1/2}$谱半径$[0,2]$太大，使用renormalization trick
$$
\boldsymbol{I_n} + \boldsymbol{D}^{-1/2} \boldsymbol{A} \boldsymbol{D}^{-1/2} \rightarrow \tilde{\boldsymbol{D}}^{-1/2}\tilde{\boldsymbol{A}} \tilde{\boldsymbol{D}}^{-1/2}
$$
其中，$\tilde{\boldsymbol{A}}=\boldsymbol{A}+\boldsymbol{I}_n$(相当于加了**self-connection**，本来$\boldsymbol{W}$对角元素为0) , $\tilde{\boldsymbol{D}}_{i,i}=\sum_{j} \boldsymbol{\tilde{A}}_{ij}$。

则：
$$
\underbrace{\boldsymbol{g}_{\boldsymbol{\theta^{\prime}}} * \boldsymbol{x}}_{\mathbb{R}^{n \times 1}} = \theta(\underbrace{\tilde{\boldsymbol{D}}^{-1/2}\tilde{\boldsymbol{A}} \tilde{\boldsymbol{D}}^{-1/2}}_{\mathbb{R}^{n \times n}}) \underbrace{\boldsymbol{x}}_{\mathbb{R}^{n \times 1}}
$$

推广到**多通道**(单个节点的信息是向量，对比图像上3个通道RGB的值构成3维向量)和**多卷积核**(每个卷积核只有1个参数)，即，
$$
\boldsymbol{x} \in \mathbb{R}^{N \times 1} \rightarrow \boldsymbol{X} \in \mathbb{R}^{N \times C}
$$
其中，$N$是节点的**数量**，$C$是通道数，或者称作表示节点的**信息维度数。** $\boldsymbol{X}$是节点的feature矩阵。

相应的卷积核参数变化：
$$
\theta \in \mathbb{R} \rightarrow \boldsymbol{\Theta} \in \mathbb{R}^{C \times F}
$$
其中，$F$为卷积核数量。

则卷积结果写作矩阵形式如下：
$$
\underbrace{\boldsymbol{Z}}_{\mathbb{R}^{N \times F}} = \underbrace{\tilde{\boldsymbol{D}}^{-1/2}\tilde{\boldsymbol{A}} \tilde{\boldsymbol{D}}^{-1/2}}_{\mathbb{R}^{N \times N}} \underbrace{\boldsymbol{X}}_{\mathbb{R}^{N \times C}} \ \ \underbrace{\boldsymbol{\Theta}}_{\mathbb{R}^{C \times F}}
$$
最终得到的卷积结果$\boldsymbol{Z} \in \mathbb{R}^{N \times F}$。即，每个节点的卷积结果的维数等于卷积核数量。

上述操作可以叠加多层，对$\boldsymbol{Z}$激活一下，然后将激活后的$Z$作为下一层的节点的feature矩阵。



## 感悟

> 看到这里感觉十分离谱：一代GCN将图投影到了频率域，每一个频率代表的不再是具体的邻居，而是总体的链接情况，低频对应大的链接优良的团簇，但是面对海量图要进行奇异值分解计算量太大了，于是想出了用级数把频域函数分解，不再正交分解这样做确实降低了计算量，还用了切比雪夫不等式，但是这同样也破坏了从频域成分的分析，级数中的一次方代表邻居，二次方代表邻居的邻居，物理意义又变成时域的了，K=1，其实就是对应某种归一化的邻接矩阵的一次方，和x乘起来对应x邻居的信息。
>
> 总结：对频域滤波器级数分解，没有进行正交分解，和扩散图卷积和很相似，分别对应归一化拉普拉斯矩阵和随机游走拉普拉斯矩阵。



> 题目：Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting
>
> 来源：IJCAI-18
>
> 作者：Peking University, Beijing

### motivation

交通预测是一种典型的时间序列预测问题，即利用数据之中所蕴含的时间、空间信息来对未来的区域内不同道路的交通流量进行预测。



要进行交通预测，首先要定义交通预测所需的时空图。本文使用39000个传感器来收集数据，每个传感器构成图中的一个节点。每30s进行一次数据采样，每5min的数据聚合成一张图，每 ![[公式]](https://www.zhihu.com/equation?tex=M) 张图构成一条数据（时空图)，即求得在已知t-M+1到t时间点内的交通流量，去求t+1到t+H时间点的交通流量。如图1所示：



**![img](https://pic2.zhimg.com/80/v2-db67c5cc6196353becaa27f50b9f5919_1440w.jpg)**

图1 时空图

用$G_t$表示时空图，则第 t 个时间步的图定义为 ![[公式]](https://www.zhihu.com/equation?tex=G_t%3D%28V_t%2C+E%2C+W%29) 。![[公式]](https://www.zhihu.com/equation?tex=V_t) 表示第 t 张图上的点集，不同时间步之间点的数量不变，但每个节点的特征改变。表示边集，通过从属关系、方向、出发点——目的地构成有向图。 ![[公式]](https://www.zhihu.com/equation?tex=W) 表示邻接矩阵，不同时间步的 ![[公式]](https://www.zhihu.com/equation?tex=W) 不变：

![[公式]](https://www.zhihu.com/equation?tex=w_%7Bij%7D+%3D%5Cleft%5C%7B+%5Cbegin%7Baligned%7D+exp%28%5Cfrac%7Bd_%7Bij%5E2%7D+%7D%7B%CF%83%5E2%7D%29%EF%BC%8Ci%E2%89%A0j%E4%B8%94exp%28%5Cfrac%7Bd_%7Bij%5E2%7D+%7D%7B%CF%83%5E2%7D%29%E2%89%A5%CF%B5%5C%5C+0%EF%BC%8C%E5%85%B6%E4%BB%96%E6%83%85%E5%86%B5+%5Cend%7Baligned%7D%5Cright.)

其中 ![[公式]](https://www.zhihu.com/equation?tex=d_%7Bij%7D) 表示节点 ![[公式]](https://www.zhihu.com/equation?tex=i) 和节点 ![[公式]](https://www.zhihu.com/equation?tex=j) 之间的距离， ![[公式]](https://www.zhihu.com/equation?tex=%CF%83) 和 ![[公式]](https://www.zhihu.com/equation?tex=%CF%B5) 用于调整 ![[公式]](https://www.zhihu.com/equation?tex=W) 的分布和稀疏度，本文所用数据集分别设置为10和0.5。

我们可以把交通流预测问题看作一个时空序列问题，

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210615104803067.png" alt="image-20210615104803067" style="zoom:50%;" />


其中 ![[公式]](https://www.zhihu.com/equation?tex=v_t%E2%88%88R%5E%7Bn%C3%97C_i+%7D) 表示时间步 ![[公式]](https://www.zhihu.com/equation?tex=t) 时$n$  个观测点的观测向量（ ![[公式]](https://www.zhihu.com/equation?tex=C_i) 是数据的特征向量长度)。



数学和物理知识建模：由于使用算法复杂度太高、计算成本大、以及过多的对现实理想化的假设，基本上已经被抛弃。

传统统计学、机器学习方法：虽然可以在中短期交通流预测中实现较好的预测，但是无法再长期预测中达到较好的结果。

深度学习模型：传统CNN和LSTM（GRU）来分别学习空间和时间特征，然而问题就在于传统CNN只能处理标准的网格数据（例如图片），而LSTM因为是迭代训练容易发生误差积累，基于RNN的方法难以捕捉短时剧烈交通流的变化u，而且计算量大难训练。

通过时空图建模：时空图卷积网络，用于交通预测任务。该架构包含多个时空卷积块，是图卷积层和卷积序列学习层的组合，用于对空间和时间依赖性进行建模。纯粹采用卷积结构来设计神经网络进行时空预测，不但参数更少，训练的还更快了

### detail





STGCN网络整体架构如图2所示，网络输入是 ![[公式]](https://www.zhihu.com/equation?tex=M)个时间步的图的特征向量 ![[公式]](https://www.zhihu.com/equation?tex=X%E2%88%88R%5E%7BM%C3%97n%C3%97C_i+%7D) （论文中 ![[公式]](https://www.zhihu.com/equation?tex=C_i%3D1)）以及对应的邻接矩阵 ![[公式]](https://www.zhihu.com/equation?tex=W%E2%88%88R%5E%7Bn%C3%97n%7D)，经过两个时空卷积块和一个输出层，输出 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bv%7D%E2%88%88R%5En) 来预测第 ![[公式]](https://www.zhihu.com/equation?tex=t) 个时间步后某个时间步特征。

![img](https://pic3.zhimg.com/80/v2-030389b5592ad95cc19e3546ae70510e_1440w.jpg)

图2 图时空网络整体架构

### 1、时域卷积块

每个时空卷积块由两个时域卷积块和一个空域卷积块组成。其中时域卷积块如图2最右侧所示，每个节点处的输入 ![[公式]](https://www.zhihu.com/equation?tex=X%E2%88%88R%5E%7BM%C3%97C_i+%7D)，沿着时间维度进行一维卷积，卷积核 ![[公式]](https://www.zhihu.com/equation?tex=%CE%93%E2%88%88R%5E%7BK_t%C3%97C_i+%7D)，个数为 ![[公式]](https://www.zhihu.com/equation?tex=2C_o)，从而得到 ![[公式]](https://www.zhihu.com/equation?tex=%5BP+Q%5D%E2%88%88R%5E%7B%28M-K_t%2B1%29%C3%972C_o+%7D)(P, Q为通道数一半)。

然后进行 ![[公式]](https://www.zhihu.com/equation?tex=GLU)激活：

![[公式]](https://www.zhihu.com/equation?tex=%CE%93%2A_%CF%84+X%3DP%E2%8A%99%CF%83%28Q%29%E2%88%88R%5E%7B%28M-K_t%2B1%29%C3%97C_o+%7D)

其中，P, Q分别是GLU的门输入，$\odot$  表示哈达玛积（即元素对应相乘），sigmoid门$\sigma(Q)$ 控制当前状态的哪个输入P与时间序列中的组成结构和动态方差相关。这里GLU门以及作者论文中提到的时间卷积网络中的短路连接（residual connection）实际上都是为了防止梯度消失、而设计的，对于一张完整的时空图：输入 ![[公式]](https://www.zhihu.com/equation?tex=X%E2%88%88R%5E%7BM%C3%97n%C3%97C_i+%7D)，输出 ![[公式]](https://www.zhihu.com/equation?tex=Y%E2%88%88R%5E%7B%28M-K_t%2B1%29%C3%97n%C3%97C_o+%7D)。

### 2、空域卷积块

空域卷积在每个时间步的图上进行（不在时间步之间进行）。输入 ![[公式]](https://www.zhihu.com/equation?tex=X%E2%88%88R%5E%7Bn%C3%97C_i+%7D)，按照ChebGCN，输出

![[公式]](https://www.zhihu.com/equation?tex=Y%3D%5Csum_%7Bi%3D0%7D%5E%7BK-1%7D%CE%B8_i+T_i+%28%5Ctilde%7BL%7D%29X)

其中 ![[公式]](https://www.zhihu.com/equation?tex=T_i+%28x%29%3D2xT_%7Bi-1%7D+%28x%29-T_%7Bi-2%7D+%28x%29)， ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde+L%3D%5Cfrac%7B2L%7D%7B%CE%BB_%7Bmax%7D+%7D-I_n)， ![[公式]](https://www.zhihu.com/equation?tex=L%3DI_n-D%5E%7B-1%2F2%7D+AD%5E%7B-1%2F2%7D)， ![[公式]](https://www.zhihu.com/equation?tex=A) 是邻接矩阵，论文中 ![[公式]](https://www.zhihu.com/equation?tex=K%3D3) 。卷积核 ![[公式]](https://www.zhihu.com/equation?tex=%CE%98%E2%88%88R%5E%7BK%C3%97C_i+%7D)，个数为 ![[公式]](https://www.zhihu.com/equation?tex=C_o) 。

对于一张完整的时空图：输入 ![[公式]](https://www.zhihu.com/equation?tex=X%E2%88%88R%5E%7BM%C3%97n%C3%97C_i+%7D)，输出 ![[公式]](https://www.zhihu.com/equation?tex=Y%E2%88%88R%5E%7BM%C3%97n%C3%97C_o+%7D) 。

### 3、输出层

如图二所示，时空卷积块包括两个时间卷积层和一个空间卷积层。处于中间的空间卷积层，承接两个时间卷积层，可取得使空间状态能够从图卷积到时间卷积快速传播。这种网络结构也助于充分应用瓶颈策略（bottleneck strategy)来实现通过压缩通道C来进行规模缩放和特征压缩（目的是为了减少学习参数）。另外，每个时空块都使用层归一化（batchnormalization）抑制过拟合。
时空块的输入和输出都是三维张量。块的输入和输出通过下式计算：



其中，和是块的上下两个时间核，是图卷积的谱核，是激活函数。

根据时域卷积块的一维卷积，每经过一个时空卷积块，数据在时间维度的长度减小 ![[公式]](https://www.zhihu.com/equation?tex=2%28K_t-1%29) 。所以经过两个时空卷积块后，输出 ![[公式]](https://www.zhihu.com/equation?tex=Y%E2%88%88R%5E%7B%28M-4%28K_t-1%29%29%C3%97n%C3%97C_o+%7D) 。

输出层包括一个时域卷积层和一个全连接层，时域卷积层的卷积核大小 ![[公式]](https://www.zhihu.com/equation?tex=%CE%93%E2%88%88R%5E%7B%28M-4%28K_t-1%29%29%C3%97C_o%7D) ，个数为 ![[公式]](https://www.zhihu.com/equation?tex=C_o) ，将输出映射到 ![[公式]](https://www.zhihu.com/equation?tex=Z%E2%88%88R%5E%7Bn%C3%97C_o+%7D)。全连接层 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat+v+%3DZw%2Bb)，其中 ![[公式]](https://www.zhihu.com/equation?tex=w%E2%88%88R%5E%7BC_o+%7D)，于是输出 ![[公式]](https://www.zhihu.com/equation?tex=v+%CC%82%E2%88%88R%5En) 。

损失函数是预测值和真实值的距离度量：

![[公式]](https://www.zhihu.com/equation?tex=L%28v+%CC%82%3B+W_%CE%B8+%29%3D%E2%80%96v+%CC%82%28v_%7Bt-M%2B1%7D%2C+%E2%8B%AF%2Cv_t%2CW_%CE%B8+%29-v_%7Bt%2B1%7D+%E2%80%96%5E2)

其中 ![[公式]](https://www.zhihu.com/equation?tex=W_%CE%B8) 是所有可训练参数， ![[公式]](https://www.zhihu.com/equation?tex=%5Chat+v+) 是预测值， ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bt%2B1%7D) 是真实值。

