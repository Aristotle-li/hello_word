## node2vec、SDNE、SCNN、DCNN、GraphSAGE、GDC

> 题目：node2vec: Scalable Feature Learning for Networks
>
> 来源：SIGKDD 2016
>
> 作者：Tsinghua University

### motivation：

 1、eigendecomposition is expensive for large real-world networks.design an objective that seeks to preserve local neighborhoods of nodes.

2、node2vec是可并行的捕获局部关系，容易扩展到大型网络。

### idea：

node2vec生成了***“在一个d维特征空间中使得保留了节点的邻居节点的可能性最大化”***的特征表示



<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210531220503624.png" alt="image-20210531220503624" style="zoom:50%;" />

考虑一种灵活的算法，该算法可以学习遵守两个原则的节点表示：

学习将来自 the same network community的节点（u和s1）紧密嵌入在一起的表示的能力

学习the different network community中起相似作用的节点（u和s6）具有相似嵌入的表示的能力

有偏随机游动算法能够通过可调参数控制搜索空间实现这一点，这些游走可以有效地探索给定节点的不同邻域。

#### 优化目标：最大似然估计

设$f(u)$是将顶点 $u $映射为embedding向量的映射函数,对于图中每个顶点 $u$，定义 $N_s(u)$  为通过采样策略 $S$  采样出的顶点 $u$   的近邻顶点集合。

node2vec优化的目标是给定每个顶点条件下，令其近邻顶点（**如何定义近邻顶点很重要**）出现的概率最大。

$max_f\sum_{u\in V}log\ Pr(N_s(u)|f(u))$

为了将上述最优化问题可解，文章提出两个假设：

- 条件独立性假设

假设给定源顶点下，其近邻顶点出现的概率与近邻集合中其余顶点无关。

$Pr(N_s(u)|f(u))=\prod_{n_i\in N_s(u)}Pr(n_i|f(u))$

- 特征空间对称性假设

这里是说一个顶点作为源顶点和作为近邻顶点的时候**共享同一套embedding向量**。(对比LINE中的2阶相似度，一个顶点作为源点和近邻点的时候是拥有不同的embedding向量的) 在这个假设下，上述条件概率公式可表示为$Pr(n_i|f(u))=\frac{exp\ f(n_i)\cdot f(u)}{\sum_{v\in V}expf(u)\cdot f(v)}$

根据以上两个假设条件，最终的目标函数表示为 ：

文章中可能问题：$max_f \ \sum_{u\in V}[-logZ_u+\sum_{n_i\in N_s(u)}f(n_i)*f(u)]$

==修改后的公式：$max_f \ \sum_{u\in V}[-|N_s(u)|logZ_u+\sum_{n_i\in N_s(u)}f(n_i)*f(u)]$==

由于归一化因子 $Z_u=\sum_{n_i\in N_s(u)}exp\ f(n_i)\cdot f(u)$ 的计算代价高，所以采用负采样技术优化。由于网络不是线性的，不能像文本那样可以在连续词上使用滑动窗口来定义邻居的概念，node2vec提出了一个随机过程来抽样一个给定源节点$u$的多个不同邻居节点。节点$u$通过顶点抽样策略S生成的网络邻居节点$N_S(u)$不限制于直接邻居，而是根据不同的抽样策略S有着许多不同的结构。

#### 顶点序列采样策略（如何定义近邻顶点很重要）：

将源节点的邻域采样问题视为局部搜索的一种形式，比较不同的采样策略：

- **宽度优先抽样(Breadth-first Sampling (BFS))**：$N_S(u)$中的节点必须是源节点的直接邻居

  BFS 采样的节点更准确地反映了邻域的局部结构，使得嵌入与结构等效性密切对应（例如，基于bridges and hubs等的结构等价性可以通过观察每个节点的邻域来推断）。通过对附近节点的搜索深度限制，BFS实现了对每个节点的邻居进行了一次微观扫描。此外，在BFS中，采样邻域中的节点往往会重复多次。这一点很重要，因为它减少了描述1-hop nodes相对于源节点的分布的方差。（This is also important as it reduces the variance in characterizing the distribution of 1-hop nodes with respect the source node. 然而，对于给定的k，图中只有非常小的一部分被探索。

- **深度优先抽样(Depth-first Sampling (DFS))**：邻居节点由到源节点的距离逐渐增加的连续抽样组成

  在DFS中，采样的节点更准确地反映了邻域的宏观结构，使得嵌入与同态性的推断密切对应。DFS不仅需要推断网络中点对点的相关性存在，同时需要描述出这些相关性的准确特征。当我们有一个**抽样规模的限制**和**很大邻域去探索**时，这很难做到。其次，采样节点可能远离源节点会导致复杂的相关性，也可能导致抽样缺乏代表性。

* the paper：网络中节点的预测任务经常在两种相似性之间变换：同质性和结构等价，即内容相似性和结构相似性。在同质性假设下，高度连通并且属于相似的网络聚类应该被embedded在一起（图1中的节点和节点属于相同的网络群体）。然而在结构等价的假设下，在网络中有相似结构的节点应该被embedded在一起（图1中的节点和节点是它们对应群体的中心节点）。与同质性不同的是，结构性不强调连接，网络中离得很远的节点仍然有相同的网络角色。真实世界的网络可能是一些节点表现出同质性而另一些节点表现出结构性。

node2vec依然采用随机游走的方式获取顶点的近邻序列，不同的是node2vec采用的是一种介于BFS和DFS之间的有偏的随机游走。

给定当前顶点 $v$ ，访问下一个顶点 $x$  的概率为

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210601122954998.png" alt="image-20210601122954998" style="zoom:50%;" />

 $\pi_{vx}$是顶点 $v$ 和顶点 $x$  之间的未归一化转移概率，$Z$   是归一化常数，$c_0=u$。



node2vec引入两个超参数 $p$ 和 $q$ 来控制随机游走的策略，假设当前随机游走经过边 $(t,v)$到达顶点 $v$ 设 $\pi_{vx}=\alpha_{pq}(t,x)\cdot w_{vx}$ ， $w_{vx}$ 是顶点  $v$ 和 $x$ 之间的边权，

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210601123019554.png" alt="image-20210601123019554" style="zoom:50%;" />

![[公式]](https://www.zhihu.com/equation?tex=d_%7Btx%7D) 为顶点 $t$ 和顶点 $x$ 之间的最短路径距离。

下面讨论超参数 $p$ 和 $q$ 对游走策略的影响 

- Return parameter,p 

参数 $p$ 控制重复访问刚刚访问过的顶点的概率。 注意到 $p $ 仅作用于 $d_{tx}=0$ 的情况，而  $d_{tx}=0$ 表示顶点 $x$ 就是访问当前顶点 $v$ 之前刚刚访问过的顶点。 那么若 $p$ 较高，则访问刚刚访问过的顶点的概率会变低，反之变高。 

- In-out papameter,q 

$q$  控制着游走是向外还是向内，若 $q>1$ ，随机游走倾向于访问和 $t$ 接近的顶点(偏向BFS)。若 $q<1$，倾向于访问远离 $t$  的顶点(偏向DFS)。

下面的图描述的是当从 $t$ 访问到 $v$ 时，决定下一个访问顶点时每个顶点对应的 $\alpha$ 。 

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210601123121031.png" alt="image-20210601123121031" style="zoom:50%;" />



#### 算法

采样完顶点序列后，剩下的步骤就和deepwalk一样了，用word2vec去学习顶点的embedding向量。 值得注意的是node2vecWalk中不再是随机抽取邻接点，而是按概率抽取，node2vec采用了Alias算法进行顶点采样。



<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210601123248919.png" alt="image-20210601123248919" style="zoom:50%;" />

#### node2vecWalk

通过上面的伪代码可以看到，node2vec和deepwalk非常类似，主要区别在于顶点序列的采样策略不同，所以这里我们主要关注**node2vecWalk**的实现。

由于采样时需要考虑前面2步访问过的顶点，所以当访问序列中只有1个顶点时，直接使用当前顶点和邻居顶点之间的边权作为采样依据。 当序列多余2个顶点时，使用文章提到的有偏采样。

### 词句

1、**informative**, discriminating, and independent features 信息丰富的、区分性的和独立的特征

2、 The challenge in feature learning is defining an objective function, which **involves** a trade-off in balancing computational efficiency and predictive accuracy.

特征学习的挑战在于定义目标函数，这涉及平衡计算效率和预测准确性的权衡。

3、We also show how feature representations of individual nodes can **be extended to** pairs of nodes (i.e., edges). In order to generate feature representations of edges, we **compose** the learned feature representations of the individual nodes using simple binary operators. 

我们还展示了如何将单个节点的特征表示扩展到节点对（即边）。为了生成边的特征表示，我们使用简单的二元算子组合学习到的各个节点的特征表示。

4、Our experiments focus on two common prediction tasks in networks 我们的实验侧重于网络中的两个常见预测任务

5、We **experiment with** several real-world networks from diverse domains, such as social networks, information networks, as well as networks from systems biology.

我们对来自不同领域的几个真实世界的网络进行了实验，例如社交网络、信息网络以及系统生物学的网络。

6、node2vec **outperforms** state-of-the-art methods **by up to** 26.7% on multi-label classification and up to 12.6% on link prediction.

node2vec 在多标签分类上比最先进的方法高出 26.7%，在链接预测上高出 12.6%。

7、We show how node2vec **is in accordance with** **established principles** in network science, providing flexibility in discovering representations **conforming to** different equivalences.我们展示了 node2vec 如何符合网络科学中的既定原则，为发现符合不同等价的表示提供灵活性。

8、We **empirically evaluate** node2vec for multi-label classification and link prediction on several real-world datasets.我们凭经验评估 node2vec 在几个真实世界数据集上的多标签分类和链接预测。

9、conventional paradigm 常规范式  In terms of computational efficiency, 在计算效率方面，

10、eigendecomposition of a data matrix is expensive unless the solution quality **is significantly compromised with** approximations,

数据矩阵的特征分解是昂贵的，除非解的质量因近似值而受到显着影响

11、We **formulate** feature learning in networks **as** a maximum likelihood optimization problem.我们将网络中的特征学习表述为最大似然优化问题。





> 题目：Structural Deep Network Embedding
>
> 来源：SIGKDD 2016
>
> 作者：Tsinghua University

### motivation：

底层网络结构复杂，浅层模型无法捕捉高度非线性的网络结构，导致网络表示不理想

 ### idea：

##### 结合深度学习解决graph embeding：

first propose a semi-supervised deep model，exploit the first-order and second-order proximity jointly to preserve the network structure.无监督组件重构二阶邻近度来捕获全局网络结构，一阶邻近度用监督组件中的监督信息以保留本地网络结构。

##### 为什么用二阶邻近度（二阶邻近度是指一对顶点的邻域结构的相似程度）：

现实世界的数据集通常非常稀疏，具有二阶邻近性的顶点对的数目比具有一阶邻近性的顶点对的数目大得多，以至于观察到的链接只占一小部分，存在许多彼此相似但没有任何边连接，但是有很多共同邻居的的顶点。因此，通过引入二阶邻近度，能够表征全局网络结构并缓解稀疏问题

##### 为什么encode-decode结构对应二阶邻近度：

虽然最小化重建损失并没有显示的保留样本之间的相似性，但重建准则可以平滑地捕获数据流形，从而保留样本之间的相似性。如果我们使用邻接矩阵 S 作为自编码器的输入，即 xi = si，由于每个实例 si 表征顶点 vi 的邻域结构，重建过程将使具有相似邻域结构的顶点具有类似的潜在表征。

##### 为什么使用B矩阵：

由于网络的稀疏性，S 中非零元素的数量远远少于零元素的数量。那么如果我们直接使用 S 作为传统自编码器的输入，更容易重构 S 中的零元素，我们对非零元素的重构误差施加了比零元素更多的惩罚。

结果具有相似邻域结构的顶点将被映射到表示空间后距离更近：

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210531163857729.png" alt="image-20210531163857729" style="zoom:50%;" />

##### 一阶近邻度：借用了拉普拉斯特征映射思想，s=0映射到距离远不产生惩罚，s=1映射到距离越远惩罚越大。

与Laplacian Eigenmaps不同，Laplacian Eigenmaps：最小$tr(Y^TLY)$可以对应于寻找$D^{-1/2}LD^{-1/2}$最小的$d$个eig对应的eidvectors，$u_1,u_2...u_d$，取$U^{n*d}=[u_1\ u_2...u_d]$各行即对应$y_1,y_2...y_n$，这里的$y$并不是特征值分解得到的，是通过神经网络学到的特征表示。



<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210531164536130.png" alt="image-20210531164536130" style="zoom:50%;" />

##### 总的loss：第三项是防止过度拟合的 L2-norm 正则项

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210531164916762.png" alt="image-20210531164916762" style="zoom:50%;" /><img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210531165152402.png" alt="image-20210531165152402" style="zoom:50%;" />

##### model：

* 整体采用AutoEncoder的思路，由Encoder和Decoder组成
* 输入的$x_i=s_i$，表示结点$i$的邻接特征$ s_i=\{s_{i,j}\}^n_{j=1}$
* 在非监督的部分，通过重建每个节点的邻居结构，来**保留二阶相似性**，提取全局特征，即 $y_i^{(K)}$到 $\hat{x}_i $ 的过程
* 监督部分，通过使相邻节点在表示空间中尽可能相近来实现**保留一阶相似性**，提取局部特征

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210531155705422.png" alt="image-20210531155705422" style="zoom:67%;" />

##### algorithm：

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210531165515620.png" alt="image-20210531165515620" style="zoom:50%;" />

### 词句



 1、This is motivated by the recent success of deep learning, which has been demonstrated to have a powerful representation ability to learn complex structures of the data  and has achieved substantial success in dealing with images  text and audio data

这是由最近深度学习的成功所激发的，深度学习已被证明具有强大的表示能力来学习数据的复杂结构，并在处理图像文本和音频数据方面取得了实质性的成功。

2、although minimizing the reconstruction loss does not explicitly preserve the similarity between samples, the recon- struction criterion can smoothly capture the data manifolds and thus preserve the similarity between samples.

尽管最小化重建损失并不能明确保留样本之间的相似性，但是重建准则可以平滑地捕获数据流形，从而保留样本之间的相似性。

3、However, real-world datasets are often so sparse that the observed links only account for a small portion.

然而，现实世界的数据集通常非常稀疏，以至于观察到的链接只占一小部分

4、However, since the underlying network structure is complex, shallow models cannot capture the highly non-linear network structure, resulting in suboptimal network representations

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210531170016781.png" alt="image-20210531170016781" style="zoom:67%;" />




> 题目：Spectral Networks and Deep Locally Connected Networks on Graphs
>
> 来源： ICLR 2014
>
> 作者：Joan Bruna  Yann LeCun
>
> 主要贡献是第一次将CNN泛化到非欧几里得空间，并提出两种并列的model，分别是基于spatial和基于spectral。

### motivation

规整的数据：基于特征平移不变性原理的参数共享机制使得CNN可以减少参数量（O(mn）到O(ks) k=channel s=卷积核尺寸）

但是现实世界大量数据是结构化的，无法应用到标准的CNN

### idea

### two constructions of GCN

### spatial construction：deep locally connected networks

##### 局部连接的实现：

通过滤波器F来实现。滤波器的非0元素位置决定。由相应节点的neighborhoods决定，非0权重表示连接，为0的权重表示未连接。

##### 下采样的实现：

下采样就是池化，在这里池化的方式为**聚类**

> input all the feature maps over a cluster, and output a single feature for that cluster.

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210601203919715.png" alt="image-20210601203919715" style="zoom:50%;" />

上图为**聚类**的示意图。

第一层为12个节点，第二次6个节点，第三次3个节点（图中未画出，用椭圆表示）

##### 深度局部连接的实现：

- locally：neighborhoods
- connected：每层与每层之间的神经元数目通过聚类而成。上一层的聚类结果对应下层的神经元。

各变量的含义：
Ω：各个层的输入信号。其中Ω0是原始的输入信号。
K scales：网络的层数。
dk clusters：第k层的下采样的cluster数，用于决定下一次输入feature的维度。
Nk,i：第k层的第i个neighborhoods，局部连接的接收域。（见figure 1）
fk：第k层的滤波器数目，graph中每一个点的特征数。

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210601204002079.png" alt="image-20210601204002079" style="zoom:50%;" />

上图为卷积+池化的整个操作过程。

核心公式：
$x_{k+1,j}=L_kh(∑_{i=1}^{f_{k-1}}F_{k,i,j}x_{k,i})(j=1…f_k)$

对应的符号说明：

$x_1\in R^{d_1*f_1} \  d_1=12$

$x_{k,i}$ ：第k层的信号的第i个特征，$R^{d_{k-1}*1}$。
$F_{k,i,j}$ ：第k层的第j种滤波器的第i个值。$F_{k,i,j}$is a $R^{d_{k-1}*d_{k−1}}$ sparse matrix with nonzero entries in the locations given by $N_k$。
$h$：非线性激活函数。
$L_k$ ：第k层的池化操作后维度从$R^{d_{k-1}*1}$到$R^{d_{k}*1}$。$L_k$ outputs the result of a pooling operation over each cluster in $Ω_k$。从$R^{d_{k-1}*1}$到$R^{d_{k}*1}$

$x_{k+1,j}$：第k+1层的信号的第j个特征，$R^{d_{k}*1}$，由于有$f_k$个滤波器，所以$x_{k+1}\in R^{d_k*f_k}$

$∑_{i=1}^{f_{k-1}}F_{k,i,j}x_{k,i} $是对信号 进$x_k$行滤波器$ F_{k,j}$ 操作，局部性体现在滤波器中（滤波器由neighborhoods给定），表示其他的邻域内的信息向中心节点汇聚。之后经过 $h$ 的非线性激活和 $L_k$ 的池化（figure1 所示的聚类操作），得到下一层的信号。

每层的计算结果：

> Each layer of the transformation loses spatial resolution but increases the number of filters.

即每层有两个结果（结合 Figure 2）：

loses spatial resolution：空间分辨率降低，即空间点数减少,$d_{k-1}->d_k$ 12-6-3。
increases the number of filters：滤波器数目增加，即每个点特征数增加$f_{k-1}->f_k$。
那么对 spatial 方法来总结一下：

信号输入到model后，卷积操作通过与滤波器 F 实现（F  由 neighborhoods 决定，实现 locally），之后通过聚类实现空间上的下采样，得到该层的输出结果。

模型评价：
优点：

> it requires relatively weak regularity assumptions on the graph. Graphs having low intrinsic dimension have localized neighborhoods, even if no nice global embedding exists.

即：对于弱规则结构的graph依旧适用，且对于无良好global embedding的低维图也适用（原因是：低维图依旧存在 localized neighborhoods）。

缺点：

> no easy way to induce weight sharing across different locations of the graph。

即：对于graph的不同位置，需采用不同的滤波器，从而导致==无法实现 weight sharing==权重共享。

CNN的单次卷积操作是由一个 filter 在整个数据立方（grid）上进行滑动（即 stride convolutions ），使得数据立方的不同位置可以使用同一个 fliter，而这个操作无法在 spatial construction 的 GCN 中实现。

### spectral construction：spectrum of the graph Laplacian

#### 原理

> draws on the properties of convolutions in the Fourier domain.
>
> In $R^d$ , convolutions are linear operators diagonalised by the Fourier basis $exp(iω·t), ω, t ∈ R^d.$
>
> One may then extend convolutions to general graphs by finding the corresponding “Fourier” basis.
>
> This equivalence is given through **the graph Laplacian,** an operator which provides an harmonic analysis on the graphs.

即：卷积是被傅里叶基底对角化的线性操作子。

对这个概念进行推广，就是找到对graph而言的“傅里叶”基底，即 graph Laplacian。通过 graph Laplacian 得到 equivalence.

#### 两种 Laplacian

##### combinatorial Laplacian

$L = D − W$

#### graph Laplacian

$L = I − D^{−1/2}W D^{−1/2}$

符号解释：

* D ：graph 的度矩阵 degree matric。

* W ：权重矩阵

* I ：对角矩阵

#### 核心公式：

$x_{k+1,j}=h (V\sum_{i=1}^{f_{k-1}} F_{k,i,j}V^T x_{k,i}) (j=1…f_k)$

对应的符号说明：

* $x_{k,i}$ ：第 k 层的信号的第 i 个特征。
* $F_{k,i,j}$：第 k 层的第 j 种滤波器的第 i 个值。
* $ F_{k,i,j}$ is a diagonal matrix.
* h：实值非线性激活。
* V：graph Laplacian L 的特征向量，由特征值排序。the eigenvectors of the graph Laplacian L, ordered by eigenvalue.
  需要注意的是，此时 $F_{k,i,j}$并不能被 shared across locations，这要经由 smooth spectral multipliers 来实现。

#### 公式解析：

graph Laplacian L 的谱分解（或称为特征分解：将 L 表示为特征值  $\lambda$ 和特征向量矩阵 V 之积）可表示为
 $L=V \Lambda V^{-1} =V\Lambda V^{T}=V\begin{pmatrix}1&&\\&\ddots&\\&&\lambda_n \end{pmatrix}V^{T}$

注意：$V^{-1}$  可写为$V^T$ 的条件为特征向量 eigenvectors 正交，此处满足条件。

graph 上的 Fourier 变换表示为
$ X_{(N×1)} = V^T_{(N×N)} x_{(N×1)}$

对应 Fourier 逆变换为
$ x_{(N×1)} = V_{(N×N)} X_{(N×1)}$


滤波器（卷积核）的 Fourier 变换为 F，即==F为定义在频谱上的滤波器，为对角矩阵==。

==通过计算出 L 的特征向量 v 组成特征向量矩阵V，利用矩阵 V，对权重进行对角化操作，来达到frenquency和smoothness的目的，并且可以将参数从 $m^2$ 降到m个。==

整个卷积的原理就是卷积定理，求卷积的过程在频域上借一步而已：
$F(f(t)*g(t))=F(\omega)·G(\omega)$

综上所述：$V^T x_{k,i}$对信$x_{k,i}$ 进行 Fourier 变换。$ \sum_{i=1}^{f_{k-1}} F_{k,i,j}V^T x_{k,i}$ 为频域上的信号和卷积核(滤波器)的卷积操作，通过$V\sum_{i=1}^{f_{k-1}} F_{k,i,j}V^T x_{k,i}$中的 V 再变回时域，从而完成时域的卷积操作。

需要注意的是，$V\sum_{i=1}^{f_{k-1}} F_{k,i,j}V^T $ 是线性操作子，非线性由 h 引入。

另外，其他论文中的公式为
$(f*g)_G = U[ (U^T h) \odot (U^T f) ]$

> smooth spectral：
> Often, only the first d dd eigenvectors of the Laplacian are useful in practice, whichcarry the smooth geometry of the graph. The cutoff frequency d dd depends upon the intrinsic regularity of the graph and also the sample size. In that case, we can replace in (5) V by $V_d$ , obtained by keeping the first d dd columns of V.

从信号的角度来看，信号的高频成份携带有信号的细节部分，去除其高频成份，即可实现 smooth。

实现 smooth 的方法就是仅仅保留特征向量矩阵 V 中前 d 个特征向量，原因是特征向量的排序是按照特征值的排序。

#### 频域的对角操作子的好处和弊端：

##### 好处：

将参数数目从$O(n^2)$ 降低到 $O(n)$

##### 弊端

> most graphs have meaningful eigenvectors only for the very top of the spectrum.
>
> Even when the individual high frequency eigenvectors are not meaningful, a cohort of high frequency eigenvectors may contain meaningful information.
>
> However this construction may not be able to access this information because it is nearly diagonal at the highest frequencies.

简单来说，对于绝大多数 graph 而言，very top of the spectrum 的 eigenvectors 是意义的。而且高频特征向量的组合也含有信息。

而对角操作子只能同时提取单个特征向量的信息，即无法提取 a cohort of high frequency eigenvectors 所含有的信息。

##### translation invariant (平移不变性) 的实现：

> linear operators $V F_{i,j}V^T $ in the Fourier basis ==> translation invariant ==> “classic” convolutions

##### spatial subsampling (空间下采样) 的实现：

> spatial subsampling can also be obtained via dropping the last part of the spectrum of the Laplacian, leading to max-pooling, and ultimately to deep convolutonal networks。

##### 局部连接和位置共享的实现：

> in order to learn a layer in which features will be not only shared across locations but also well localized in the original domain, one can learn spectral multipliers which are smooth.
>
> Smoothness can be prescribed by learning only a subsampled set of frequency multipliers and using an interpolation kernel to obtain the rest, such as cubic splines.

即：要实现每层的 feature 的位置共享和局部连接，spectral multipliers 应当 smooth，由对 frequency multipliers 的 subsample 来实现。

至此，GCN 在 spectral 上的卷积和池化的操作和局部连接和位置共享的实现已经介绍完毕，就可以实现deep convolutonal networks。

#### 背景知识

##### CNN在image和audio recognition表现出色的原因 ：

> ability to exploit the local translational invariance of signal classes over their domain 。

在各域上各类信号的“局部平移不变性”。比如对于语音信号的时移和图像中物体的平移，CNN的结果不变。

##### CNN的结构特点：

multiscale：几何形状一致，尺寸不同。
hierarchical：层次化，信息的逐层提取。
local receptive fields：局部接收域（局部连接）。

##### 由于gird的性质，使得CNN可以有以下的特点：

> The translation structure(平移结构) ==> filters instead of generic linear maps(滤波器替代线性映射) ==> weight sharing(权重共享)
> The metric on the grid ==> compactly supported filters ==> smaller than the size of the input signals(小尺寸滤波器)。
> The multiscale dyadic clustering of the grid ==> stride convolutions and pooling(卷积和池化) ==> subsampling(下采样)

##### CNN的局限

> Although the spatial convolutional structure can be exploited at several layers, typical CNN architectures do not assume any geometry in the “feature” dimension, resulting in 4-D tensors which are only convolutional along their spatial coordinates.

即对于一个4-D的tensor而言，其有X，Y，Z，feature四个维度，典型的CNN只能对X，Y，Z三个维度（即空间维度）进行卷积操作（通过3D convolution 操作），而不能对feature维度（特征维度）进行操作。

#### CNN的适用范围以及GCN与GCN的联系：

##### CNN适用范围：

data of Euclidean domain。即欧几里得域的数据，论文中称之为grid，即具有标准的几何结构。
translational equivariance/invariance with respect to this grid。即有由于grid而产生的平移不变性。

##### GCN与CNN的联系：

generalization of CNNs to signals defined on more general domains 。

即GCN是CNN在domain上的推广，推广的方式是通过推广卷积的概念（by extension the notion of convolution ）。从CNN的Euclidean domain（有规则几何结构）推广到更加general的domains（无规则几何结构，即graph）。



> 题目：Diffusion-Convolutional Neural Networks
>
> 来源：NIPS2016
>
> 作者：James Atwood and Don Towsley 
>
> College of Information and Computer Science University of Massachusetts

### 解决的问题：

NN4G是将所有邻居特征融合，不同的图对应不同的参数量，提出DCNN，只考虑 $H$ 阶邻居的特征，对应参数量$O(H*F)$，使diffusion-convolutional representation不受输入大小的限制。

### idea

假设有如下定义（截取自该文献的符号定义）：

* a set of T graphs $G = {\{Gt|t ∈ 1...T }\}$. Each graph $G_t = (Vt, Et)$is composed of vertices $V_t$ and edges$ E_t$ .

* ==$X_t$== ：The vertices are collectively described by an  $ N_t × F$ design matrix $X_t$ of ==features==, where $N_t$ is the number of nodes and $F$ is the dimension of features in $ G_t$ 

* ==$ A_t$==：the edges $E_t$ are encoded by an $N_t × N_t$ ==adjacency matrix==$ A_t$, 

* ==$P_t$==： we can compute a degree-normalized ==transition matrix== $P_t$that gives the probability of jumping from node $i$ to node$ j$ in one step. 

* ==$Y$==：Either the nodes, edges, or graphs have ==labels $Y$== associated with them, with the dimensionality of $Y$ differing in each case.

  

#### Task: predict Y

训练集：一些标记的entity（节点、图或边）。任务是预测剩余未标记entity的值。

DCNN takes $G$ as input and returns either a hard prediction for $Y$ or a conditional distribution$ P(Y |X)$

#### model：DCNN

该模型**对每一个节点(或边、或图)采用H个hop的矩阵**进行表示，每一个hop都表示该邻近范围的邻近信息，由此，对于局部信息的获取效果比较好，得到的节点的representation的表示能力很强。

> 所谓的hop，按照字面意思是“跳”，对于某一节点 ![[公式]](https://www.zhihu.com/equation?tex=n) ，其**H-hop**的节点就是，从 ![[公式]](https://www.zhihu.com/equation?tex=n) 节点开始，跳跃H次所到达的节点，比如对于 ![[公式]](https://www.zhihu.com/equation?tex=n) 节点的1-hop的节点，就是 ![[公式]](https://www.zhihu.com/equation?tex=n) 节点的邻居节点。
> 这里对于节点representation并不是采用一个向量来表示，而是采用一个矩阵进行表示，**矩阵的第 ![[公式]](https://www.zhihu.com/equation?tex=i) 行就表示i-hop的邻接信息**。
>
> - 对于 Hidden state 0，计算出与所选节点距离为1的特征的和的平均，得到各个节点的特征![H^0](https://math.jianshu.com/math?formula=H%5E0)[![H^0](https://math.jianshu.com/math?formula=H%5E0)是由Hidden state 0所有节点组成的矩阵]
> - 对于 Hidden sate 1，计算出与所选节点距离为2的特征的和的平均，得到各个节点的特征![H^1](https://math.jianshu.com/math?formula=H%5E1)[![H^1](https://math.jianshu.com/math?formula=H%5E1)是由Hidden state 1所有节点组成的矩阵]
> - 以此类推，可以得到多个上述的矩阵将得到的矩阵按照如图形式表示
>
> <img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210603164500161.png" alt="image-20210603164500161" style="zoom:50%;" />

#### 模型详述

DCNN模型输入图 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BG%7D) ，返回硬分类预测值 ![[公式]](https://www.zhihu.com/equation?tex=Y) 或者条件分布概率 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbb%7BP%7D%28Y%7CX%29) 。该模型将每一个预测的目标对象(节点、边或图)转化为一个diffusion-convolutional representation，大小为 ![[公式]](https://www.zhihu.com/equation?tex=H%5Ctimes%7B%7DF) ， ![[公式]](https://www.zhihu.com/equation?tex=H) 表示扩散的hops。因此，对于节点分类任务，图 ![[公式]](https://www.zhihu.com/equation?tex=t) 的confusion-convolutional representation为大小为 ![[公式]](https://www.zhihu.com/equation?tex=N_t%5Ctimes%7BH%7D%5Ctimes%7BF%7D) 的张量，表示为 ![[公式]](https://www.zhihu.com/equation?tex=Z_t) ，对于图分类任务，张量 ![[公式]](https://www.zhihu.com/equation?tex=Z_t) 为大小为 ![[公式]](https://www.zhihu.com/equation?tex=H%5Ctimes%7BF%7D) 的矩阵，对于边分类任务，张量 ![[公式]](https://www.zhihu.com/equation?tex=Z_t) 为大小为 ![[公式]](https://www.zhihu.com/equation?tex=M_t%5Ctimes%7BH%7D%5Ctimes%7BF%7D) 的矩阵。示意图如下

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210603164755802.png" alt="image-20210603164755802" style="zoom: 67%;" />

对于**节点分类任务**，假设 ![[公式]](https://www.zhihu.com/equation?tex=P_t%5E%2A) 为 ![[公式]](https://www.zhihu.com/equation?tex=P_t) 的power series，即 ![[公式]](https://www.zhihu.com/equation?tex=%5C%7BP_t%2CP_t%5E2%2CP_t%5E3%2C...%5C%7D) ，大小为 ![[公式]](https://www.zhihu.com/equation?tex=N_t%5Ctimes%7BH%7D%5Ctimes%7BN_t%7D) ，那么对于图 ![[公式]](https://www.zhihu.com/equation?tex=t) 的节点 ![[公式]](https://www.zhihu.com/equation?tex=i) ，第 ![[公式]](https://www.zhihu.com/equation?tex=j) 个hop，第 ![[公式]](https://www.zhihu.com/equation?tex=k) 维特征值 ![[公式]](https://www.zhihu.com/equation?tex=Z_%7Btijk%7D) 计算公式为

![[公式]](https://www.zhihu.com/equation?tex=Z_%7Bt+i+j+k%7D%3Df%5Cleft%28W_%7Bj+k%7D%5E%7Bc%7D+%5Ccdot+%5Csum_%7Bl%3D1%7D%5E%7BN_%7Bt%7D%7D+P_%7Bt+i+j+l%7D%5E%7B%2A%7D+X_%7Bt+l+k%7D%5Cright%29+%5C%5C)

使用矩阵表示为

![[公式]](https://www.zhihu.com/equation?tex=Z_%7Bt%7D%3Df%5Cleft%28W%5E%7Bc%7D+%5Codot+P_%7Bt%7D%5E%7B%2A%7D+X_%7Bt%7D%5Cright%29+%5C%5C)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Codot) 表示element-wise multiplication，由于模型只考虑 ![[公式]](https://www.zhihu.com/equation?tex=H) 跳的参数，即参数量为 ![[公式]](https://www.zhihu.com/equation?tex=O%28H%5Ctimes%7BF%7D%29) ，**使得diffusion-convolutional representation不受输入大小的限制**。

在计算出 ![[公式]](https://www.zhihu.com/equation?tex=Z) 之后，过一层全连接得到输出 ![[公式]](https://www.zhihu.com/equation?tex=Y) ，使用 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7BY%7D) 表示硬分类预测结果，使用 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbb%7BP%7D%28Y%7CX%29) 表示预测概率，计算方式如下

![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7BY%7D%3D%5Carg+%5Cmax+%5Cleft%28f%5Cleft%28W%5E%7Bd%7D+%5Codot+Z%5Cright%29%5Cright%29+%5C%5C+%5Cmathbb%7BP%7D%28Y+%7C+X%29%3D%5Coperatorname%7Bsoftmax%7D%5Cleft%28f%5Cleft%28W%5E%7Bd%7D+%5Codot+Z%5Cright%29%5Cright%29%5C%5C)

对于**图分类任务**，直接采用所有节点表示的均值作为graph的representation

![[公式]](https://www.zhihu.com/equation?tex=Z_%7Bt%7D%3Df%5Cleft%28W%5E%7Bc%7D+%5Codot+1_%7BN_%7Bt%7D%7D%5E%7BT%7D+P_%7Bt%7D%5E%7B%2A%7D+X_%7Bt%7D+%2F+N_%7Bt%7D%5Cright%29+%5C%5C)

其中，是全为1的的向量。

对于**边分类任务**，通过**将每一条边转化为一个节点来进行训练和预测**，这个节点与原来的边对应的首尾节点相连，转化后的图的邻接矩阵 ![[公式]](https://www.zhihu.com/equation?tex=A_t%27) 可以直接从原来的邻接矩阵增加一个**incidence matrix**得到

![[公式]](https://www.zhihu.com/equation?tex=A_%7Bt%7D%5E%7B%5Cprime%7D%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%7BA_%7Bt%7D%7D+%26+%7BB_%7Bt%7D%5E%7BT%7D%7D+%5C%5C+%7BB_%7Bt%7D%7D+%26+%7B0%7D%5Cend%7Barray%7D%5Cright%29%5C%5C)

之后，使用 ![[公式]](https://www.zhihu.com/equation?tex=A_t%27) 来计算 ![[公式]](https://www.zhihu.com/equation?tex=P_t%27) ，并用来替换 ![[公式]](https://www.zhihu.com/equation?tex=P_t) 来进行分类。

对于**模型训练**，使用梯度下降法，并采用early-stop方式得到最终模型。

### 改进

- **内存占用大**：DCNN建立在密集的张量计算上，需要存储大量的张量，需要的空间复杂度。
- **长距离信息传播不足**：模型对于局部的信息获取较好，但是远距离的信息传播不足。



> 题目：Inductive Representation Learning on Large Graphs
>
> 来源：NeurIPS 2019
>
> 作者：William L. Hamilton

### motivation

现在大多数方法都是直推式学习， 不能直接泛化到未知节点。

这些方法是在一个固定的图上直接学习每个节点embedding，但是大多情况图是会演化的，当网络结构改变以及新节点的出现，直推式学习需要重新训练（复杂度高且可能会导致embedding会偏移），很难落地在需要快速生成未知节点embedding的机器学习系统上。

本文提出归纳学习—GraphSAGE框架，通过训练聚合节点邻居的函数（卷积层），使GCN扩展成归纳学习任务，对未知节点起到泛化作用。

> **直推式(transductive)学习**：从特殊到特殊，仅考虑当前数据。在图中学习目标是学习目标是直接生成当前节点的embedding，例如DeepWalk、LINE，把每个节点embedding作为参数，并通过SGD优化，又如GCN，在训练过程中使用图的拉普拉斯矩阵进行计算，
> **归纳(inductive)学习**：平时所说的一般的机器学习任务，从特殊到一般：目标是在未知数据上也有区分性。

### idea

 GraphSAGE：与基于矩阵分解的嵌入方法不同，

1、我们利用节点特征（例如，文本属性、节点配置文件信息、==节点度数==）来学习可推广到不可见节点的嵌入函数。通过在学习算法中加入节点特征，我们==同时学习每个节点邻域的拓扑结构以及邻域中节点特征的分布==。

2、没有为每个节点训练不同的嵌入向量，而是训练一组==聚合器函数==，这些函数学习==从节点的局部邻域聚合特征信息==。

本文提出GraphSAGE框架的核心是如何聚合节点邻居特征信息，本章先介**绍GraphSAGE前向传播过程**（生成节点embedding），**不同的聚合函数**设定；然后介绍**无监督和有监督的损失函数**以及**参数学习。**

#### 2.GraphSAGE框架

#### 2.1 前向传播

**a. 伪代码:**

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210605220108509.png" alt="image-20210605220108509" style="zoom:50%;" />

4-5行是核心代码，介绍卷积层操作：聚合与节点v相连的邻居（采样）k-1层的embedding，得到第k层邻居聚合特征 ![[公式]](https://www.zhihu.com/equation?tex=h%5Ek_%7BN%28v%29%7D) ，与节点v第k-1层embedding ![[公式]](https://www.zhihu.com/equation?tex=h%5E%7Bk-1%7D_v) 拼接，并通过全连接层转换，得到节点v在第k层的embedding ![[公式]](https://www.zhihu.com/equation?tex=h%5Ek_v) 。

**b. Neighborhood definition**

将算法 1 扩展到小批量设置，统一采样一个固定大小的邻域集，而不是在算法1中使用完整的邻域集，以保持每个批的计算量是固定的

我们在每次迭代k中抽取不同的均匀样本，

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210605222633636.png" alt="image-20210605222633636" style="zoom: 67%;" />

**c. 可视化例子：**下图是GraphSAGE 生成目标节点（红色）embededing并供下游任务预测的过程：

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210605223405927.png" alt="image-20210605223405927" style="zoom:67%;" />

1. 先对邻居随机采样，降低计算复杂度（图中一跳邻居采样数=3，二跳邻居采样数=5）
2. 生成目标节点emebedding：先聚合2跳邻居特征，生成一跳邻居embedding，再聚合一跳邻居embedding，生成目标节点embedding，从而获得二跳邻居信息。（后面具体会讲）。
3. 将embedding作为全连接层的输入，预测目标节点的标签。

#### 2.2 聚合函数

 如何从节点的局部邻域（例如，附近节点的度或文本属性）聚合特征信息：

伪代码第5行可以使用不同聚合函数，本小节介绍五种满足排序不变量的聚合函数：平均、GCN归纳式、LSTM、pooling聚合器。（因为邻居没有顺序，聚合函数需要满足排序不变量的特性，即输入顺序不会影响函数结果）

**a.平均聚合：**先对邻居embedding中每个维度取平均，然后与目标节点embedding拼接后进行非线性转换。

![[公式]](https://www.zhihu.com/equation?tex=h%5Ek_%7BN%28v%29%7D%3Dmean%28%5C%7Bh_u%5E%7Bk-1%7D%2Cu+%5Cin+N%28v%29%5C%7D%29+%5C%5C+h_v%5Ek%3D%5Csigma%28W%5Ek%5Ccdot+CONCAT%28h_v%5E%7Bk-1%7D%2Ch_%7BN%28u%29%7D%5E%7Bk%7D%29%29)

**b. 归纳式聚合：**直接对目标节点和所有邻居emebdding中每个维度取平均（替换伪代码中第5、6行），后再非线性转换：

![[公式]](https://www.zhihu.com/equation?tex=h_v%5Ek%3D%5Csigma%28W%5Ek%5Ccdot+mean%28%5C%7Bh_v%5E%7Bk-1%7D%5C%7D%5Ccup+%5C%7Bh_u%5E%7Bk-1%7D%2C%5Cforall+u+%5Cin+N%28v%29%5C%7D%29)

**c. LSTM聚合：**LSTM函数不符合“排序不变量”的性质，需要先对邻居随机排序，然后将随机的邻居序列embedding ![[公式]](https://www.zhihu.com/equation?tex=%5C%7Bx_t%2C+t+%5Cin+N%28v%29%5C%7D) 作为LSTM输入。

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210605223539926.png" alt="image-20210605223539926" style="zoom:67%;" />

**d. Pooling聚合器:**先对每个邻居节点上一层embedding进行非线性转换（等价单个全连接层，每一维度代表在某方面的表示（如信用情况）），再按维度应用 max/mean pooling，捕获邻居集上在某方面的突出的／综合的表现 以此表示目标节点embedding。

![[公式]](https://www.zhihu.com/equation?tex=h_%7BN%28v%29%7D%5Ek%3Dmax%28%5C%7B%5Csigma%28W_%7Bpool%7Dh_%7Bui%7D%5Ek%2Bb%29%5C%7D%2C%5Cforall+u_i+%5Cin+N%28v%29%29+%5C%5C+h_v%5Ek%3D%5Csigma%28W%5Ek%5Ccdot+CONCAT%28h_v%5E%7Bk-1%7D%2Ch_%7BN%28u%29%7D%5E%7Bk-1%7D%29%29)

#### 2.3 无监督和有监督损失设定

损失函数根据具体应用情况，可以使用**基于图的无监督损失**和**有监督损失**。

**a. 基于图的无监督损失：**希望节点u与“邻居”v的embedding也相似（对应公式第一项），而与“没有交集”的节点 ![[公式]](https://www.zhihu.com/equation?tex=v_n) 不相似（对应公式第二项)。

![img](https://pic2.zhimg.com/80/v2-f74c0b25dc4cb406035659966d0e8691_1440w.png)

- ![[公式]](https://www.zhihu.com/equation?tex=z_u) 为节点u通过GraphSAGE生成的embedding。
- 节点v是节点u固定长度随机游走访达“邻居”。
- ![[公式]](https://www.zhihu.com/equation?tex=v_n+%5Csim+P_n%28u%29) 表示负采样：节点 ![[公式]](https://www.zhihu.com/equation?tex=v_n) 是从节点u的负采样分布 ![[公式]](https://www.zhihu.com/equation?tex=P_n) 采样的，Q为采样样本数，即负样本的数量。
- embedding之间相似度通过向量点积计算得到
- 通过随机梯度下降聚合函数的参数

**b. 有监督损失：**无监督损失函数的设定来学习节点embedding 可以供下游多个任务使用，若仅使用在特定某个任务上，则可以替代上述损失函数符合特定任务目标，如交叉熵。

#### 2.4 参数学习

通过前向传播得到节点u的embedding ![[公式]](https://www.zhihu.com/equation?tex=z_u) ,然后梯度下降（实现使用Adam优化器） **进行反向**传播优化参数 ![[公式]](https://www.zhihu.com/equation?tex=W%5Ek) 和聚合函数内参数。

### 3.实验

#### 3.1 实验目的

1. 比较GraphSAGE 相比baseline 算法的提升效果；
2. 比较GraphSAGE的不同聚合函数。

#### 3.2 数据集及任务

1. Citation 论文引用网络（节点分类）
2. Reddit web论坛 （节点分类）
3. PPI 蛋白质网络 （graph分类）

#### 3.3 比较方法

1. 随机分类器
2. 手工特征（非图特征）
3. deepwalk（图拓扑特征）
4. deepwalk+手工特征
5. GraphSAGE四个变种 ，并无监督生成embedding输入给LR 和 端到端有监督

(分类器均采用LR)

#### 3.4 GraphSAGE 设置

- K=2，聚合两跳内邻居特征
- S1=25，S2=10： 对一跳邻居抽样25个，二跳邻居抽样10个
- RELU 激活单元
- Adam 优化器
- 对每个节点进行步长为5的50次随机游走
- 负采样参考word2vec，按平滑degree进行，对每个节点采样20个。
- 保证公平性，：所有版本都采用相同的minibatch迭代器、损失函数、邻居抽样器。

#### 3.5 运行时间和参数敏感性

1. **计算时间：**下图A中GraphSAGE中LSTM训练速度最慢，但相比DeepWalk，GraphSAGE在预测时间减少100-500倍（因为对于未知节点，DeepWalk要重新进行随机游走以及通过SGD学习embedding）
2. **邻居抽样数量：**下图B中邻居抽样数量递增，边际收益递减（F1），但计算时间也变大。 平衡F1和计算时间，将S1设为25。
3. **聚合K跳内信息**：在GraphSAGE， K=2 相比K=1 有10-15%的提升；但将K设置超过2，边际效果上只有0-5%的提升，但是计算时间却变大了10-100倍。

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210605223606409.png" alt="image-20210605223606409" style="zoom:67%;" />

#### 3.6 效果

1. GraphSAGE相比baseline 效果大幅度提升
2. GraphSAGE有监督版本比无监督效果好。
3. LSTM和pool的效果较好

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210605223624344.png" alt="image-20210605223624344" style="zoom:67%;" />

### 改进

聚合函数还是太粗糙，如何既能保持邻居节点的信息，又可以聚合，cnn中是靠卷积实现的。

> 题目：Diffusion Improves Graph Learning
>
> 来源：NeurIPS 2019
>
> 作者：Johannes Klicpera Stefan Weißenberger   Technical University of Munich

### motivation

一种受频谱方法启发的执行消息传递的新技术：图扩散卷积（GDC）。图扩散卷积（GDC）。 GDC 不是只聚合来自1-hop 邻居的信息，而是聚合来自更大邻域的信息。这个邻域是通过一个新的图来构建的，这个新图是通过对图扩散的广义形式进行稀疏化而生成的。我们展示了图扩散如何

### idea

与DCNN区别：DCNN是将H矩阵concat在一起，取每一个节点的各阶信息,GDC是将H矩阵加在一起，类似于谱分析中的多项式矩阵

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210604194214806.png" alt="image-20210604194214806" style="zoom:50%;" />                 <img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210604194244203.png" alt="image-20210604194244203" style="zoom: 25%;" />

广义图扩散：$S=\sum_{k=0}^{∞}\theta_kT^k$

* 用加权系数 $θ_k$ 和广义转移矩阵 $T$ 

* walk transition matrix $T_rw = AD^{−1} $， $T_{rw}$ is column-stochastic

* the symmetric transition matrix $T_{sym} = D^{−1/2}AD^{−1/2}$

* a dense matrix S，S 中的值代表所有节点对之间的影响
* D is the diagonal matrix of node degrees，$$D_{ii} $$= 􏰄$\sum_{j=1}^{N}\ A_{ij}$

扩散卷积 (GDC) 就是将邻接矩阵 A 用稀疏矩阵 S 代替，频谱域不提供任何局部性概念。空间定位允许我们简单地截断 S 的小值并恢复稀疏，得到矩阵 $\hat{S}$。

稀疏化方法：

1、top-k，Use the k entries with the highest mass per column,

2、阈值：将 ε 以下的条目设置为零。

稀疏化仍然需要在预处理期间计算密集矩阵 S。然而，许多流行的图扩散可以在线性时间和空间中有效且准确地近似

 there are fast approximations for both PPR  and the heat kernel , with which GDC achieves a linear runtime O(N)

### 不足：

GDC 基于同质性假设

### 谱分析：

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210604172137059.png" alt="image-20210604172137059" style="zoom:50%;" />

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210604172153137.png" alt="image-20210604172153137" style="zoom:50%;" />

文献中 g 的常见选择是多项式J阶滤波器，因为它是局部的并且参数数量有限

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210604172253561.png" alt="image-20210604172253561" style="zoom: 33%;" />

多项式滤波器与广义图扩散的有密切关系：<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210604172627150.png" alt="image-20210604172627150" style="zoom: 33%;" />

