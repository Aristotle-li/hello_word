https://zhuanlan.zhihu.com/p/43480539?utm_source=qq&utm_medium=social&utm_oi=660108897164201984

首发于[北极圈智能](https://www.zhihu.com/column/AI-cmvs)

![活体检测Face Anti-spoofing综述](https://pic4.zhimg.com/v2-d5ae8693024b512e374df1c9b3a3009d_1440w.jpg?source=172ae18b)

# 活体检测Face Anti-spoofing综述

[![Fisher 鱼子](https://pic4.zhimg.com/v2-876364464e3100d345afb7884eae6641_xs.jpg?source=172ae18b)](https://www.zhihu.com/people/yu-zi-tong-13)

[Fisher 鱼子](https://www.zhihu.com/people/yu-zi-tong-13)

通过眼睛读懂你

最新综述地址：

Deep learning for Face Anti-Spoofing: A Survery

https://arxiv.org/pdf/2106.14948.pdf



**1. 什么是活体检测？**

​        -->  判断捕捉到的人脸是真实人脸，还是伪造的人脸攻击（如：彩色纸张打印人脸图，电子设备屏幕中的人脸数字图像 以及 面具 等）

**2. 为什么需要活体检测？**

​        --> 在金融支付，门禁等应用场景，活体检测一般是嵌套在人脸检测与人脸识别or验证中的模块，用来验证是否用户真实本人

**3. 活体检测对应的计算机视觉问题：**

​        --> 就是分类问题，可看成二分类（真 or 假）；也可看成多分类（真人，纸张攻击，屏幕攻击，面具攻击）

------

## **Anti-spoofing 1.0 时代**

从早期 handcrafted 特征的传统方法说起，目标很明确，就是找到活体与非活体攻击的difference，然后根据这些差异来设计特征，最后送给分类器去决策。

那么问题来了，活体与非活体有哪些差异？

- 颜色纹理
- 非刚性运动变形
- 材料 （皮肤，纸质，镜面）
- 图像or视频质量 

所以这段时期的文章都是很有针对性地设计特征，列举几篇比较重要的：

**[1] Image Distortion Analysis, 2015**

如下图，单帧输入的方法，设计了 镜面反射+图像质量失真+颜色 等统计量特征，合并后直接送SVM进行二分类。

![img](https://pic2.zhimg.com/80/v2-6be35e04329b55c2039674978292bd1d_1440w.jpg)Image Distortion Analysis, 2015

Cons: 对于高清彩色打印的纸张 or 高清录制视频，质量失真不严重时，难区分开

**[2] Colour Texture, 2016** 

Oulu CMVS组的产物，算是传统方法中的战斗机，特别简洁实用，Matlab代码（课题组官网有），很适合搞成C++部署到门禁系统。

原理：活体与非活体，在RGB空间里比较难区分，但在其他颜色空间里的纹理有明显差异

算法：HSV空间人脸多级LBP特征 + YCbCr空间人脸LPQ特征  （后在17年的paper拓展成SURF特征）

![img](https://pic2.zhimg.com/80/v2-fc257d3fdfaa437b6320667ba3ee0c6d_1440w.jpg)Colour Texture, 2016

Pros: 算法简洁高效易部署；也证明了活体与非活体在 HSV等其他空间也是 discriminative，故后续深度学习方法有将HSV等channel也作为输入来提升性能。

**[3]  Motion mag.-HOOF + LBP-TOP, 2014**

**[4]  DMD + LBP, 2015**

前面说的都是单帧方法，这两篇文章输入的是连续多帧人脸图；

主要通过捕获活体与非活体微动作之间的差异来设计特征。

一个是先通过运动放大来增强脸部微动作， 然后提取方向光流直方图HOOF + 动态纹理LBP-TOP 特征；一个是通过动态模式分解DMD，得到最大运动能量的子空间图，再分析纹理。

PS：这个 motion magnification 的预处理很差劲，加入了很多其他频段噪声（18年新出了一篇用 Deep learning 来搞 Motion mag. 看起来效果挺好，可以尝试用那个来做运动增强，再来光流orDMD）

![img](https://pic4.zhimg.com/80/v2-c765652dfa0b23577cfc48f7f84aaa93_1440w.jpg)Motion mag.-HOOF + LBP-TOP, 2014

![img](https://pic3.zhimg.com/80/v2-1fc6c5b9e1252585022b65880956dc3a_1440w.jpg)DMD + LBP, 2015

Cons: 基于Motion的方法，对于 仿人脸wrapped纸张抖动 和 视频攻击，效果不好；因为它假定了活体与非活体之间的非刚性运动有明显的区别，但其实这种微动作挺难描述与学习~

**[5]  Pulse + texture, 2016**

第一个将 remote pluse 应用到活体检测中，多帧输入

（交代下背景：在CVPR2014，当时 Xiaobai Li 已经提出了从人脸视频里测量心率的方法）

算法流程： 

​       \1. 通过 pluse 在频域上分布不同先区分 活体 or 照片攻击 （因为照片中的人脸提取的心率分布不同）

​       \2. 若判别1结果是活体，再 cascade 一个 纹理LBP 分类器，来区分 活体 or 屏幕攻击（因为屏幕视频中人脸心率分布与活体相近）

![img](https://pic1.zhimg.com/80/v2-59ea4618559c07bdc97260c2eba7a48c_1440w.jpg)Pulse + texture, 2016

Pros: 从学术界来说，引入了 心理信号 这个新模态，很是进步；从工业界来看，如果不能一步到位，针对每种类型攻击，也可进行 Cascade 对应的特征及分类器的部署方式

Cons: 由于 remote heart rate 的算法本来鲁棒性也一般，故出来的 pulse-feature 的判别性能力很不能保证；再者屏幕video里的人脸视频出来的 pulse-feature 是否也有微小区别，还待验证~ 

------

## **Anti-spoofing 2.0 时代**

其实用 Deep learning 来做活体检测，从15年陆陆续续就有人在研究，但由于公开数据集样本太少，一直性能也超越不了传统方法：

**[6]  CNN-LSTM, 2015**

多帧方法，想通过 CNN-LSTM 来模拟传统方法 LBP-TOP，性能堪忧~

**[7]  PatchNet pretrain，CNN finetune, 2017**

单帧方法，通过人脸分块，pre-train 网络；然后再在 global 整个人脸图 fine-tune，作用不大

**[8]  Patch and Depth-Based CNNs, 2017**

第一个考虑把 人脸深度图 作为活体与非活体的差异特征，因为像屏幕中的人脸一般是平的，而纸张中的人脸就算扭曲，和真人人脸的立体分布也有差异；

就算用了很多 tricks 去 fusion，性能还是超越不了传统方法。。。 

![img](https://pic4.zhimg.com/80/v2-98186d06a1ebcd60b36cefbcea26499b_1440w.jpg)

------------------------------- 华丽的分割线 -----------------------------

**[9]  Deep Pulse and Depth, 2018**

发表在 CVPR2018 的文章，终于超越了传统方法性能。

文章[8]的同一组人，设计了深度框架 准端到端 地去预测 Pulse统计量 及 Depth map （这里说的“准”，就是最后没接分类器，直接通过样本 feature 的相似距离，阈值决策）

在文章中明确指明：

- 过去方法把活体检测看成二分类问题，直接让DNN去学习，这样学出来的cues不够general 和 discriminative
- 将二分类问题换成带目标性地特征监督问题，即 回归出 *pulse 统计量 + 回归出 Depth map*，保证网络学习的就是这两种特征（哈哈，不排除假设学到了 color texture 在里面，黑箱网络这么聪明）

![img](https://pic4.zhimg.com/80/v2-8e6d0311f08eddb996530021370a07e3_1440w.jpg)Deep Pulse and Depth, 2018

回归 Depth map，跟文章[8]中一致，就是通过 Landmark 然后 3DMMfitting 得到 人脸3D  shape，然后再阈值化去背景，得到 depth map 的 groundtruth，最后和网络预测的 estimated depth map 有 L2 loss。

而文章亮点在于设计了 Non-rigid Registration Layer 来对齐各帧人脸的非刚性运动（如姿态，表情等），然后通过RNN更好地学到 temporal pulse 信息。

![img](https://pic2.zhimg.com/80/v2-83ac87895d7ee50434c3637e7c223add_1440w.jpg)Non-rigid Registration Layer

为什么需要这个对齐网络呢？我们来想想，在做运动识别任务时，只需简单把 sampling或者连续帧  合并起来喂进网络就行了，是假定相机是不动的，对象在运动；而文中需要对连续人脸帧进行pulse特征提取，主要对象是人脸上对应ROI在  temporal 上的 Intensity 变化，所以就需要把人脸当成是相机固定不动。



**[10]  Micro-texture + SSD or binocular depth , 2018**

ArXiv 刚挂出不久的文章，最大的贡献是把 活体检测 直接放到 人脸检测（SSD，MTCNN等） 模块里作为一个类，即人脸检测出来的 bbox 里有 背景，真人人脸，假人脸 三类的置信度，这样可以在早期就过滤掉一部分非活体。

所以整个系统速度非常地快，很适合工业界部署~

至于后续手工设计的  SPMT feature 和 TFBD feature 比较复杂繁琐，分别是表征 micro-texture 和 stereo structure of face，有兴趣的同学可以去细看。 

![img](https://pic4.zhimg.com/80/v2-b4f7a276a3477dad64723bd546ec7813_1440w.jpg)ture + SSD or binocular depth , 2018

**[11]  De-Spoofing, ECCV2018**

单帧方法，与Paper[8]和[9]一样，是MSU同一个课题组做的。

文章的idea很有趣，启发于图像去噪de-noise 和 图像去抖动 de-blur。无论是噪声图还是模糊图，都可看成是在原图上加噪声运算或者模糊运算（即下面的公式），而去噪和去抖动，就是估计噪声分布和模糊核，从而重构回原图。

![[公式]](https://www.zhihu.com/equation?tex=x+%3D+%5Ctilde%7Bx%7D+%2B+N%28%5Ctilde%7Bx%7D%29) 

文中把活体人脸图看成是原图 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Bx%7D) ，而非活体人脸图看成是加了噪声后失真的 ![[公式]](https://www.zhihu.com/equation?tex=x) ，故 task 就变成估计 Spoof noise ![[公式]](https://www.zhihu.com/equation?tex=N%28%5Ctilde%7Bx%7D%29) ，然后用这个 Noise pattern feature 去分类决策。

![img](https://pic1.zhimg.com/80/v2-188a29e827236d636e6fbe5431475df4_1440w.jpg)De-spoofing process

那问题来了，数据集没有像素级别一一对应的 groundtruth，也没有Spoof Noise模型的先验知识（如果有知道Noise模型，可以用Live Face来生成Spoofing  Face），那拿什么来当groundtruth，怎么设计网络去估计 Spoofing noise 呢？

如一般Low-level image 任务一样，文中利用Encoder-decoder来得到 Spoof noise N，然后通过残差重构出 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7BI%7D+%3D+I+-+N%28%5Ctilde%7BI%7D%29) ，这就是下图的DS Net。为了保证网络对于不同输入，学出来的Noise是有效的，根据先验知识设计了三个Loss来constrain：

Magnitude loss(当输入是Live face时，N尽量逼近0)；

Repetitive loss(Spooing face的Noise图在高频段有较大的峰值)；

0\1Map Loss(让Real Face 的 deep feature map分布尽量逼近全0，而Spoofing face的 deep feature map 尽量逼近全1)

![img](https://pic2.zhimg.com/80/v2-28d56381988f239f577d74fb751396d1_1440w.jpg)De-spoofing网络架构

 那网络右边的 VQ-Net 和 DQ-Net 又有什么作用呢？因为没有 Live face 的 Groundtruth，要保证重构出来的分布接近 Live face，作者用了对抗生成网络GAN (即 VQ-Net )去约束重构生成的 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7BI%7D) 与Live face分布尽量一致；而用了文章[8]中的 pre-trained Depth model 来保证 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7BI%7D) 的深度图与Live face的深度图尽量一致。



Pros: 通过可视化最终让大众知道了 Spoofing Noise 是长什么样子的~

Cons: 在实际场景中难部署（该模型假定Spoofing Noise是 strongly 存在的，当实际场景中活体的人脸图质量并不是很高，而非活体攻击的质量相对高时，Spoofing noise走不通）

------

## **后记：不同模态的相机输入对于活体检测的作用**

**1.** **近红外NIR**

由于NIR的光谱波段与可见光VIS不同，故真实人脸及非活体载体对于近红外波段的吸收和反射强度也不同，即也可通过近红外相机出来的图像来活体检测。从出来的图像来说，近红外图像对屏幕攻击的区分度较大，对高清彩色纸张打印的区分度较小。

从特征工程角度来说，方法无非也是提取NIR图中的光照纹理特征[12] 或者 远程人脸心率特征[13]  来进行。下图可见，上面两行是真实人脸图中人脸区域与背景区域的直方图分布，明显与下面两行的非活体图的分布不一致；而通过与文章[5]中一样的rPPG提取方法，在文章[]中说明其在NIR图像中出来的特征更加鲁棒~

![img](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='508' height='332'></svg>)NIR人脸区域与背景区域直方图[12]

**2.** **结构光/ToF**

由于结构光及ToF能在近距离里相对准确地进行3D人脸重构，即可得到人脸及背景的点云图及深度图，可作为精准活体检测（而不像单目RGB或双目RGB中仍需估计深度）。不过就是成本较高，看具体应用场景决定。

**3.** **光场 Light field**

光场相机具有光学显微镜头阵列，且由于光场能描述空间中任意一点向任意方向的光线强度，出来的raw光场照片及不同重聚焦的照片，都能用于活体检测：

3.1 raw光场照片及对应的子孔径照片[14] 

如下图所示，对于真实人脸的脸颊边缘的微镜图像，其像素应该是带边缘梯度分布；而对应纸张打印或屏幕攻击，其边缘像素是随机均匀分布：

![img](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='611' height='440'></svg>)光场相机图[14]

3.2 使用一次拍照的重聚焦图像[15]

原理是可以从两张重聚焦图像的差异中，估计出深度信息；从特征提取来说，真实人脸与非活体人脸的3D人脸模型不同，可提取差异图像中的 亮度分布特征+聚焦区域锐利程度特征+频谱直方图特征。

------



至此，Face anti-spoofing 的简单Survey已完毕~

毫无疑问，对于**学术界**，后续方向应该是用DL学习更精细的 人脸3D特征 和 人脸微变化微动作(Motion Spoofing Noise?) 表征；而也可探索活体检测与人脸检测及人脸识别之间更紧密的关系。 

对于**工业界**，最后倒数第二篇文章的启发很不错；更可借助近红外，结构光/ToF等硬件做到更精准。



**Reference:**

[1] Di Wen, Hu Han, Anil K. Jain. Face Spoof Detection with Image  Distortion Analysis. IEEE Transactions on Information Forensics and  Security, 2015

[2] Zinelabidine Boulkenafet, Jukka Komulainen,   Abdenour Hadid. Face Spoofing Detection Using Colour Texture Analysis.   IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY,  2016

[3] Samarth Bharadwaj. Face Anti-spoofing via Motion Magnification and

Multifeature Videolet Aggregation,  2014

[4] Santosh Tirunagari, Norman Poh. Detection of Face Spoofing Using Visual Dynamics.  IEEE TRANS. ON INFORMATION FORENSICS AND SECURIT,  2015

[5] Xiaobai Li, , Guoying Zhao. Generalized face anti-spoofing by detecting pulse

from face videos,  2016 23rd ICPR

[6] Zhenqi Xu. Learning Temporal Features Using LSTM-CNN Architecture for Face Anti-spoofing,  2015 3rd IAPR

[7] Gustavo Botelho de Souza, On the Learning of Deep Local Features for

Robust Face Spoofing Detection,  2017

[8] Yousef Atoum, Xiaoming Liu. Face Anti-Spoofing Using Patch and Depth-Based CNNs,  2017

[9] Yaojie Liu, Amin Jourabloo, Xiaoming Liu, Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision ，CVPR2018

[10] Discriminative Representation Combinations for Accurate Face Spoofing Detection ”，2018 PR，上海交大

[11] Face De-Spoofing: Anti-Spoofing via Noise Modeling, ECCV2018

[12]Xudong Sun, Context Based Face Spoofing Detection Using Active Near-Infrared Images, ICPR 2016

[13]Javier Hernandez-Ortega, Time Analysis of Pulse-based Face Anti-Spoofing in Visible and NIR, CVPR2018 workshop

[14]Sooyeon Kim, Face Liveness Detection Using a Light Field Camera, 2014

[15]Xiaohua Xie, One-snapshot Face Anti-spoofing Using a Light Field Camera, 2017

## Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision

Yaojie Liu Amin Jourabloo Xiaoming Liu Department of Computer Science and Engineering Michigan State University, East Lansing MI 48824

`摘要`：人脸反欺骗对于防止人脸识别系统发生安全漏洞至关重要。以前的深度学习方法将人脸反欺骗视为二元分类问题。他们中的许多人很难掌握足够的欺骗线索，并且概括性很差。在本文中，我们论证了辅助监督对指导学习具有判别性和可概括性线索的重要性。学习 CNN-RNN 模型以通过像素级监督来估计人脸深度，并通过序列式监督来估计 rPPG 信号。估计的深度和 rPPG 被融合以区分真人脸和恶搞脸。此外，我们引入了一个新的人脸反欺骗数据库，它涵盖了大范围的光照、主题和姿势变化。实验表明，我们的模型在内部和跨数据库测试中都达到了最先进的结果。

### 1. 简介

随着在手机解锁、访问控制和安全方面的应用，生物识别系统在我们的日常生活中得到了广泛的应用，而面部是最受欢迎的生物识别方式之一。虽然人脸识别系统越来越流行，但攻击者会向系统展示人脸欺骗（即演示攻击，PA）并试图将其验证为真正的用户。人脸 PA 包括在纸上打印人脸（打印攻击）、在数字设备上重放人脸视频（重放攻击）、戴口罩（面具攻击）等。 为了对抗 PA，人脸反欺骗技术 [16, 22 , 23, 29] 被开发用于在识别面部图像之前检测 PA。因此，人脸反欺骗对于确保人脸识别系统对 PA 的鲁棒性和使用安全性至关重要。 RGB 图像和视频是人脸反欺骗系统的标准输入，类似于人脸识别系统。研究人员通过将手工特征提供给二元分类器 [13, 18, 19, 27, 33, 34, 38, 49] 来启动基于纹理的反欺骗方法。在深度学习时代的后期，几种卷积神经网络 (CNN) 方法利用 softmax 损失作为监督 [21, 30, 37, 48]。似乎几乎所有先前的工作都将面部反欺骗问题视为一个二元（实时与欺骗）分类问题。使用二元监督学习深度反欺骗模型有两个主要问题。首先，有不同级别的图像退化，即欺骗模式，将欺骗面部与真实面部进行比较，包括皮肤细节丢失、颜色失真、莫尔条纹、形状变形和欺骗伪影（例如，反射）[29， 38]。具有 softmax 损失的 CNN 可能会发现能够将这两个类别分开的任意线索，例如屏幕边框，但不会发现忠实的欺骗模式。当这些线索在测试过程中消失时，这些模型将无法区分假脸和真人脸，并导致泛化能力差。其次，在测试过程中，通过二元监督学习的模型只会生成二元决策，而没有解释或决策理由。在追求可解释的人工智能 [1] 的过程中，需要学习模型生成支持最终二元决策的欺骗模式。

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210702103902878.png" alt="image-20210702103902878" style="zoom:50%;" />

图 1. 传统的基于 CNN 的人脸反欺骗方法利用二元监督，鉴于 CNN 的巨大解决方案空间，这可能会导致过度拟合。这项工作设计了一种新颖的网络架构，以利用两个辅助信息作为监督：深度图和 rPPG 信号，目的是在推理过程中改进泛化和可解释的决策。



为了解决这些问题，如图 1 所示，我们提出了一个深度模型，它使用来自空间和时间辅助信息的监督而不是二进制监督，目的是从面部视频中稳健地检测面部 PA。这些辅助信息是基于我们关于真人脸和恶搞脸之间关键差异的领域知识获得的，其中包括两个方面：空间和时间。从空间角度来看，已知活人脸具有类似人脸的深度，例如，在正面观察的人脸中，鼻子比脸颊更靠近相机，而打印或重放攻击中的人脸则具有平坦或平面的深度，例如，纸张图像上的所有像素对相机具有相同的深度。因此，深度可以用作辅助信息来监督实时和欺骗面部。从时间的角度来看，显示正常的 rPPG 信号（即心脏脉搏信号）可以从实时面部视频中检测到，但不能从欺骗性面部视频中检测到 [31, 35]。因此，我们提供不同的 rPPG 信号作为辅助监督，引导网络分别从实时或恶搞视频中学习。为了实现这两种监督，我们设计了一个具有捷径连接的网络架构来捕获不同的尺度和一个新颖的非刚性配准层来处理 rPPG 估计的运动和姿势变化。此外，与许多视觉问题类似，数据在训练反欺骗模型方面起着重要作用。众所周知，相机/屏幕质量是影响恶搞质量的关键因素。现有的人脸反欺骗数据库，例如 NUAA [43]、CASIA [50]、ReplayAttack [17] 和 MSU-MFSD [45]，是在 3 - 5 年前收集的。鉴于消费电子产品的快速发展，这些数据收集中使用的设备类型（例如，相机和欺骗介质）与当今的设备相比，在分辨率和成像质量方面已经过时。最近的 MSU-USSA [38] 和 OULU 数据库 [14] 的主题在姿势、光照、表情 (PIE) 方面的变化较少。缺乏必要的变化会使学习一个有效的模型变得困难。鉴于对更高级数据库的明确需求，我们收集了一个人脸反欺骗数据库，名为 Spoof in the Wild Database (SiW)。 SiW 数据库由 165 个主题、6 个欺骗媒体和 4 个会话组成，涵盖了 PIE、到相机的距离等变化。 SiW 涵盖的变化比以前的数据库大得多，详见表。 1和秒。 4、本工作的主要贡献包括：

* 我们建议利用新的辅助信息（即深度图和 rPPG）来监督 CNN 学习以改进泛化。
* 我们提出了一种新颖的 CNN-RNN 架构，用于端到端学习深度图和 PPG 信号。
* 我们发布了一个包含 PIE 变体和其他实际因素的新数据库。我们实现了最先进的面部反欺骗性能。

### 2. 前期工作

我们将先前的面部反欺骗工作分为三组：基于纹理的方法、基于时间的方法和远程光体积描记法。

`基于纹理的方法` 由于大多数人脸识别系统仅采用 RGB 摄像头，因此使用纹理信息已成为解决人脸反欺骗的自然方法。许多先前的工作利用手工制作的特征，例如 LBP [18, 19, 33]、HoG [27, 49]、SIFT [38] 和 SURF [13]，并采用传统的分类器，例如 SVM 和 LDA。为了克服光照变化的影响，他们在不同的输入域中寻找解决方案，例如 HSV 和 YCbCr 颜色空间 [11, 12] 和傅立叶光谱 [29]。

由于深度学习已被证明在许多计算机视觉问题中有效，因此最近有许多尝试在面部反欺骗中使用基于 CNN 的特征或 CNN [21,30,37,48]。大多数工作通过应用 softmax 损失将人脸反欺骗视为一个简单的二元分类问题。例如，[30, 37] 使用 CNN 作为特征提取器，并从 ImageNet 预训练的 CaffeNet 和 VGG-face 进行微调。 [21, 30] 的工作将不同设计的人脸图像输入 CNN，例如多尺度人脸和手工制作的特征，并直接对实时与欺骗进行分类。与我们有相似之处的一项先前工作是 [5]，其中 Atoum 等人。提出了一种使用纹理和深度的基于两蒸汽 CNN 的反欺骗方法。我们在多个方面推进 [5]，包括与时间监督（即 rPPG）的融合、更精细的架构设计、新颖的非刚性配准层和全面的实验支持。

`基于时间的方法` 最早的面部反欺骗解决方案之一是基于时间线索，例如眨眼 [36,37]。 [26,42] 等方法跟踪嘴巴和嘴唇的运动以检测面部活力。虽然这些方法对典型的纸张攻击有效，但当攻击者进行重放攻击或切开眼睛/嘴巴部分的纸张攻击时，它们就会变得脆弱。

还有一些方法依赖于更一般的时间特征，而不是特定的面部运动。最常见的方法是帧级联。许多手工制作的基于特征的方法可以通过简单地连接连续帧的特征来训练分类器来提高数据库内测试性能 [11,18,28]。此外，还有一些作品提出了特定于时间的特征，例如 Haralick 特征 [4]、运动 mag [7] 和光流 [6]。在深度学习时代，冯等人。将光流图和 Shearlet 图像特征提供给 CNN [21]。在 [47] 中，Xu 等人。提出了一种 LSTM-CNN 架构来利用时间信息进行二元分类。总体而言，所有先前的方法仍然将人脸反欺骗视为二元分类问题，因此它们很难在跨数据库测试中很好地泛化。在这项工作中，我们通过学习面部视频的 rPPG 信号来提取判别时间信息。

`远程光电容积脉搏波 (rPPG)` 远程光电容积脉搏波 (rPPG) 是一种无需接触人体皮肤即可跟踪心率等生命信号的技术 [9, 20, 41, 44, 46]。研究从没有运动或照明变化的面部视频开始，到具有多种变化的视频。在 [20] 中，Haan 等人。从具有光照和运动变化的 RGB 面部视频中估计 rPPG 信号。它利用色差消除镜面反射并估计两个正交色度信号。应用带通滤波器 (BPM) 后，色度信号的比率用于计算 rPPG 信号。

rPPG 以前曾被用于解决人脸反欺骗 [31, 35]。在 [31] 中，rPPG 信号用于检测 3D 面具攻击，与 3D 面具不同，实时面部表现出心率脉冲。他们使用 [20] 提取的 rPPG 信号并计算相关特征进行分类。同样，马格达莱纳等人。 [35] 从三个面部区域和两个非面部区域中提取 rPPG 信号（也通过 [20]），用于检测打印和重放攻击。尽管在重放攻击中，rPPG 提取器可能仍会捕获正常脉冲，但多个区域的组合可以区分真人脸与恶搞脸。虽然 rPPG 提取 [20] 的解析解很容易实现，但我们观察到它对 PIE 变化很敏感。因此，我们采用了一种新颖的 CNN-RNN 架构来学习从面部视频到 rPPG 信号的映射，这不仅对 PIE 变化具有鲁棒性，而且还可以区分实时与欺骗。

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210702104434360.png" alt="image-20210702104434360" style="zoom:50%;" />

​																				图 2. 所提出方法的概述

### 3. 使用深度网络进行人脸反欺骗

所提出方法的主要思想是引导深度网络专注于跨空间和时间域的已知欺骗模式，而不是提取任何可以将两个类分开但不可推广的线索。如图 2 所示，所提出的网络以连贯的方式结合了 CNN 和 RNN 架构。 CNN 部分利用深度图监督来发现细微的纹理属性，从而导致真实和欺骗人脸的不同深度。然后，它将估计的深度和特征图提供给一个新的非刚性配准层，以创建对齐的特征图。 RNN 部分使用对齐的地图和 rPPG 监督进行训练，该监督检查跨视频帧的时间可变性。

#### 3.1.深度图监督

深度图是 2D 图像中人脸 3D 形状的表示，它显示了人脸位置和不同面部区域的深度信息。这种表示比二元标签信息量更大，因为它表明了实时人脸与打印和重放 PA 之间的根本区别之一。我们利用深度损失函数中的深度图来监督 CNN 部分。基于像素的深度损失引导 CNN 学习从感受野内的人脸区域到标记深度值的映射——[0, 1] 内的比例用于真实人脸，0 用于欺骗人脸。

为了估计 2D 人脸图像的深度图，给定一张人脸图像，我们利用最先进的密集人脸对齐 (DeFA) 方法 [25, 32] 来估计人脸的 3D 形状。正面密集的 3D 形状 $S_F ∈ R^{3×Q}$，具有 Q 个顶点，表示为单位基$ {\{S_i^{id}}\}_{i=1}^{N_{id}}$和表达式基 ${\{S^i_{exp} }\}_{i=1}^{N_{EXP}} $的线性组合，

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210702105417108.png" alt="image-20210702105417108" style="zoom:33%;" />

其中 $α_{id} ∈ \R^{199}$ 和$ α_{ext} ∈ \R^{29}$ 是标识和表达式参数，$α = [α_{id} , α_{exp} ] $是形状参数。我们利用Basel 3D人脸模型[39]和facewearhouse[15]作为身份和表情基础。使用估计的姿势参数$\bf{P}=(s，\bf{R}，\bf{t})$，其中$\bf{R}$ 是三维旋转矩阵，$\bf{t}$ 是三维平移，s是尺度缩放，我们将 3D 形状 $\bf{S}$ 对齐到 2D 人脸图像：



<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210702110231654.png" alt="image-20210702110231654" style="zoom:33%;" />

鉴于估计 2D 人脸绝对深度的挑战，我们将 S 中 3D 顶点的 z 值归一化到 [0, 1] 之内。也就是说，离相机最近的顶点（例如鼻子）的深度为 1，最远的顶点的深度为 0。然后，我们将 Z-Buffer 算法 [51] 应用于 S 以将归一化的 $z$ 值投影到 2D 平面，从而为面部图像生成估计的“地面实况”2D 深度图$\bf{D} ∈ \R^{32×32}$。

#### 3.2. rPPG监督

rPPG 信号最近已被用于面部反欺骗 [31,35]。rPPG信号提供关于面部活跃度的时间信息，因为它与面部皮肤随时间的强度变化有关。这些强度变化与血流高度相关。用于提取rPPG信号的传统方法有三个缺点。首先，它对姿势和表情变化很敏感，因为跟踪特定面部区域以测量强度变化变得更加困难。其次，它对光照变化也很敏感，因为额外的光照会影响皮肤反射光的数量。第三，出于反欺骗的目的，从欺骗视频中提取的rPPG信号可能无法与实时视频的信号充分区分。

我们方法的一个新颖之处在于，我们的 RNN 部分不是通过 [20] 计算 rPPG 信号，而是学习估计 rPPG 信号。这简化了来自具有 PIE 变化的面部视频的信号估计，并且还导致更具辨别力的 rPPG 信号，因为为实时视频与欺骗视频提供了不同的 rPPG 监督。我们假设同一主题在不同 PIE 条件下的视频具有相同的ground truth rPPG 信号。该假设是有效的，因为对于在短时间内（< 5 分钟）拍摄的同一主题的视频，心跳是相似的。从受约束视频中提取的 rPPG 信号（即，无 PIE 变化）被用作同一主题的所有实时视频的 rPPG 损失函数中的“ground truth”监督。这种一致的监督有助于 CNN 和 RNN 部分对 PIE 变化具有鲁棒性。

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210702160327817.png" alt="image-20210702160327817" style="zoom:50%;" />

图3.提出的CNN-RNN架构。卷积核的数量显示在每层的顶部，所有卷积核的大小为3 × 3 with stride 1 for convolutional and 2 for pooling layers。使用的颜色代码：橙色=卷积，绿色=池，紫色=response map。

为了从没有 PIE 的面部视频中提取 rPPG 信号，我们将 DeFA [32] 应用于每一帧并估计密集的 3D 面部形状。我们利用估计的 3D 形状来跟踪面部区域。对于跟踪区域，我们计算了两个正交的色度信号$x_f=3r_f−2g_f$，$yf=1.5r_f+g_f−1.5b_f$，其中$r_f$、$g_f$、$b_f$ 是具有肤色标准化的r、g、b通道的带通滤波版本。我们利用色度信号的标准差之比$γ = \frac{σ(xf)}{σ(yf）}$计算血液 流量信号 [20]。我们计算信号 p 为：

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210702170820896.png" alt="image-20210702170820896" style="zoom:33%;" />

通过对 p 应用 FFT，我们获得了 rPPG 信号$ f\in \R^{50}$，它显示了每个频率的幅度。

#### 3.3.网络架构

我们提出的网络由两个深度网络组成。首先，CNN 部分分别评估每一帧，并估计每一帧的深度图和特征图。其次，循环神经网络 (RNN) 部分评估序列特征图中的时间可变性。

#### 3.3.1 CNN网络

我们设计了一个全卷积网络 (FCN) 作为我们的 CNN 部分，如图 3 所示。 CNN 部分包含三个卷积层的多个块、池化和调整大小层，其中每个卷积层后跟一个指数线性层和批量归一化层。然后，the resizing layers将每个块之后的输出调整为预先定义的大小$ 64 \times 64$ 并连接响response maps。The bypass connections帮助网络利用从不同深度的层中提取的特征，类似于 ResNet 结构 [24]。之后，我们的 CNN 有两个分支，一个用于估计深度图，另一个用于估计特征图。

CNN 的第一个输出是输入帧$ I\in R^{256\times256 }$的估计深度图，由估计的“ground truth”深度 D 监督，

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210702174912961.png" alt="image-20210702174912961" style="zoom:33%;" />

其中$\Theta_D$ 是CNN参数，$N_d$ 是训练图像的数量。CNN 的第二个输出是特征图，它被输入到非刚性配准层。

#### 3.3.2 RNN网络

RNN部分的目的是估计具有$N_f$帧${\{I_j}\}_{j=1}^{N_f}$ 的输入序列的rPPG信号f。如图3所示，我们利用一个具有100个隐藏神经元的LSTM层、一个全连接层和一个FFT层，将全连接层的响应转换为傅立叶域。给定输入序列${\{I_j}\}_{j=1}^{N_f}$ 和ground truth rPPG信号 f ，我们训练 RNN 以最小化估计的 rPPG 信号到“ground truth” f 的 $l_1$ 距离，

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210702175927787.png" alt="image-20210702175927787" style="zoom:33%;" />

#### 3.3.3 Implementation Details

`Ground Truth Data` 给定一组实时和欺骗人脸视频，我们为深度图 D 和 rPPG 信号 f 提供ground truth 监督，如图 4 所示。我们遵循第3.1节中的程序计算实时视频的“ground truth”数据。对于欺骗视频，我们将ground truth深度图设置为平面，即零深度。类似地，我们按照第3.2节中的程序计算每个受试者的一个实时视频的“ground truth”rPPG信号，而不存在PIE变化。此外，我们对估计的 rPPG 信号的范数进行归一化，使得 $||f||_2=1$。对于欺骗视频，我们认为 rPPG 信号为零。

注意，虽然这里使用术语“深度”，但是我们估计的深度与计算机视觉中的传统深度图不同。它可以被看作是一种“伪深度”，其目的是为学习过程提供有区别的辅助监督。同样的观点也适用于基于伪rPPG信号的监控。

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210702182107235.png" alt="image-20210702182107235" style="zoom:50%;" />

​																	图 4. Example ground truth depth maps and rPPG signals.

`Training Strategy ` 我们提出的网络结合CNN和RNN部分进行端到端的训练。CNN部分所需的训练数据应来自不同的主题，以使训练过程更加稳定，并提高所学模型的泛化性。同时，RNN部分的训练数据应该是长序列，以便跨帧利用时间信息。这两个偏好可能相互矛盾，特别是考虑到有限的GPU内存。因此，为了区分两种偏好，我们设计了一种双流训练策略。第一个流满足CNN部分的偏好，其中输入包括面部图像$I$ 和ground truth深度图$D$ 。第二个流满足RNN 部分，其中输入包括人脸序列${\{I_j}\}_{j=1}^{N_f}$ 、ground truth深度图${\{D_j}\}_{j=1}^{N_f}$、估计的3D形状${\{S_j}\}_{j=1}^{N_f}$，以及相应的ground truth rPPG信号f。在训练过程中，我们的方法在这两个流之间交替地收敛到一个模型，该模型使深度图和rPPG损失最小化。注意，即使第一流仅更新CNN部分的权重，第二流的反向传播也以端到端的方式更新CNN和RNN部分的权重。

`Testing ` 为了提供分类分数，我们将测试序列输入到我们的网络中，并计算最后一帧深度图$\hat{D}$ 和rPPG信号$\hat{f}$ 。我们没有使用 $\hat{D}$  和$\hat{f}$ 设计分类器，而是将最终分数计算为：

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210702181533393.png" alt="image-20210702181533393" style="zoom:33%;" />

其中$\lambda$ 是用于组合网络的两个输出的恒定权重。

#### 3.4.非刚性配准层

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210702182144569.png" alt="image-20210702182144569" style="zoom:50%;" />

​																								图5. 非刚性配准层

我们设计了一个新的非刚性配准层来为RNN部分准备数据。该层利用估计的密集3D形状来对齐CNN部分的激活或特征图。这一层对于确保RNN跟踪和学习同一面部区域随时间以及所有受试者的激活变化非常重要。

如图5所示，该层有三个输入：特征图$T\in \R$，深度图$\hat{D}$，3D形状$S$ 。在这个层中，我们首先对深度图设置阈值，并生成一个二元掩码$\bf{V}\in \R^{32\times32}$ :

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210702230639882.png" alt="image-20210702230639882" style="zoom:33%;" />

然后，我们计算二值掩码和特征图的内积$ U = T\odot V $，它本质上利用深度图作为特征图中每个像素的可见性指标。如果一个像素的深度值小于阈值，我们认为该像素是不可见的。最后，我们利用估计的 3D 形状 S 对 U 进行frontalize ，

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210702231028248.png" alt="image-20210702231028248" style="zoom:33%;" />

其中$m\in\R^K$是$S_0$ 中面部区域的K个索引的预定义列表，$m_{ij}$ 是像素$i，j$ 的对应索引。我们利用 m 将掩码激活图 U 投影到正面图像 F。 这个proposed非刚性配准层对我们的网络有三个贡献：

* 通过应用非刚性配准，输入数据对齐，RNN 可以比较特征图，而无需考虑面部姿势或表情。换句话说，它可以学习相同面部区域的特征图激活的时间变化。
* 非刚性配准去除了特征图中的背景区域。因此背景区域不会参与 RNN 学习，尽管背景信息已经在 CNN 部分的层中使用。
* 对于假的脸，深度图可能更接近于零。因此，深度图的内积大大削弱了特征图中的激活，这使得 RNN 更容易输出零 rPPG 信号。同样，来自 rPPG 损失的反向传播也鼓励 CNN 部分为所有帧或输入序列中大多数帧中的一个像素位置生成零深度图。

![image-20210702231606403](/Users/lishuo/Library/Application Support/typora-user-images/image-20210702231606403.png)

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210702231616907.png" alt="image-20210702231616907" style="zoom:50%;" />

​                                                    图6.SiW数据库中受试者的统计数据。左侧：直方图显示了面部大小的分布。

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210702231651934.png" alt="image-20210702231651934" style="zoom:50%;" />

​                                                                           图7.SiW中的实时（顶部）和欺骗（底部）视频示例。

### 4.人脸防欺骗数据库收集

随着传感器技术的进步，现有的反垃圾邮件系统很容易受到新兴的高质量欺骗媒体的攻击。使系统对这些攻击具有鲁棒性的一种方法是收集新的高质量数据库。为了响应这一需求，我们收集了一个新的人脸反欺骗数据库，名为Spoof In the Wild（SiW）database，它比Tab中的以前的数据集具有多个优点。1.首先，它包含了大量不同种族的活体受试者，例如，受试者是Oulu-NPU的3倍。请注意，MSU-USSA是使用现有的名人图片构建的，没有捕捉到真实的面孔。第二，现场视频是用两个高质量的摄像头（佳能EOS T6，罗技C920网络摄像头）不同的PIE变化捕捉。

SiW 提供来自 165 个主题的实时和欺骗性 30-fps 视频。对于每个主题，我们有 8 个直播和 20 个欺骗视频，总共 4620 个视频。部分受试者的统计数据如图 6 所示。实时视频收集在四个会话中。在会话 1 中，对象移动他的头部，使其与相机的距离不同。在Session 2中，受试者在$[ -90^{\circ},90^{\circ}]$内改变头部的偏航角，并做出不同的面部表情。在会话 3、4 中，主体重复会话 1、2，同时收集器从不同方向围绕面部移动点光源

两台摄像机拍摄的实时视频分辨率均为$ 1, 920\times 1,080$。我们为每个主题提供两个打印和四个重放视频攻击，示例如图 7 所示。为了生成不同质量的打印攻击，我们为每个主题捕获高分辨率图像 ($5, 184\times 3, 456$) 并使用它进行高质量的打印攻击。此外，我们从实时视频中提取正面视图帧以进行低质量打印攻击。我们使用 HP 彩色 LaserJet M652 打印机打印图像。打印攻击视频是通过保持打印的纸张静止或在摄像机前扭曲它们来捕获的。为了生成高质量的重播攻击视频，我们选择了四种欺骗媒体：三星 Galaxy S8、iPhone 7、iPad Pro 和 PC（华硕 MB168B）屏幕。对于每个主题，我们随机选择四个高质量直播视频中的两个在欺骗媒体中显示。

### 5.实验结果

#### 5.1. 实验设置

`Databases `我们在多个数据库上评估了我们的方法，以证明其通用性。我们利用SiW和Oulu数据库[14]作为新的高分辨率数据库，并在它们之间执行内部和交叉测试。此外，我们使用CASIA-MFSD[50]和Replay Attack[17]数据库进行交叉测试，并与现有技术进行比较。

`Parameter setting `所提出的方法在 TensorFlow [3] 中实现，学习率为 $3e-3$，训练阶段为 10 个epoch。 CNN stream 的batch size为 10，CNN-RNN stream 的batch siz为 2，$N_f$ 为 5。我们使用均值为零且标准差为 0.02 的正态分布随机初始化我们的网络。我们将 Eq.6 中的$\lambda$ 设置为 0.015，将 Eq.7 中的阈值设置为 0.1。

`Evaluation metrics `为了与以前的工作进行比较，我们report了以下指标的结果：攻击表示分类错误率APCER[2]、真实表示分类错误率bpcer[2]、$ACER=\frac{AP CER+BP CER}{2}$[2]和Half Total Error Rate HTER。HTER是错误拒绝率（FRR）和错误接受率（FAR）之和的一半。

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210704221400026.png" alt="image-20210704221400026" style="zoom:50%;" />

#### 5.2. 实验比较

#### 5.2.1消融实验

`Advantage of proposed architecture `我们比较了四种结构来说明所提出的损失层和非刚性配准层的优点。模型1的体系结构与我们的方法中的CNN部分（图3）相似，不同之处在于它扩展了额外的池化层、全连接层和用于二进制分类的softmax损失。模型2是我们方法中的CNN部分，具有深度图损失函数。我们只是使用$|| \hat{D }||^ 2$ 进行分类。模型3包含CNN和RNN部分，没有非刚性配准层。该模型同时利用了深度图和rPPG损失函数。但是，RNN部分将处理来自CNN的未配准的特征图。模型4是proposed的体系结构。

我们用SiW的20个主题的现场视频和欺骗视频来训练所有四个模型。计算了Oulu数据库协议1上所有模型的交叉测试性能。不同FDR的TDR在表中报告。2.模型1由于二进制监督，性能较差。相比之下，通过仅使用深度图作为监视，模型2获得了更好的性能。然而，在加入带有rPPG监督的RNN部分之后，我们提出的模型4可以进一步提高性能。通过比较模型4和模型3，可以看出非刚性配准层的优势。显然，RNN部分不能直接使用特征映射来跟踪激活的变化和估计rPPG信号。

`Advantage of longer sequences `为了显示使用较长序列来估计rPPG的优势，我们在序列长度Nf为5、10或20时对我们的模型进行训练和测试，在Oulu协议2上使用内部测试。3.我们可以看到，随着序列长度的增加，由于rPPG估计更加可靠，ACER减小。尽管有较长序列的好处，但实际上，我们受到GPU内存大小的限制，被迫将图像大小减小到$128\times 128$用于表3中的所有实验。3.因此，我们将$N_f$ 设置为5，图像大小为$256\times256$ 在随后的实验中，由于更高分辨率的重要性（例如，表4中2.5%的ACER低于4.16%）。

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210704222731945.png" alt="image-20210704222731945" style="zoom:50%;" />

#### 5.2.1 内部测试

我们对Oulu和SiW数据库执行内部测试。对于Oulu，我们遵循四个协议[10]，并报告它们的APCER、BPCER和ACER。选项卡。4展示了在面对反欺骗竞争时，我们提出的方法和每种协议的最佳两种方法的比较[10]。我们的方法在4个协议中有3个实现了最低的ACER。我们在方案2上的ACER稍差一些。为了为将来的SiW研究设定基线，我们为SiW定义了三个方案。协议1处理面部姿势和表情的变化。我们使用训练视频的前60帧（主要是正面视图）进行训练，并对所有测试视频进行测试。协议2评估了重放攻击的交叉欺骗媒体的性能。协议3评估交叉PA的性能，即从打印攻击到重放攻击，反之亦然。选项卡。图5显示了协议定义和每个协议的性能。

#### 5.2.3交叉试验

为了证明该方法的通用性，我们进行了多次交叉测试实验。我们的模型通过SiW中80名受试者的现场视频和恶搞视频进行训练，并对Oulu的所有协议进行测试。协议1-4的ACER分别为：10.0%，14.1%，13.8%± 5.7%和10.0%± 8.8%. 将这些交叉测试结果与[10]中的内部测试结果进行比较，我们在face anti-Spooking竞赛的15名参与者中，四个协议的平均ACER排名第六。特别是在协议4中，最难的协议之一，我们实现了10.0%的ACER作为顶级执行者。这是一个值得注意的结果，因为交叉测试要比内部测试困难得多，而且我们的交叉测试结果与最高的内部测试性能相当。这证明了我们所学模型的泛化能力。

![image-20210704223015472](/Users/lishuo/Library/Application Support/typora-user-images/image-20210704223015472.png)

图8.（a）8个成功的反欺骗示例及其估计的深度图和rPPG信号(b） 4个失败例子：前两个是实时的，另外两个是欺骗的。注意我们的能力，估计有区别的深度图和rPPG信号。

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210704223038202.png" alt="image-20210704223038202" style="zoom:50%;" />

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210704223048822.png" alt="image-20210704223048822" style="zoom:50%;" />

​                                                                               图9.live和spoof的正面化特征图的平均值/标准差。

此外，我们利用CASIA-MFSD和ReplayTack数据库进行交叉测试，这是广泛使用的交叉测试基准。表6比较了不同方法的交叉测试结果。我们提出的方法在重放攻击和CASIA-MFSD数据库上的交叉测试错误分别比以前的SOTA减少了8.9%和24.6%。

#### 5.2.4可视化和分析

在所提出的架构中，前端化的特征图被用作RNN部分的输入，并由rPPG损失函数进行监控。这些图的值可以显示不同面部区域对rPPG估计的重要性。图9显示了根据Oulu的1080个live视频和spoof视频计算出的锋面化特征图的平均值和标准差。我们可以看到额头和脸颊的侧面区域对rPPG估计有较大的影响。

当我们的系统的目标是检测PAs时，我们的模型被训练来估计辅助信息。因此，除了反欺骗，我们还喜欢评估辅助信息估计的准确性。为此，我们计算了深度图和rPPG信号的估计精度，用于Oulu协议2中的测试数据。如图10所示，欺骗数据中的两种估计的准确度都高，而实时数据的准确度相对较低。注意，口腔区域的深度估计有更多的误差，这与图9中相同区域的较少激活是一致的。估计深度图和rPPG信号的成功和失败案例如图8所示。

最后，我们对失效案例进行统计分析，因为我们的系统可以使用辅助信息来确定潜在的原因。使用欧鲁的proctocl2，我们确定了31例失败病例（ACER为2.7%）。对于每种情况，我们计算是否反欺骗使用其深度图或rPPG信号将失败，如果单独使用该信息。在总数中，$\frac{29}{31},\frac{13}{31}$and$\frac{11}{31}$的样本由于深度图、rPPG 3131信号或两者都失败。这为今后的研究指明了方向。

### 6.结论

本文指出了辅助监控在基于深度模型的人脸反欺骗中的重要性。该网络结合CNN和RNN两种网络结构，联合估计人脸图像的深度和人脸视频的rPPG信号。我们引入了SiW数据库，它比以前的数据库包含更多的主题和变体。最后，通过实验验证了该方法的优越性。

`Acknowledgment`这项研究基于国家情报总监办公室、情报高级研究项目活动(IARPA)通过IARPA研发合同编号201717020200004支持的工作。本文中包含的观点和结论是作者的观点和结论，不应被解释为一定代表ODNI、IARPA或美国政府的官方政策或背书，无论是明示的还是默示的。美国政府有权为政府目的复制和分发再版，尽管其上有任何版权注释。

