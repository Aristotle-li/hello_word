在追求人工智能的过程中，我们对进度最重要的衡量标准是代理在广泛的环境中实现目标的能力。现有的构建此类环境的平台通常受到其所基于的技术的约束，因此只能提供评估进度所必需的部分方案。为了克服这些缺点，我们介绍了Unity的使用，Unity是一种广为人知的综合游戏引擎，可用于创建更加多样化，复杂的虚拟仿真。我们描述了旨在简化这些环境的创作而开发的概念和组件，这些概念和组件主要用于强化学习领域。我们还将介绍一种实用的方法来打包和重新分发环境，以尝试提高实验结果的鲁棒性和可重复性。为了说明与其他解决方案相比，我们使用Unity的多功能性，我们重点介绍了使用已发表论文中的方法已经创建的环境。我们希望其他人可以从Unity如何适应我们的需求中汲取灵感，并随着熟悉程度的提高，从我们的方法中看到越来越多样化和复杂的环境

1.引言

在过去的十年中，强化学习在追求实现通用智能方面取得了重大进展（例如Espeholt等人，2018; Mnih等人，2015; Silver等人，2016）。通用情报一词的广泛接受的定义是“代理商在广泛的环境中实现目标的能力”（Legg和Hutter，2007年）。这样一来，虚拟环境的创建量就增加了，既可以探测特定的认知能力，也可以作为座席绩效的基准。长期以来，游戏一直被用作评估人工智能进展的一种手段，例如西洋双陆棋（Tesauro，1994），国际象棋（Campbell等，2002）或围棋（Silver等，2016）。最近，视频游戏也证明了丰富的学习环境。热门示例包括Arcade学习环境（Bellemare等，2013），Retro学习环境（Bhonker等，2016; Nichol等，2018），多人第一人称游戏Quake III（Jaderberg等，2019） ，以及实时策略游戏《 Dota 2》（OpenAI等，2019）和《星际争霸II》（Vinyals等，2017）。桌游和视频游戏之所以受欢迎，是因为它们提供了丰富的综合观测数据来源，通常具有明确的成功衡量标准，可以凭经验评估进度，并且可以根据人员绩效直接评估座席绩效。此外，此类游戏还具有额外的有效性衡量标准，因为它们不是由AI研究人员定义的，也不是专门为AI研究目的而设计的。随着对更复杂环境的需求增加，将现有交互游戏转变为有用的研究环境。因此，已经发生了向使用视频游戏的底层引擎作为创建更大环境套件的平台的转变。例子包括VizDoom（Kempka等，2016），DeepMind Lab（Beattieet等，2016）和Malmo（Johnson等，2016），它们分别使用流行视频游戏Doom，Quake III和Minecraft的底层引擎。这些平台虽然在推动进步方面发挥了重要作用，但它们所基于的游戏在设计上都存在限制其环境多样性潜力的设计约束。例如，DeepMind实验室证明是探索复杂导航任务（Banino等人，2018）和经典实验室心理实验（Leibo等人，2018）等的绝佳平台，但仅限于第一人称视角和原始物理学。这将平台的范围限制在相对狭窄的可能实验范围内，并影响了平台在评估通用情报方面的适用性。与此同时，视频游戏行业也发生了发展，现在大多数游戏都是使用更多通用引擎创建的，这些通用引擎从一开始就被开发出来，以便能够在其核心技术上构建各种各样的游戏。创建广泛的强化学习环境（Sutton和Barto，2018），我们选择使用流行的商业游戏引擎Unity（Unity Technologies，2018）。 Unity在视觉和仿真逼真度方面的灵活性，程序员友好的工具以及庞大而活跃的社区使其成为构建学习环境的基础的绝佳选择。在设计方法时要考虑的另一个重要考虑因素是可再现性问题，即“研究人员能够使用与原始研究者所使用的相同的材料和程序复制先前研究的结果的能力”（Bollen等人，2015）。这已成为包括加强学习社区（Henderson等，2017; Pineau等，2020）在内的多个科学领域（Baker，2016; Dodge等，2019）的紧迫课题。开发高级概念来使Unity适应我们的需求。我们详细介绍了通过开源通信协议与这些环境进行交互的方式。我们提出了一种实用的方法，以易于分发的形式打包预先构建的环境及其所有依赖关系。最后，我们重点介绍使用此方法发表的几篇论文，以评估解决方案的成功性

2.相关工作

使用Unity创建强化学习环境已有一些先例。实际上，Unity的创建者已经发布了一个插件：Unity ML-Agents工具包（Juliani等，2020）。这是一个现成的工具包，其中包括各种学习算法，训练方案和样本环境，以及连接外部定义的代理的机制。该工具包的重点是简化使现有Unity项目可作为学习环境使用的过程，然后生成经过培训的代理。对于使用此工具包培训过的代理人，预计将有各种各样的游戏开发应用程序，例如控制非玩家角色，实现更丰富的自动化测试或帮助制定新的游戏设计决策。他们的工具包还提供了Python API，以允许外部定义的代理与已配置的游戏进行交互。尽管我们的方法与Unity ML-Agents工具包之间有很多相似之处，但解决方案的主要目的有所不同。我们的工作重点是尽可能简化构建一组异构环境，并将它们打包为可配置的黑盒服务器，任何数量的代理都可以连接到该服务器。环境和代理之间关注点的强烈分离旨在鼓励研究自主性，从而使学习方法的进步独立于模拟创作而发生。我们反对采用简单的方法将预先训练的代理直接嵌入到环境中进行交易，而我们的方法可能不适合这种方法。我们的去耦方法还有其他一些好处，例如分布式培训，持续模拟，简化的多代理支持以及更好地保证实验可重复性。值得注意的是，这两个命题在总体目标上都有同等的价值。除Unity ML-Agents工具包外，还开发了其他几个平台来创建复杂的环境。如第1节所述，这些平台中的大多数（Beattie等，2016； Johnson等，2016； Kempka等，2016）都利用了现有的视频游戏引擎。其他诸如DeepMind Control Suite（Tassa等人，2018）的专家则使用了专业引擎，例如MuJoCo，这是一种高质量的物理引擎。尽管每个平台都提供了一定程度的环境自定义功能，但它们还依赖于专门为特定需求而构建的高度专业化的引擎。这种专业化意味着在这些平台内创建的环境固有地受到底层引擎的约束。例如，基于MuJoCo的环境能够准确地模拟多关节动态，但在生成视觉上准确的场景表示方面的能力较低。在某些情况下，这种专业化是可取的，因为它可以确保一定程度的一致性。例如，使用基于视频游戏《我的世界》的平台Malmo（Johnson等人，2016），在呈现世界的方式以及代理如何与世界互动方面，存在内在的统一性。这可以奠定代理的学习基础，简化知识从一项任务到另一项任务的转移。朱莉安娜（Juliani）等。 （2020年）提供了对这些平台和其他平台的更深入分析。我们考虑到使用Unity的多功能性及其对创建各种任务的适用性，以此作为对这种隐式一致性的可接受的折衷方案。我们还尝试通过重新提出软件领域的解决方案来提高科学研究的可重复性工程（Tanner和White，2009年）。一个这样的解决方案是使用“ Docker”（Boettiger，2015年），该机制提供了一种打包软件的机制，以可靠地从一个计算环境运行到另一个计算环境。在AI研究领域内使用Docker有先例。一个示例是OpenAI Universe（OpenAI，2016年），该平台使用Docker打包环境，代理通过远程桌面协议（VNC）和辅助连接进行交互。我们建议使用类似的方法，但不是使用通用协议（例如VNC），而是使用更定制的，定义明确的通信方式。与使用通用协议相比，它具有更高效，实用和可靠的优点
3.技术细节

为了创建推进人工智能所需的大量异构环境，我们在Unity中建立了我们的解决方案。 Unity可以视为两个独立的部分：跨平台运行时环境（称为“播放器”）和内容创建工具（称为“编辑器”）。使用Unity编辑器，我们通过创建带有一个或多个场景对象的Unity项目来产生新的环境。每个场景代表模拟的不同初始世界状态。场景由游戏对象实体的层次结构组成。每个游戏对象代表环境中的一个物理或逻辑项目，其行为由区域分配给它的组件集确定。 Unity提供了许多常见组件作为标准。例如用于分配几何形状的网格组件，用于通过物理模拟器控制对象位置的刚体组件，或用于确定如何将GameObject绘制到屏幕上的渲染器组件。重要的是，还可以利用Unity的C＃脚本API创建新组件



<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210127161928069.png" alt="image-20210127161928069" style="zoom:50%;" />

图2j我们各层及其相互作用的高级概述。有关接口，会话和通信层的更多详细信息，请分别参见3.2、3.3和3.4节



3.1。概述现在我们考虑将这些环境暴露给学习代理的问题。从高层的角度来看，我们的方法可以描述为三个相互连接的层：接口，会话和通信层。每一层都负责代理与环境交互的特定阶段，从将Unity项目转换为强化学习环境，一直到与一个或多个代理进行通信所必需的低级通信协议。图2提供了广泛的概述每层的关系以及它们之间的相互关系

3.2。化身，执行器和传感器

代理通过控制anAvatar（通常由aGameObject表示）来与模拟进行交互。阿凡达的工作是提供执行器和传感器的集合。执行器提供了一种媒介可以影响模拟的方式，而传感器则是媒介如何观察模拟的方式。阿凡达通常只是一个物理体现，但不是一定是必要的-它可能是抽象的东西，例如工厂的冷却系统，或者能够观察模拟某些特征的长相控制器。我们使用C＃属性来标记作动器或传感器等组件对象上的字段。然后将这些组件附加到Avatar的GameObject，然后通过扫描GameObject的这些属性来汇总这些属性，以创建最终的执行器和传感器集。我们还允许通过使用属性参数来包含辅助信息。其中的示例包括自定义执行器或传感器名称，定义执行器的有效范围或相关传感器元数据的方式。清单1显示了一个示例，该示例公开了两个执行器：MOVE_BACK_FORWARD，其有效范围为1：1：1º，以及一个布尔执行器JUMP。

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210127162145097.png" alt="image-20210127162145097" style="zoom:50%;" />

提供对所有基本类型的支持，传感器的尺寸可以从类型推断出，也可以通过Shapeattribute参数明确定义。我们还为更复杂的Unity类型提供支持，其中最突出的是Unity Cameras。当代理要求从“摄像头传感器”类型进行观察时，我们要求Unity将“摄像头”的视图渲染为多维字节数组。清单2显示了代理可以观察到的四个不同传感器类型的示例



<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210127162217604.png" alt="image-20210127162217604" style="zoom:50%;" />

3.2.2。任务

任务的目的是定义环境中座席的工作条件。其中包括：

•代理控制的头像。一个任务可以生成一个新的AvatarGameObject，或者占有已经存在的一个。

•代理的当前奖励。该奖励以单个标量值的形式公开，该任务每步进行一次离散评估，即通常用于强化学习（Sutton和Barto，2018）。也可以得出如有必要，可以使用Avatar Sensors的输出来生成更复杂的奖励信号。

•如何开始新的情节。Task实现了StartEpisode功能来开始新的情节。在情节开始时，通常还会将模拟重置为从固定分布得出的状态。•情节可以终止的条件。达到最终状态时，可以查询任务以了解情节终止的类型。这使业务代表可以消除情节是否达到终极状态或例如是否达到预定时间限制的歧义。•在模拟中如何以及何时提前时间。任务可能还需要操纵业务代表对时间的感知。例如，我们可能希望进行几秒钟的物理模拟，以准备开始新的情节。有关步进时间的更多详细信息，请参见第3.3.1节。通过将Task逻辑与用于构建模拟世界的场景内容分开，我们可以混合和匹配Tasks和内容，以为代理创建各种不同的场景。可以通过代理提供的设置进一步配置任务。我们还可以为每个connectedagent分配不同的任务。因此，可以想到的是，连接到同一模拟的特工会经历不同的剧集边界，甚至经历时间的不同概念，尽管最常见的多人游戏设置是所有特工都可以按步进行游戏



3.3。会话LayerASession表示单个代理与环境的连接的当前状态。它负责响应从通信层收到的代理请求。代理连接后，aSessionFactory会根据提供的设置创建一个新的会话。在实践中，会话主要负责合并Task和Avatar组件。例如，当代理人踏入环境时，会话会将动作转发给阿凡达的执行器，并汇总来自相应阿凡达传感器的请求观察值。会议还负责处理标准强化学习范式之外的其他请求，例如环境诊断信息，或使研究人员能够进行受控干预以修改模拟结果。3.3.1。提前TimeUnity提供了一系列连续的离散模拟帧来逼近连续时间。每个帧都可以使模拟状态提前一小部分“增量时间”。在典型的游戏设置中，Unity为下一帧动态调整此增量时间，以保持仿真的幻觉，其运行速度与真实世界相同。例如，如果场景变得太复杂而无法模拟，Unity将增加帧之间的时间，从而降低了模拟的保真度。对于可重现的机器学习结果，这种自适应实时帧速率行为可能是不可取的。为了减轻这种行为，我们将Unity切换为对帧之间的时间具有显式控制的模式。具体来说，我们可以通过控制Unity的Time.captureFrameRateandTime.timeScale属性来实现。注意，不需要代理观察每个模拟帧。座席会议的职责是确定何时处理座席动作，以及在返回相应观察值之前要等待多长时间。例如，在构造环境中（有关更多详细信息，请参见第5节），每个代理动作在环境中放置一个新块。在智能体收到下一个观察值之前，环境会先进行框架模拟球在放置的木块上滚动，并在球达到其随后的静止状态后返回观察值。结果是代理观察到一个

动作/观察对，但环境已经发展了许多框架。我们通过一个称为“世界时间管理器”（WTM）的全局对象协调环境和连接的代理。 WTM具有两个主要功能。首先，它提供集中调度服务，以精确执行每个座席对各自会话的请求。这样可以确保在模拟状态更改之间，在更新循环中的特定点可靠地处理了代理请求。其次，WTM根据执行计划的代理请求的结果来操纵仿真时间的增加。为实现此目的，我们的实现要求严格根据会话上的操作在会话层内定义所有代理请求，并且在完成后需要每个请求以返回模拟进度优先级。我们将此首选项描述为aTickState。ATickState可以是下列值之一：•MustTick：必须在执行下一个动作之前进行模拟。•MustNotTick：必须在执行下一个动作之前不允许进行模拟。•MayTick：可以选择进行模拟选择根据外部条件来推进，例如达到处理代理动作的最大时间。代理交互由工作线程处理。代理控制器使用线程安全队列来安排WTM在特定的滴答上执行的工作。在每个滴答声的结尾，WTM都会创建一个“待处理”列表，表示仍对当前模拟框架有未决请求的代理。接下来，WTM通过将代理请求顺序发送到其Session并接收相应的TickState来处理每个队列。如果某个操作返回“ MustTick”，则WTM会将座席队列中的其余请求转移到新的“未来”列表中。处理过程将继续处理所有剩余队列，直到每个队列返回MustTick或座席队列为空。此时，WTM允许Unity运行其更新循环，然后WTM将将来的列表转移到新的待处理列表中。此方法允许所有连接的代理同时处理观察，同时在与仿真的交互中保持时间一致性.

3.4 。通讯层通讯层有两个主要职责：在每个代理与环境之间建立通信通道，并编排代理与会话层之间的请求和响应。对于通信通道，我们使用Google的高性能，开源远程过程调用（RPC）框架，gRPC（gRPC，2020年）。具体来说，我们使用其双向streamingRPC变体从每个连接的代理发送和接收消息的有序流。 gRPC具有与多种运行时平台和编程语言一起使用的多功能性，非常适合我们的代理与环境互操作性。接下来，我们需要一个消息传递约定来进行通信。为了实现这一目标，我们引入了m_env_rpc（Ward和Lemmon，2019），这是一种旨在标准化机器学习代理与环境之间通信的消息传递协议。每个代理客户端都有到Unity环境服务器的单个双向RPC流。服务器可以接受多个以上的同时代理连接，具体取决于模拟是否支持这种用例。每个服务器针对每个代理请求准确地发送一个响应，其中响应消息与请求消息相对应（例如，aStepRequest会产生aStepResponse），或者是错误消息。在非典型的单代理场景中，一个代理将按以下方式进行交互：

1.代理发送一个CreateWorldRequest，它根据请求的设置（例如要加载哪个Unity场景）来设置Unity模拟2.代理接下来发送一个带有一组以代理为中心的可选设置的JoinWorldRequest，其中可能包括要使用哪种头像实例化，或加入多代理设置的团队。 JoinWorldResponse返回代理可以用来与环境进行交互的动作和观察结果。3。代理现在能够发送𝑁StepRequest消息以步进环境。每个步骤请求都包含一组要应用的稀疏动作，以及期望响应的观察值。 AStepResponse会填充这些请求的观察值，从代理的Avatar传感器中检索到的信息以及代理的Task是否已达到终端状态。4。代理可以随时发送ResetRequest以开始新的Task情节，或者可以发送ResetWorld-Request将模拟设置回其初始状态。初始状态。有关代理可以发送的这些请求和其他请求的更多信息，请参见dm_env_rpc文档，网址为http://github.com/deepmind/dm_env_rpc.3.5。代理API虽然代理可以通过dm_env_rpc直接与环境进行交互，但该协议主要用于描述在线协议格式，因此有利于性能而非可用性。为了简化代理的互换，dm_env_rpc包括流行的强化学习界面的实现。一旦这样的API是dm_env（Muldal等，2019），Python界面已经在DeepMind的Control Suite（Tassa等，2018）等环境中使用。 dm_envex构成dm_env_rpc协议的子集，以Python开发人员熟悉的方式提供功能。有关dm_env API的更多详细信息，请参阅位于http://github.com/deepmind/dm_env.

3.6的文档。可重现性为了最好地确保在我们的环境中可靠地重现结果，我们建议使用“容器”与我们前面提到的通信层协同工作。容器是“打包代码及其所有依赖项的标准软件单元，因此应用程序可以从一个计算环境快速可靠地运行到另一个计算环境”（Docker，2020年）。与在物理设备级别虚拟化的传统VirtualMachines（VM）不同，容器是应用程序层的抽象。这意味着容器比VM对应的容器更精简，性能更高，并且可以更轻松地共享资源。容器彼此之间都是隔离的，从而使每个环境都有自己独特的一组软件，库和配置文件。容器也是可组合的，这意味着在重现结果时更容易集成到其他系统中。为了提供此容器功能，我们使用了流行的容器化平台Docker.Docker在广泛的操作系统和云平台上得到支持，从而确保了我们的Unity环境能够在多个位置运行。它支持版本控制，这意味着同一环境的历史版本可以同时运行。 Docker还提供了进一步简化软件应用程序打包和运行的技术。例如，它提供了通过创建一个称为“ Dockerfile”的文本文件来创建新Docker映像的方法。这是一个简单的脚本，类似于Makefile，它总结了组装映像和运行应用程序所需的步骤。这不仅简化了新映像的创建，而且还提供了人类可读的概要文件，其中包括启动环境所需的依赖项，环境变量和参数。清单3显示了Unity应用程序的Dockerfile示例，名为“ dm_unity_examples”。该示例使用了Docker保留的最小图像抓痕，这表明该图像没有取决于任何基本映像，并复制adm_unity_examples目录的内容。然后，它在运行Unity应用程序之前设置几个环境变量来定义要使用的渲染器（有关渲染的更多信息，请参见第4.1节）。



<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210127162623929.png" alt="image-20210127162623929" style="zoom:50%;" />



使用Docker打包和运行环境，使用gRPC建立连接，使用dm_env_rpc进行通信以及使用dm_env简化代理与环境的交互，我们拥有构建连接代理和环境的可靠可靠方法所需的所有组件。清单4提供了参考实现。代理如何启动示例Docker容器，连接并发送操作直到情节结束的说明

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210127162656545.png" alt="image-20210127162656545" style="zoom:50%;" />
4.表现

尽管我们的方法的简单性和多功能性至关重要，但它必须具有高效的计算能力。在传统的视频游戏中，开发人员力求达到持续低的水平每帧延迟，以实现流畅，实时的体验。对于机器学习环境，相对于可用的计算资源，目标是产生最大量的模拟体验。为了最大化模拟吞吐量，我们尽可能并行地运行环境的多个实例。代理通过使用分布式架构来利用这种并行化（Nair等人，2015），将学习扩展到代理可以利用的尽可能多的机器（Espeholt等人，2018）。这种范式从每帧延迟转变为总吞吐量，从对于人工玩家而言，这对我们使用Unity的方式有影响。我们最显着的变化如下：•通过消除不再需要的同步点（例如，协调何时刷新显示设备），使仿真比实时运行得更快。•鼓励代理为每个Unity实例运行越长越好。这样一来，我们就可以分摊任何启动成本，例如加载3D网格或图像。•直观上，我们禁用多线程以减少在并行运行Unity的多个实例时的CPU争用，并避免了线程同步的计算开销。其他更改包括禁用不必要的Unity功能，默认情况下以33ms的时间增量运行仿真，等效于30Hz.

4.1的更新频率

。渲染通常，游戏引擎通过直接渲染到物理显示设备来呈现像素数据。机器学习代理通常在没有这种设备的计算机上运行。通过添加对渲染的支持来解决此问题，而无需物理显示，也称为“无头渲染”。这些设备除了没有显示设备外，可能无法访问GPU来获取硬件加速的图形，或者不希望使用此设备进行渲染。为此，我们还使用通过Mesa3d提供的开源LLVMpipe软件光栅化程序为在CPU上渲染提供支持（Paul等人，2020年）。为了提高性能，我们再次禁用多线程以减少CPU争用和线程开销，但同时也禁用纹理压缩。这是因为CPU渲染缺少用于纹理查找的专用硬件，因此通过禁用它，我们以纹理内存为代价提高了CPU渲染吞吐量。

4.2。性能结果

由于环境性能在很大程度上取决于模拟的复杂性和代理要求的观察，因此我们根据具有代表性的强化学习任务（称为“寻求避免”）提供结果。此任务是第一人称的3维环境，代理商的目标是尽可能多地摘苹果，同时避免柠檬。为代理提供了每个模拟步骤的像素观察结果，该情节在固定的时间后结束。表1显示了在我们的参考环境中运行多个并发Unity实例时的总帧速率（帧/秒）。我们通过向每个环境发送随机动作并接收RGB像素观测值来捕获总帧率，该观测值以96 72的分辨率呈现，这对于学习代理来说是典型的（Paine et al。，2019）。以下结果记录在具有两个18核Intel Xeon Gold 6154 3GHz CPU和NVIDIA Quadro P1000 GPU的Linuxdesktop上。10

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210127162859156.png" alt="image-20210127162859156" style="zoom:50%;" />

5.结果

为了评估我们使用Unity的成功性，我们通过以下论文的选择展示了多功能性的证据，这些论文的发表使用的是使用我们的方法创建的环境。

•物理构造的结构化代理（Bapst等，2019）：本文使用连续的，由程序生成的2D世界，该世界测试一个Agent解决各种基于物理的构造任务的能力。对于每个步骤，代理都可以从要选择放置在场景中的块中进行选择，也可以将现有的块粘合在一起以达到各种目标。一旦采取行动，环境将向前运行仿真，直到所有模块都停止运行。如果任务已完成，特工放置的任何障碍物与障碍物相交或超过最大动作次数，则该情节终止。

•有效利用示威解决难题（Paine等人，2019）：初始条件高度可变的八项艰苦探索，第一人称3D任务套件。在每个任务中，代理必须与世界上的对象进行交互才能获得提供奖励的大苹果。

•使用RL的生成环境模型来塑造信念状态（Gregor等，2019）：本文利用了三种不同的环境，其中两个是使用我们的方法构建的：

– Random City：程序生成的3D导航环境。在每个片段开始时，将彩色框（即“建筑物”）的均匀分布放置在2D平面上，然后将其用于生成训练数据以分析模型的置信状态（此实验中不进行强化学习）。 ：基于体素的程序环境，代理可以通过构建机制对其进行修改。环境由放置在固定3维网格中的不同块类型组成。其中一些区块可以由代理人拾取并放置，目标是每个任务变体消耗所有黄色区块，每个区块都有正面奖励。

•具有工作记忆和情节记忆的强化学习者的推广（Fortunato等，2019年） ）：一套十三项需要记忆解决的多样化机器学习任务，其中八项是使用我们的方法开发的。在这八种方法中，有四项测试代理是否可以正确识别两个几乎相同的场景之间的差异，三项是受莫里斯水迷宫实验启发的（Miyake和Shah，1999年），最后一项任务是测试代理是否可以学习整个链上的传递顺序通过呈现有序的相邻对象对来显示对象。所有这些任务都设置在第一人称3D世界中，初始条件下存在一定程度的差异11。

<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210127163030514.png" alt="image-20210127163030514" style="zoom:50%;" />
6.结论

我们已经介绍了如何使用Unity来创建大量异构环境，以用于评估和开发人工智能。我们对Unity letus的使用充分利用了其在视觉和模拟逼真度方面的灵活性以及其对程序员友好的工具，使研究人员可以更轻松地编写评估其算法进步所需的环境。我们描述了一种打包分发环境的方法，该方法可以实现结果的鲁棒性再现，并表明所产生的环境在计算上足以进行有意义的研究。最后，我们通过展示精选的论文来评估我们使用Unity的成功性，这些论文采用了使用我们的方法创建的环境。我们希望在适应Unity的需求中描述的概念可以通过使用开放式开发帮助其他人开发未来的模拟环境。采购通讯协议和包装方法以提高可重复性。

7。致谢如果没有DeepMind的许多同事的支持，这项工作将无法实现。特别要感谢Robin Alazard，Pauline Coquinot，Tom Hudson，Jason Sanmiya和Marcus Wainwright对本文的贡献，感谢Don Williamson的技术专长，最后感谢Tim Harley，Max Jaderberg和Patrick Pilarski的深刻见解。也要感谢Unity Technologies的每一个人的持续支持