##   HED (ICCV,2015)

[论文地址](https://arxiv.org/abs/1504.06375)：https://arxiv.org/abs/1504.06375

[pytorch代码：](https://github.com/BinWang-shu/pytorch_hed)https://github.com/BinWang-shu/pytorch_hed

[tensorflow代码：](https://github.com/whyguu/hed-tf)https://github.com/whyguu/hed-tf

![image-20210328152121391](/Users/lishuo/Library/Application Support/typora-user-images/image-20210328152121391.png)

> ### 多尺度学习:
>
> 多尺度学习可以是“在神经网络内部，以越来越大的感受野和下采样层的形式”。在这种“内部”情况下，在每个层中学习的特征表示自然是多尺度的。另一方面，多尺度学习可以在神经网络的“外部”，例如通过“调整输入图像的尺度”。我们将多尺度深度学习的可能配置形式化为四类，即多流学习、跳线链接学习、在多个输入上运行的单一模型和独立网络的训练
>
> 
>
> (a) Multi-stream learning: 使用不同结构，不同参数的网络训练同一副图片，多个（并行）网络流具有不同的参数数目和感受野大小，对应于多个尺度，之后由各个流产生的串联特征响应被馈入全局输出层以产生最终结果，类似的结构有Inception
>
> (b) Skip-layer network learning:该结构有一个主干网络，在主干网络中添加若干条到输出层的skip-layer，类似的结构有FPN、FCN；添加link以合并来自主网络流的不同level的特征响应，然后将这些响应组合在共享输出层中（将来自不同卷积层的输出通过跳线连接在一起，作为特征提取结果）（实际上跳线连接也可以在各个卷积层之间连接，而不仅限于到输出层，比如 U-Net 结构）；
>
> (c) Single model on multiple inputs: 该方法使用同一个网络，不同尺寸的输入图像得到不同尺度分Feature Map，YOLOv2采用了该方法；
>
> ##### 上述三种设置的一个共同点是，在这两种体系结构中，只有一个loss function产生一个预测。FCN只有一个输出损失函数。因此，在FCN中，尽管skip layer结构是将语义丰富的高层信息与细节丰富的底层信息相结合，但它并不显式地产生多尺度输出预测。
>
> (d) Training independent network: 使用完全独立的网络训练同一张图片，得到多个尺度的结果，该方法类似于集成模型；
>
> (e) Holistically-Nested networks: HED采用的方法。和 (b) 的不同在于每个侧输出都能被监督并进行反向传播，这里应用了中继监督的思想，也是一个很泛用的做法）。



### 中继监督

这篇文章也用了中继监督，之前看的 Stacked Hourglass 也是。不过 Stacked Hourglass 的侧输出是还要被输入到下个特征提取网络里继续 refine 的，旨在迭代地优化输出结果。

HED 的侧输出和 GoogLnet 等一些常见的侧输出比较像，前面也说了，浅层的特征保留了更多的信息，但是相对而言感受野更小，那么 HED 就取多个不同深度的特征，分别在这些位点设置输出层。具体地，HED 在每个 VGG 内尺寸的特征图上引出一个卷积层作为侧输出层。



<img src="/Users/lishuo/Library/Application Support/typora-user-images/image-20210328152215266.png" alt="image-20210328152215266" style="zoom: 67%;" />![img](https://img-blog.csdn.net/20171218103557861?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2FuZ2t1bjEzNDAzNzg=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

backbone为VGG16，根据尺寸分为5个stage。再通过一个1x1x1的conv，将每个输出进行反卷积到原来尺寸，然后进行相加。再经过一个1x1x1的conv降低维度得到结果图。同时loss也与每张图的loss有关。



## RCF(CVPR2017)：

论文地址:https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Richer_Convolutional_Features_CVPR_2017_paper.pdf

caffe代码地址：https://github.com/yun-liu/rcf

> RCF相较于HED，利用了每个stage中所有卷积层的特征,改进就是将每个卷积都进行element-sum，利用了更多特征,随后进行特征融合，也带来了结果上的提升，取得了不错的结果



![image-20210328152705515](/Users/lishuo/Library/Application Support/typora-user-images/image-20210328152705515.png)



