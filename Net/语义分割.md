## 语义分割数据集

在“数据，算法，计算力”这AI发展的三大驱动力中，眼下最重要的就是数据，数据集在人工智能中有着举足轻重的地位，具体根据不同的应用领域，目前的数据集主要有：

1. Pascal VOC系列:　http://host.robots.ox.ac.uk/pascal/VOC/voc2012/　通常采用PASCAL VOC 2012，最开始有1464 张具有标注信息的训练图片，2014 年增加到10582张训练图片。主要涉及了日常生活中常见的物体，包括汽车，狗，船等20个分类。
2. Microsoft COCO:　http://link.zhihu.com/?target=http%3A//mscoco.org/explore/　一共有80个类别。这个数据集主要用于实例级别的分割（Instance-level Segmentation）以及图片描述Image Caption）。
3. Cityscapes: https://www.cityscapes-dataset.com/ 适用于汽车自动驾驶的训练数据集，包括19种都市街道场景：road、side-walk、building、wal、fence、pole、traficlight、trafic　sign、vegetation、terain、sky、person、rider、car、truck、bus、train、motorcycle 和 bicycle。该数据库中用于训练和校验的精细标注的图片数量为3475，同时也包含了 2 万张粗糙的标记图片。

## **什么是图像分割？**

顾名思义，这是将图像分割成多个部分的过程。在这个过程中，图像中的每个像素都与一个物体类型相关联。图像分割主要有两种类型：语义分割和实例分割。

在语义分割中，同一类型的所有物体都使用一个类标签进行标记，而在实例分割中，相似的物体使用自己的独立标签。

![img](https://pic2.zhimg.com/80/v2-e1bf6e09a665ab2caef72e9d871a3845_1440w.jpg)

## 语义分割中的深度学习技术##

- 全卷积网络FCN：上采样提高分割精度，不同特征向量相加。[3]
- UNET：拼接特征向量；编码-解码结构；采用弹性形变的方式，进行数据增广；用边界加权的损失函数分离接触的细胞。[4]
- SegNet：记录池化的位置，反池化时恢复。[3]
- PSPNet：多尺度池化特征向量，上采样后拼接[3]
- Deeplab：池化跨度为1，然后接带孔卷积。
- ICNet：多分辨图像输入，综合不同网络生成结果。

- **全卷积神经网络 FCN（2015）**

论文：[Fully Convolutional Networks for Semantic Segmentation](https://link.zhihu.com/?target=https://arxiv.org/abs/1411.4038)　FCN 所追求的是，输入是一张图片是，输出也是一张图片，学习像素到像素的映射，端到端的映射，网络结构如下图所示：
![img](https://blog.geohey.com/content/images/2017/10/59098c944637a.jpg)

《Fully Convolutional Networks for Semantic Segmentation》https://arxiv.org/abs/1411.4038 FCN是不含全连接层的**全卷积网络**，对图像进行像素级的分类，解决了图像的语义分割问题，可以接受任意尺寸的图像大小，采用反卷积对最后一个特征图（feature map）进行处理，使其恢复到输入图像的尺寸，对每个像素产生一个预测，反卷积和卷积类似，都是相乘相加的运算。只不过后者是多对一，前者是一对多，网络结构：

FCN有几个版本，**FCN-32s**、**FCN-16s**、**FCN-8s**，分别对应反卷积的步长stride。FCN32s在第五个下采样之后直接经过一个反卷积上采样为输入图像大小。FCN16s做了两次反卷积，把第四个下采样的结果也做一次反卷积融合起来，第二次反卷积的步长为16。FCN8s做了三次反卷积，进一步融合了第三个下采样的预测结果，第三次反卷积的步长为8，而FCN8s效果最好，说明了较浅层的预测结果包含更多细节特征。

全卷积神经网络主要使用了三种技术：

1. 卷积化（Convolutional）
2. 上采样（Upsample）
3. 跳跃结构（Skip Layer）

**卷积化（Convolutional）**
卷积化即是将普通的分类网络，比如VGG16，ResNet50/101等网络丢弃全连接层，换上对应的卷积层即可。
![img](https://blog.geohey.com/content/images/2017/10/v2-b55b025a40d279b5d2d2c17f5453e013_r.jpg)

**上采样（Upsample）**
有的说叫conv_transpose更为合适。因为普通的池化会缩小图片的尺寸，比如VGG16 五次池化后图片被缩小了32倍。为了得到和原图等大的分割图，我们需要上采样/反卷积。反卷积和卷积类似，都是相乘相加的运算。只不过后者是多对一，前者是一对多。而反卷积的前向和后向传播，只用颠倒卷积的前后向传播即可。图解如下：
![img](https://blog.geohey.com/content/images/2017/10/YyCu2.gif)

**跳跃结构（Skip Layer）**
这个结构的作用就在于优化结果，因为如果将全卷积之后的结果直接上采样得到的结果是很粗糙的，==所以作者将不同池化层的结果进行上采样之后来优化输出。==具体结构如下：

   前面说了，FCN有两种方式产生密集输出（dense output/dense prediction），直接放大和拼接放大，直接放大即单纯的对最后的输出特征进行反卷积操作，得到的结果是非常粗糙的，丢失了很多细节，只能够开一个大概，那怎么办呢，于是想到一个办法，不就是丢失了很多特征嘛 ，那我把前面卷积层和池化层输出的低层特征添加一些进来，不就可以弥补一些特征了嘛，于是就出现了 skip layer。跳层结构：结合上采样和上层卷积池化后数据（更详细的特征），修复还原的图像。

![image-20210328171844180](../../Library/Application Support/typora-user-images/image-20210328171844180.png)

![img](https://blog.geohey.com/content/images/2017/10/ccb6dd0a7f207134ae7690974c3e88a5_b.png)

而不同上采样结构得到的结果对比如下：
![img](http://img.blog.csdn.net/20171011095205758?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzQ3NTkyMzk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

这是第一种结构，也是深度学习应用于图像语义分割的开山之作，获得了CVPR2015的最佳论文。但还是无法避免有很多问题，比如，精度问题，对细节不敏感，以及像素与像素之间的关系，忽略空间的一致性等，后面的研究极大的改善了这些问题。

### **3、Unet++**

UNet++: A Nested U-Net Architecture for Medical Image Segmentation https://arxiv.org/pdf/1807.10165.pdf Unet++借鉴了DenceNet的**密集连接**，对Unet改进的点主要是skip connection，下图所示，其中黑色部分代表的就是原始Unet结构,绿色代表添加的卷积层，蓝色代表改进的skip connection，每一个水平层就是非常标准的DenseNet的结构。

![image-20210401114331012](/Users/lishuo/Library/Application Support/typora-user-images/image-20210401114331012.png)

还引入了**deep supervision深度监督**的思路。网络的loss函数是由不同层得到的分割图的loss的平均，每层的loss函数为DICE LOSS和Binary cross-entropy LOSS之和，引入DSN(deep supervision net)后，通过model pruning（模型剪枝）能够实现模型的两种模式：高精度模式和高速模式。

- ## SegNet（2015）
  
  《A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation》 https://arxiv.org/abs/1511.00561 Segnet和Unet有点像，它采用的是编码-解码的结构，这样的对称结构有种自编码器的感觉在里面，先编码再解码。这样的结构主要使用了反卷积和上池化。解码器通过池化索引来实现非线性的上采样，这个池化索引是由与解码器相对应的编码器进行最大池化操作计算得到的。

  论文：[A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation](https://arxiv.org/abs/1511.00561)
  主要贡献：将最大池化指数转移至解码器中，改善了分割分辨率。

图像分割的基本结构包括编码器和解码器：

* 编码器通过滤波器从图像中提取特征
  
  * 解码器负责生成最终的输出，通常是一个包含物体轮廓的分割掩码。大多数体系结构都具有这种结构或其变体。
* SegNet中的Pooling比其他Pooling多了一个**Pooling Index功能**，也就是每次Pooling，都会保存通过max选出的权值在2x2 filter中的相对位置。Upsamping就是Pooling的逆过程（index在Upsampling过程中发挥作用），Upsamping使得图片变大2倍。我们清楚的知道Pooling之后，每个filter会丢失了3个权重，这些权重是无法复原的，但是在Upsamping层中可以得到在Pooling中相对Pooling filter的位置。SegNet在Unpooling时用index信息，直接将数据放回对应位置，后面再接Conv训练学习。
  
  ![image-20210328163659587](../../Library/Application Support/typora-user-images/image-20210328163659587.png)
- 空洞卷积（2015）
  论文：[Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122)
  主要贡献：使用了空洞卷积，这是一种可用于密集预测的卷积层；提出在多尺度聚集条件下使用空洞卷积的“背景模块”。

- DeepLab（2016）
  论文：[DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs](https://link.zhihu.com/?target=https://arxiv.org/abs/1606.00915)
  主要贡献：使用了空洞卷积；提出了在空间维度上实现金字塔型的空洞池化atrous spatial pyramid pooling(ASPP)；使用了全连接条件随机场。

## **U-Net**

U-Net是最初为分割生物医学图像而开发的卷积神经网络。当它的结构可视化时，它看起来像字母U，因此取名为U-Net。它的结构由两部分组成，左边的部分是收缩路径，右边的部分是扩展路径。收缩路径的目的是捕获上下文，而扩展路径的作用是帮助精确定位。

收缩路径用来获得上下文信息，对称扩张路径用来精确定位分割边界。U-net 使用图像切块进行训练，所以训练数据量远远大于训练图像的数量，这使得网络在少量样本的情况下也能获得不变性和鲁棒性。

对称结构**，左半部分收缩路径采用卷积，RELU 和最大池化获得图像的上下文信息，右边的扩展层**直接复制过来，然后裁剪到与上采样的图片大小一样，再将它们连接起来，实现了不同层特征相结合的上采样特征图。但**只能处理** **2D** 图像。

![img](https://pic2.zhimg.com/80/v2-1d3379e5b85ba20a09ecd6445e19a39d_1440w.jpg)

> Unet的结构是先编码（下采用）再解码（上采样）的**U形结构**，保持输入和输出大小一样。在FCN中，**Skip connection**的联合是通过对应像素的求和，而U-Net则是对其的channel的concat过程。

U-Net由右边的扩展路径和左边的收缩路径组成。收缩路径由两个3×3的卷积组成。卷积之后是一个ReLU和一个用于下采样的2×2最大池化。

你可以在这里找到U-Net的完整实现：[https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/](https://link.zhihu.com/?target=https%3A//lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)。



### **5、RefineNet**

《Multi-Path Refinement Networks for High-Resolution Semantic Segmentation》https://arxiv.org/abs/1611.06612 U-Net在上采样后直接和encoder的feature map进行级联，而RefineNet模型通过ResNet进行下采样，之后经过4个**RefineNet module**进行上采样，得到原来大小的特征图。把encoder产生的feature和上一阶段decoder的输出同时作为RefineNet module的输入，在RefineNet module中进行一系列卷积，融合，池化，使得多尺度特征的融合更加深入。 另一创新点就是RefineNet模块中的**链式残余池化**，使用不同尺寸的窗口池化，并且使用残余连接和可学习的权重把他们融合起来。

![image-20210401114226652](/Users/lishuo/Library/Application Support/typora-user-images/image-20210401114226652.png)

下图是单独一个refineNet的结构

![image-20210401114209225](/Users/lishuo/Library/Application Support/typora-user-images/image-20210401114209225.png)

每个RefineNet module包含4个部分 1-Residual convolution unit ：对ResNet block进行2层的卷积操作。注意这里有多个ResNet block作为输入。 2-Multi-resolution fusion：将1中得到的feature map进行加和融合。 3-Chained residual pooling ：该模块用于从一个大图像区域中捕捉背景上下文。注意：pooling的stride为1。 4-Output convolutions：1个RCUs构成。 RefineNet还有一些变体，分别为仅使用1个RefineNet模块，2个RefineNet模块级联，4个RefineNet模块级联且使用两种尺度输入。

![image-20210401114154571](/Users/lishuo/Library/Application Support/typora-user-images/image-20210401114154571.png)

给大家一个多个分割网络的pytorch实现，如：Deeplabv3, Deeplabv3_plus, PSPNet, UNet, UNet_AutoEncoder, UNet_nested, R2AttUNet, AttentionUNet, RecurrentUNet, SEGNet, CENet, DsenseASPP, RefineNet, RDFNet。链接：

https://github.com/Minerva-J/Pytorch-Segmentation-multi-models

## PSP net



![image-20210328164121153](../../Library/Application Support/typora-user-images/image-20210328164121153.png)



## DeepLab



- 空洞卷积（2015）
  论文：[Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122)
  主要贡献：使用了空洞卷积，这是一种可用于密集预测的卷积层；提出在多尺度聚集条件下使用空洞卷积的“背景模块”。
- DeepLab（2016）
  论文：[DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs](https://link.zhihu.com/?target=https://arxiv.org/abs/1606.00915)
  主要贡献：使用了空洞卷积；提出了在空间维度上实现金字塔型的空洞池化atrous spatial pyramid pooling(ASPP)；使用了全连接条件随机场
- ![image-20210328173208802](../../Library/Application Support/typora-user-images/image-20210328173208802.png)。



![image-20210328164319836](../../Library/Application Support/typora-user-images/image-20210328164319836.png)



![image-20210328164430737](../../Library/Application Support/typora-user-images/image-20210328164430737.png)



### 文章目录

- [介绍](https://blog.csdn.net/qq_20084101/article/details/80432960#_10)
- [网络架构](https://blog.csdn.net/qq_20084101/article/details/80432960#_30)
- - [Fully Convolution Networks (FCNs) 全卷积网络](https://blog.csdn.net/qq_20084101/article/details/80432960#Fully_Convolution_Networks_FCNs__43)
  - [SegNet](https://blog.csdn.net/qq_20084101/article/details/80432960#SegNet_127)
  - [U-Net](https://blog.csdn.net/qq_20084101/article/details/80432960#UNet_160)
  - [DeepLab v1](https://blog.csdn.net/qq_20084101/article/details/80432960#DeepLab_v1_185)
  - [DeepLab v2](https://blog.csdn.net/qq_20084101/article/details/80432960#DeepLab_v2_214)
  - [DeepLab v3](https://blog.csdn.net/qq_20084101/article/details/80432960#DeepLab_v3_251)
  - [Fully Convolutional DenseNet](https://blog.csdn.net/qq_20084101/article/details/80432960#Fully_Convolutional_DenseNet_281)
  - [E-Net 和 Link-Net](https://blog.csdn.net/qq_20084101/article/details/80432960#ENet__LinkNet_302)
  - [Mask R-CNN](https://blog.csdn.net/qq_20084101/article/details/80432960#Mask_RCNN_335)
  - [PSPNet](https://blog.csdn.net/qq_20084101/article/details/80432960#PSPNet_371)
  - [RefineNet](https://blog.csdn.net/qq_20084101/article/details/80432960#RefineNet_410)
  - [G-FRNet](https://blog.csdn.net/qq_20084101/article/details/80432960#GFRNet_445)
- [半监督语义分割](https://blog.csdn.net/qq_20084101/article/details/80432960#_475)
- - [DecoupledNet](https://blog.csdn.net/qq_20084101/article/details/80432960#DecoupledNet_476)
  - [基于GAN的方法](https://blog.csdn.net/qq_20084101/article/details/80432960#GAN_505)
- [数据集](https://blog.csdn.net/qq_20084101/article/details/80432960#_533)
- [结果](https://blog.csdn.net/qq_20084101/article/details/80432960#_591)



原文地址：https://meetshah1995.github.io/semantic-segmentation/deep-learning/pytorch/visdom/2017/06/01/semantic-segmentation-over-the-years.html

# 介绍

图像的语义分割是将输入图像中的每个像素分配一个语义类别，以得到像素化的密集分类。虽然自 **[2007](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/index.html)** 年以来，语义分割/场景解析一直是计算机视觉社区的一部分，但与计算机视觉中的其他领域很相似，自 2014 年 **[Long等人](https://arxiv.org/abs/1411.4038)** 首次使用全卷积神经网络对自然图像进行端到端分割，语义分割才产生了大的突破。

![img](https://img-blog.csdnimg.cn/20190325193029702.jpg) ![img](https://img-blog.csdnimg.cn/20190325193121990.png)

**图1：**输入图像（左），FCN-8s 网络生成的语义分割图（右）（使用 **[pytorch-semseg](https://github.com/meetshah1995/pytorch-semseg)** 训练）




FCN-8s 架构在 Pascal VOC 2012 数据集上的性能相对以前的方法提升了 20%，达到了 62.2% 的mIOU 。这种架构是语义分割的基础，此后一些较新的和更好的体系结构都基于此架构。 

全卷积网络（FCNs）可以用于自然图像的语义分割、多模态医学图像分析和多光谱卫星图像分割。

我总结了 FCN、SegNet、U-Net、FC-Densenet E-Net 和 Link-Net、RefineNet、PSPNet、Mask-RCNN 以及一些半监督方法，例如 DecoupledNet 和 GAN-SS，并为其中的一些网络提供了[PyTorch](https://github.com/meetshah1995/pytorch-semseg)实现。在文章的最后一部分，我总结了一些流行的数据集，并展示了一些网络训练的结果。

# 网络架构

一般的语义分割架构可以被认为是一个**编码器-解码器**网络。**编码器**通常是一个预训练的分类网络，像 VGG、ResNet，然后是一个**解码器**网络。这些架构不同的地方主要在于**解码器**网络。**解码器**的任务是将**编码器**学习到的可判别特征（较低分辨率）从语义上投影到像素空间（较高分辨率），以获得密集分类。

不同于分类任务中网络的最终结果（对图像分类的概率）是唯一重要的事，语义分割不仅需要在像素级有判别能力，还需要有能将编码器在不同阶段学到的可判别特征投影到像素空间的机制。不同的架构采用不同的机制（跳跃连接、金字塔池化等）作为解码机制的一部分。

一些上述架构和加载数据的代码可在以下链接获得：

- **Pytorch**：**[meetshah1995/pytorch-semseg](https://github.com/meetshah1995/pytorch-semseg)**

**[这篇论文](https://arxiv.org/pdf/1704.06857.pdf)** 对语义分割（包括 Recurrent Style Networks）作了一个更正式的总结。

------

## Fully Convolution Networks (FCNs) 全卷积网络

| CVPR 2015 | Fully Convolutional Networks for Semantic Segmentation | [Arxiv](https://arxiv.org/abs/1411.4038) |
| --------- | ------------------------------------------------------ | ---------------------------------------- |
|           |                                                        |                                          |

> 我们将当前分类网络（AlexNet, VGG net 和 GoogLeNet）修改为全卷积网络，通过对分割任务进行微调，将它们学习的表征转移到网络中。然后，我们定义了一种新的架构，它将深的、粗糙的网络层的语义信息和浅的、精细的网络层的表层信息结合起来，来生成精确和详细的分割。我们的全卷积网络在 PASCAL VOC（在2012年相对以前有20%的提升，达到了62.2%的平均IU），NYUDv2 和 SIFT Flow 上实现了最优的分割结果，对于一个典型的图像，推断只需要三分之一秒的时间。

![img](https://img-blog.csdnimg.cn/20190402190441881.png)**图2：**FCN 端到端密集预测流程

**关键特点：**

- 特征是由编码器中的不同阶段合并而成的，它们在**语义信息的粗糙程度**上有所不同。
- 低分辨率语义特征图的上采样使用**经双线性插值滤波器初始化的反卷积**操作完成。
- 从 VGG16、Alexnet 等分类器网络进行知识迁移来实现语义细分。

![img](https://img-blog.csdnimg.cn/20190402190521695.png)**图3：**将全连接层转换成卷积层，使得分类网络可以输出一个类的热图。



如上图所示，像 `VGG16` 分类网络的全连接层（`fc6`，`fc7`）被转换为全卷积层。它生成了一个低分辨率的类的热图，然后使用经双线性插值初始化的反卷积，并在上采样的每一个阶段通过融合（简单地相加） `VGG16` 中的低层（`conv4`和`conv3`）的更加粗糙但是分辨率更高的特征图进一步细化特征。在 [这里](http://ethereon.github.io/netscope/#/preset/fcn-8s-pascal) 可以找到更加详细的 netscope 风格的网络可视化。

在传统的分类 CNNs 中，池化操作用来增加视野，同时减少特征图的分辨率。这对于分类任务来说非常有用，因为分类的最终目标是找到某个特定类的存在，而对象的空间位置无关紧要。因此，在每个卷积块之后引入池化操作，以使后续块能够从已池化的特征中提取更多抽象、突出类的特征。

![img](https://img-blog.csdnimg.cn/20190402190620569.png)**图4：**FCN-8s 网络架构



另一方面，池化和带步长的卷积对语义分割是不利的，因为这些操作造成了空间信息的丢失。下面列出的大多数架构主要在**解码器**中使用了不同的机制，但目的都在于恢复在**编码器**中降低分辨率时丢失的信息。如上图所示，FCN-8s 融合了不同粗糙度（`conv3`、`conv4`和`fc7`）的特征，利用编码器不同阶段不同分辨率的空间信息来细化分割结果。

![conv_1_1_gradient](https://raw.githubusercontent.com/shekkizh/FCN.tensorflow/master/logs/images/conv_1_1_gradient.png)   ![conv_4_1_gradient](https://raw.githubusercontent.com/shekkizh/FCN.tensorflow/master/logs/images/conv_4_1_gradient.png)   
![conv_4_2_gradient](https://raw.githubusercontent.com/shekkizh/FCN.tensorflow/master/logs/images/conv_4_2_gradient.png)   ![conv_4_3_gradient](https://raw.githubusercontent.com/shekkizh/FCN.tensorflow/master/logs/images/conv_4_3_gradient.png)   

**图5：**训练 FCNs 时卷积层的梯度 **[图源](https://github.com/shekkizh/FCN.tensorflow)**



第一个卷积层捕捉低层次的几何信息，因为这完全依赖数据集，你可以注意到梯度调整了第一层的权重以使模型适应数据集。VGG 中更深层的卷积层有非常小的梯度流，因为这里捕获的高层次的语义概念足够用于分割。

![img](https://img-blog.csdnimg.cn/20190325194358270.gif)   

**图6：**反卷积（转置卷积），蓝色为输入，绿色为输出 **[图源](https://github.com/vdumoulin/conv_arithmetic)**



语义分割架构的另一个重要方面是，对特征图使用反卷积，将低分辨率分割图上采样至输入图像分辨率，或者花费大量计算成本，使用空洞卷积在编码器上部分避免分辨率下降。即使在现代 GPUs 上，空洞卷积的计算成本也很高。**[distill.pub](https://distill.pub/2016/deconv-checkerboard/)** 上的这篇文章详细介绍了反卷积。

------

## SegNet

| 2015 | SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation | [Arxiv](https://arxiv.org/abs/1511.00561) |
| ---- | ------------------------------------------------------------ | ----------------------------------------- |
|      |                                                              |                                           |

> SegNet 的新颖之处在于解码器对其较低分辨率的输入特征图进行上采样的方式。具体地说，解码器使用了在相应编码器的最大池化步骤中计算的池化索引来执行非线性上采样。这种方法消除了学习上采样的需要。经上采样后的特征图是稀疏的，因此随后使用可训练的卷积核进行卷积操作，生成密集的特征图。我们将我们所提出的架构与广泛采用的 FCN 以及众所周知的 DeepLab-LargeFOV，DeconvNet 架构进行比较。比较的结果揭示了在实现良好的分割性能时所涉及的内存与精度之间的权衡。

![img](https://img-blog.csdnimg.cn/20190325194645429.png)**图7：**SegNet 架构

**关键特点：**

- SegNet 在解码器中使用**反池化**对特征图进行上采样，并在分割中保持高频细节的完整性。
- 编码器不使用全连接层（和 FCN 一样进行卷积），因此是拥有较少参数的轻量级网络。

![img](https://img-blog.csdnimg.cn/20190325194741517.png)**图8：**反池化



如上图所示，编码器中的每一个最大池化层的索引都被存储起来，用于之后在解码器中使用那些存储的索引来对相应的特征图进行反池化操作。虽然这有助于保持高频信息的完整性，但当对低分辨率的特征图进行反池化时，它也会忽略邻近的信息。

------

## U-Net

| MICCAI 2015 | U-Net: Convolutional Networks for Biomedical Image Segmentation | [Arxiv](https://arxiv.org/abs/1505.04597) |
| ----------- | ------------------------------------------------------------ | ----------------------------------------- |
|             |                                                              |                                           |

> U-Net 架构包括一个捕获上下文信息的收缩路径和一个支持精确本地化的对称扩展路径。我们证明了这样一个网络可以使用非常少的图像进行端到端的训练，并且在ISBI神经元结构分割挑战赛中取得了比以前最好的方法（一个滑动窗口的卷积网络）更加优异的性能。我们使用相同的网络，在透射光显微镜图像（相位对比度和 DIC）上进行训练，以很大的优势获得了2015年 ISBI 细胞追踪挑战赛。此外，网络推断速度很快。一个512x512的图像的分割在最新的 GPU 上花费了不到一秒。

![img](https://img-blog.csdnimg.cn/20190325194852702.png)**图9：**U-Net 架构

**关键特点：**

- U-Net 简单地将**编码器**的特征图拼接至每个阶段**解码器**的上采样特征图，从而形成一个梯形结构。该网络非常类似于 **[Ladder Network](https://arxiv.org/abs/1507.02672)** 类型的架构。
- 通过跳跃 `拼接` 连接的架构，在每个阶段都允许解码器学习在编码器池化中丢失的相关特征。
- 上采样采用转置卷积。

U-Net 在 EM 数据集上取得了最优异的结果，该数据集只有30个密集标注的医学图像和其他医学图像数据集，U-Net 后来扩展到3D版的 **[3D-U-Net](https://arxiv.org/abs/1606.06650)**。虽然 U-Net 最初的发表在于其在生物医学领域的分割、网络的实用性以及从非常少的数据中学习的能力，但现在已经成功应用其他几个领域，例如 **[卫星图像分割](https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection)**，同时也成为许多 **[kaggle竞赛](https://www.kaggle.com/c/ultrasound-nerve-segmentation)** 中关于医学图像分割的获胜的解决方案的一部分。

------

## DeepLab v1

| ICLR 2015 | Semantic Image Segmentation with deep convolutional nets and fully connected CRFs | [Arxiv](https://arxiv.org/abs/1412.7062) |
| --------- | ------------------------------------------------------------ | ---------------------------------------- |
|           |                                                              |                                          |

> 近来，深度卷积网络在高级视觉任务（图像分类和目标检测）中展示了优异的性能。本文结合 DCNN 和概率图模型来解决像素级分类任务（即语义分割）。我们展示了 DCNN 最后一层的响应不足以精确定位目标边界，这是 DCNN 的不变性导致的。我们通过在最后一层网络后结合全连接条件随机场来解决糟糕的定位问题。我们的方法在 PASCAL VOC 2012 上达到了 71.6% 的 mIoU。

![img](https://img-blog.csdnimg.cn/20190325194437242.gif) 

**图10：**空洞卷积，蓝色为输入，绿色为输出 **[来源](https://github.com/vdumoulin/conv_arithmetic)**



**关键特点：**

- 提出 **空洞卷积（atrous convolution）（又称扩张卷积（dilated convolution））**。
- 在最后两个最大池化操作中不降低特征图的分辨率，并在倒数第二个最大池化之后的卷积中使用空洞卷积。
- 使用 **CRF（条件随机场）** 作为后处理，恢复边界细节，达到准确定位效果。
- 附加输入图像和前四个最大池化层的每个输出到一个两层卷积，然后拼接到主网络的最后一层，达到 **多尺度预测** 效果。

------

## DeepLab v2

| TPAMI 2017 | DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs | [Arxiv](https://arxiv.org/abs/1606.00915) |
| ---------- | ------------------------------------------------------------ | ----------------------------------------- |
|            |                                                              |                                           |

> 首先，我们强调上采样过滤器的卷积，或“空洞卷积”，在密集预测任务中是一个强大的工具。空洞卷积允许我们显式地控制在深度卷积神经网络中计算的特征响应的分辨率。它还允许我们有效地扩大过滤器的视野，在不增加参数数量或计算量的情况下引入更大的上下文。其次，提出了一种空洞空间金字塔池化（ASPP）的多尺度鲁棒分割方法。ASPP 使用多个采样率的过滤器和有效的视野探测传入的卷积特征层，从而在多个尺度上捕获目标和图像上下文。第三，结合 DCNNs 方法和概率图形模型，改进了目标边界的定位。DCNNs 中常用的最大池化和下采样的组合实现了不变性，但对定位精度有一定的影响。我们通过将 DCNN 最后一层的响应与一个全连接条件随机场(CRF)相结合来克服这个问题。DeepLab v2 在 PASCAL VOC 2012 上得到了 79.7% 的 mIoU。

论文中提出了语义分割中的三个挑战：

1. 由于池化和卷积而减少的特征分辨率。
2. 多尺度目标的存在。
3. 由于 DCNN 不变性而减少的定位准确率。

对于第一个挑战可以减少特征图下采样的次数，但是会增加计算量。
对于第二个挑战可以使用图像金字塔、空间金字塔等多尺度方法获取多尺度上下文信息。
对于第三个挑战可以使用跳跃连接或者引入条件随机场。

DeepLab v2 使用 VGG 和 ResNet 作为主干网络分别进行了实验。

![img](https://img-blog.csdnimg.cn/20190326164616615.png) 

**图11：**(a) DeepLab v1，(b) DeepLab v2 



**关键特点：**

- 提出了**空洞空间金字塔池化（Atrous Spatial Pyramid Pooling）**，在不同的分支采用不同的空洞率以获得多尺度图像表征。

------

## DeepLab v3

| 2017 | Rethinking Atrous Convolution for Semantic Image Segmentation | [Arxiv](https://arxiv.org/abs/1706.05587) |
| ---- | ------------------------------------------------------------ | ----------------------------------------- |
|      |                                                              |                                           |

> 在本工作中，我们再次讨论空洞卷积，一个显式调整过滤器视野，同时控制特征相应分辨率的强大工具。为了解决多尺度目标的分割问题，我们串行/并行设计了能够捕捉多尺度上下文的模块，模块中采用不同的空洞率。此外，我们增强了先前提出的空洞空间金字塔池化模块，增加了图像级特征来编码全局上下文，使得模块可以在多尺度下探测卷积特征。提出的 “DeepLab v3” 系统在没有 CRF 作为后处理的情况下显著提升了性能。

DeepLab v3 使用 ResNet 作为主干网络。

![img](https://img-blog.csdnimg.cn/20190402190228185.png) 

**图12：** DeepLab v3 



**关键特点：**

- 在残差块中使用多网格方法（MultiGrid），从而引入不同的空洞率。
- 在空洞空间金字塔池化模块中加入图像级（Image-level）特征，并且使用 BatchNormalization 技巧。

------

## Fully Convolutional DenseNet

| 2016 | The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation | [Arxiv](https://arxiv.org/abs/1611.09326) |
| ---- | ------------------------------------------------------------ | ----------------------------------------- |
|      |                                                              |                                           |

> 在本文中，我们扩展了 DenseNets，以解决语义分割的问题。我们在城市场景基准数据集（如 CamVid 和 Gatech ）上获得了最优异的结果，没有使用进一步的后处理模块和预训练模型。此外，由于模型的优异结构，我们的方法比当前发布的在这些数据集上取得最佳的网络的参数要少得多。

![img](https://img-blog.csdnimg.cn/20190326164925478.png)**图10：**全卷积 DenseNet 架构

全卷积 DenseNet 使用 **[DenseNet](https://arxiv.org/abs/1608.06993)** 作为它的基础编码器，并且也以类似于U-Net的方式，在每一层级上将编码器和解码器进行拼接。

------

## E-Net 和 Link-Net

| 2016 | ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation | [Arxiv](https://arxiv.org/abs/1606.02147)            |
| ---- | ------------------------------------------------------------ | ---------------------------------------------------- |
| 2017 | LinkNet: Feature Forwarding: Exploiting Encoder Representations for Efficient Semantic Segmentation | [Blog](https://codeac29.github.io/projects/linknet/) |

> 在本文中，我们提出了一种新颖的深度神经网络架构，称为 ENet（efficient neural network），专门为需要低延迟操作的任务创建。ENet 比当前网络模型快18倍，少用75倍的 FLOPs，参数数量少79倍，并且提供相似甚至更好的准确率。我们在 CamVid、Cityscapes 和 SUN 数据集上进行了测试，展示了与现有的最优方法进行比较的结果，以及网络准确率和处理时间之间的权衡。

> LinkNet 可以在 TX1 和 Titan X 上，分别以 2fps 和 19fps 的速率处理分辨率为 1280x720 的图像。

![img](https://img-blog.csdnimg.cn/20190326165005165.png)   ![img](https://img-blog.csdnimg.cn/2019032616503450.png)   

**图11：**（左）LinkNet 架构（右）LinkNet 中使用的编码器和解码器模块



LinkNet 架构类似于一个梯形网络架构，编码器的特征图（横向）和解码器的上采样特征图（纵向）相加。还需要注意的是，由于它的通道约减方案，解码器模块包含了相当少的参数。大小为`[H, W, n_channels]`的特征图先通过`1*1`卷积核得到大小为`[H, W, n_channels / 4]`的特征图，然后使用反卷积将其变为`[2*H, 2*W, n_channels / 4]`，最后使用`1*1`卷积使其大小变为`[2*H, 2*W, n_channels / 2]`，因此解码器有着更少的参数。这些网络在实现相当接近于最优准确率的同时，可以实时地在嵌入式 GPU 上进行分割。

------

## Mask R-CNN

| 2017 | Mask R-CNN | [Arxiv](https://arxiv.org/abs/1703.06870) |
| ---- | ---------- | ----------------------------------------- |
|      |            |                                           |

> 该方法被称为 Mask R-CNN，以Faster R-CNN 为基础，在现有的边界框识别分支基础上添加一个并行的预测目标掩码的分支。Mask R-CNN 很容易训练，仅仅在 Faster R-CNN 上增加了一点小开销，运行速度为 5fps。此外，Mask R-CNN 很容易泛化至其他任务，例如，可以使用相同的框架进行姿态估计。我们在 COCO 所有的挑战赛中都获得了最优结果，包括实例分割，边界框目标检测，和人关键点检测。在没有使用任何技巧的情况下，Mask R-CNN 在每项任务上都优于所有现有的单模型网络，包括 COCO 2016 挑战赛的获胜者。

![img](https://img-blog.csdnimg.cn/20190402190932701.png)   
![img](https://img-blog.csdnimg.cn/20190402191000286.png)   
**图12：**（上）Mask R-CNN 分割流程 
（下）原始 Faster-RCNN 架构和辅助分割分支



Mask R-CNN 架构相当简单，它是流行的 Faster R-CNN 架构的扩展，在其基础上进行必要的修改，以执行语义分割。

**关键特点：**

- 在Faster R-CNN 上添加辅助分支以执行语义分割
- 对每个实例进行的 **RoIPool** 操作已经被修改为 **RoIAlign** ，它避免了特征提取的空间量化，因为在最高分辨率中保持空间特征不变对于语义分割很重要。
- Mask R-CNN 与 **[Feature Pyramid Networks](https://arxiv.org/abs/1612.03144)**（类似于PSPNet，它对特征使用了金字塔池化）相结合，在 **[MS COCO](http://mscoco.org/)** 数据集上取得了最优结果。

在2017-06-01的时候，在网络上还没有 Mask R-CNN 的工作实现，而且也没有在 Pascal VOC 上进行基准测试，但是它的分割掩码显示了它与真实标注非常接近。

------

## PSPNet

| CVPR 2017 | PSPNet: Pyramid Scene Parsing Network | [Arxiv](https://arxiv.org/abs/1612.01105) |
| --------- | ------------------------------------- | ----------------------------------------- |
|           |                                       |                                           |

> 在本文中，我们利用基于不同区域的上下文信息集合，通过我们的金字塔池化模块，使用提出的金字塔场景解析网络（PSPNet）来发挥全局上下文信息的能力。我们的全局先验表征在场景解析任务中产生了良好的质量结果，而 PSPNet 为像素级的预测提供了一个更好的框架，该方法在不同的数据集上达到了最优性能。它首次在2016 ImageNet 场景解析挑战赛，PASCAL VOC 2012 基准和 Cityscapes 基准中出现。

![img](https://img-blog.csdnimg.cn/20190402191049975.png)   
![img](https://img-blog.csdnimg.cn/20190402191116835.png)   
**图13：**（上）PSPNet 架构 
（下）使用 netscope 实现的可视化的空间金字塔池化

**关键特点：**

- PSPNet 通过**引入空洞卷积来修改基础的 ResNet 架构**，特征经过最初的池化，在整个编码器网络中以相同的分辨率进行处理（原始图像输入的`1/4`），直到它到达空间池化模块。
- 在 ResNet 的中间层中引入**辅助损失**，以优化整体学习。
- 在修改后的 ResNet 编码器顶部的**空间金字塔池化**聚合全局上下文。

![img](https://img-blog.csdnimg.cn/20190402191224917.png)   
**图14：**图片展示了全局空间上下文对语义分割的重要性。它显示了层之间感受野和大小的关系。在这个例子中，更大、更加可判别的感受野（ **蓝**）相比于前一层（ **橙**）可能在细化表征中更加重要，这有助于解决歧义

------

## RefineNet

| CVPR 2017 | RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation | [Arxiv](https://arxiv.org/abs/1611.06612) |
| --------- | ------------------------------------------------------------ | ----------------------------------------- |
|           |                                                              |                                           |

> 在这里，我们提出了 RefineNet，一个通用的多路径优化网络，它明确利用了整个下采样过程中可用的所有信息，使用远程残差连接实现高分辨率的预测。通过这种方式，可以使用早期卷积中的细粒度特征来直接细化捕捉高级语义特征的更深的网络层。RefineNet 的各个组件使用遵循恒等映射思想的残差连接，这允许网络进行有效的端到端训练。

![img](https://img-blog.csdnimg.cn/20190402191317912.png)   
![img](https://img-blog.csdnimg.cn/20190402191340916.png)   
**图15：**（上）RefineNet 架构 
（下）建立 RefineNet 的块 - 残差卷积单元，多分辨率融合和链式残差池化



RefineNet 解决了传统卷积网络中空间分辨率减少的问题，与 PSPNet（使用计算成本高的空洞卷积）使用的方法非常不同。提出的架构迭代地池化特征，利用特殊的 RefineNet 模块增加不同的分辨率，并最终生成高分辨率的分割图。

**关键特点：**

- 使用**多分辨率**作为输入，将提取的特征融合在一起，并将其传递到下一个阶段。
- 引入**链式残差池化**，可以从一个大的图像区域获取背景信息。它通过多窗口尺寸有效地池化特性，利用残差连接和学习权重方式融合这些特征。
- 所有的特征融合都是使用`sum`（ResNet 方式）来进行端到端训练。
- 使用普通ResNet的残差层，**没有计算成本高的空洞卷积**。

------

## G-FRNet

| CVPR 2017 | G-FRNet: Gated Feedback Refinement Network for Dense Image Labeling | [Arxiv](http://www.cs.umanitoba.ca/~ywang/papers/cvpr17.pdf) |
| --------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
|           |                                                              |                                                              |

> 本文提出了 Gated Feedback Refinement Network (G-FRNet)，这是一种用于密集标记任务的端到端深度学习框架，解决了现有方法的局限性。最初，GFRNet 进行粗略的预测，然后通过在细化阶段有效地集成局部和全局上下文信息，逐步细化细节。我们引入了控制信息前向传递的门控单元，以过滤歧义。

![img](https://img-blog.csdnimg.cn/2019040219143578.png)   
![img](https://img-blog.csdnimg.cn/20190402191456774.png)   
**图16：**（上）G-FRNet 架构 
（下）门控细化单元



上述大多数架构都依赖于从编码器到解码器的简单特征，使用`拼接`、`反池化`或简单的`加和`。然而，在编码器中，从高分辨率（较难判别）层到对应的解码器中相应的上采样特征图的信息，可能或不能用于分割。在每个阶段，通过使用**门控细化反馈单元**，控制从编码器传送到解码器的信息流，这样可以帮助解码器解决歧义，并形成更相关的**门控**空间上下文。

另一方面，本文的实验表明，在语义分割任务中，ResNet是一个远优于VGG16的编码器。这是我在以前的论文中找不到的。

------

# 半监督语义分割

## DecoupledNet

| NIPS 2015 | Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation | [Arxiv](https://arxiv.org/abs/1506.04924) |
| --------- | ------------------------------------------------------------ | ----------------------------------------- |
|           |                                                              |                                           |

> 与现有的将语义分割作为基于区域分类的单一任务的方法相反，我们的算法将分类和分割分离，并为每个任务学习一个单独的网络。在这个架构中，通过分类网络识别与图像相关的标签，然后在分割网络中对每个识别的标签执行二进制分割。它通过利用从桥接层获得的特定类的激活图来有效地减少用于分割的搜索空间。

![img](https://img-blog.csdnimg.cn/20190402191545892.png)   

**图17：**DecoupledNet 架构

这也许是第一个使用全卷积网络进行语义分割的半监督方法。

**关键特点：**

- **分离分类和分割任务**，从而使预训练的分类网络能够即插即用（plug and play）。
- 分类和分割网络之间的**桥接层**生成突出类的特征图（k类），然后输入分割网络，生成一个二进制分割图（k类）
- 但是，这个方法在一张图像中**分割k类需要传递k次**。

------

## 基于GAN的方法

| 2017 | Semi and Weakly Supervised Semantic Segmentation Using Generative Adversarial Network | [Arxiv](https://arxiv.org/abs/1703.09695) |
| ---- | ------------------------------------------------------------ | ----------------------------------------- |
|      |                                                              |                                           |

> 特别地，我们基于生成对抗网络（GANs）提出了一种半监督框架，它包含一个生成器网络以提供额外的用于多类别分类器的训练样本，作为在 GAN 框架中的判别器，从K个可能的类中为样本分配一个标签y或者将其标记为一个假样本（额外的类）。为了确保 GANs 生成的图像质量更高，随之改进像素分类，我们通过添加弱标注数据来扩展上述框架，即我们向生成器提供类级别的信息。

![img](https://img-blog.csdnimg.cn/20190402191618557.png)   

**图18：**弱监督（类级别标签） GAN

![img](https://img-blog.csdnimg.cn/2019040219164493.png)   

**图19：**半监督GAN

------

# 数据集

| 数据集                                                       | 训练数量                   | 测试数量 | 类别数量                      |
| ------------------------------------------------------------ | -------------------------- | -------- | ----------------------------- |
| [CamVid](http://mi.eng.cam.ac.uk/projects/segnet/)           | 468（包含 101 张验证图像） | 233      | 32（SegNet 论文中使用 11 类） |
| [PascalVOC 2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/) | 9963                       | 1447     | 20                            |
| [NYUDv2](http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html) | 795                        | 645      | 40                            |
| [Cityscapes](https://www.cityscapes-dataset.com/)            | 2975                       | 500      | 19                            |
| [Sun-RGBD](http://rgbd.cs.princeton.edu/)                    | 10355                      | 2860     | 37                            |
| [MS COCO](http://mscoco.org/)                                | 80000                      | 40000    | 80                            |
| [ADE20K](http://groups.csail.mit.edu/vision/datasets/ADE20K/) | 20210                      | 2000     | 150                           |

------

# 结果

![img](https://img-blog.csdnimg.cn/20190402191926539.jpg)

**图20：**FCN-32s 生成的样本语义分割图，来自 Pascal VOC 验证集


最后挂一个复制粘贴还标翻译标原创的

https://blog.csdn.net/mieleizhi0522/article/details/82469160

## **图像风格的损失函数**

语义分割模型在训练过程中通常使用一个简单的交叉熵损失函数。但是，如果你对获取图像的粒度信息感兴趣，则必须使用到稍微高级一些的损失函数。

我们来看几个例子。

## **Focal Loss**

这种损失是对标准交叉熵的一种改进。这是通过改变其形状来实现的，这样分配给分类良好的样本的损失的权重就会降低。最终，这确保了不存在类别不平衡。在这个损失函数中，随着对预测正确的类别的信心增加，交叉熵损失的比例因子在0处衰减。比例因子自动降低了训练时简单样本的权重，并将重点放在困难的样本上。

![img](https://pic3.zhimg.com/80/v2-a7a97b81f8503f13082cbbd77a30b56a_1440w.jpg)

## **Dice loss**

这个损失是通过计算smooth dice coefficient函数得到的。这种损失是分割问题中最常用的损失。

![img](https://pic1.zhimg.com/80/v2-bcccfdf2ce77a6bd2464bd65a338b05c_1440w.jpg)

## **Interp over Union (IoU)-balanced Loss**

IoU平衡分类损失的目的是增加高IoU样本的梯度，减少低IoU样本的梯度。这样可以提高机器学习模型的定位精度。

![img](https://pic1.zhimg.com/80/v2-5739db1c019f71ecc30950dd815ccbb4_1440w.jpg)

## **Boundary loss**

边界损失的一种变体适用于具有高度不平衡分割的任务。这种损失的形式是空间轮廓上的距离度量，而不是区域。通过这种方式，它解决了高度不平衡的分割任务的区域损失所带来的问题。

![img](https://pic4.zhimg.com/80/v2-a8d80b06d0f65346335739874892b94f_1440w.jpg)

## **Weighted cross-entropy**

交叉熵的一种变体，所有正样本都用一个确定的系数来加权。它用于类别不平衡的情况。

![img](https://pic3.zhimg.com/80/v2-a91dd42a1635117e6105b2e1a32c106a_1440w.jpg)

## **Lovász-Softmax loss**

这个损失直接在神经网络中基于子模块的凸Lovasz扩展损失来优化平均IOU损失。

![img](https://pic3.zhimg.com/80/v2-01d7e31f169873ed8e265f3ee8e8c0d2_1440w.jpg)

其他值得一提的损失有：

- **TopK loss** 其目的是确保网络在训练过程中集中于困难样本。
- **Distance penalized CE loss** 让网络去重点优化难以分割的边界区域。
- **Sensitivity-Specificity (SS) loss** 计算特异性和敏感性的均方差的加权和。
- **Hausdorff distance(HD) loss** 估计了卷积神经网络的Hausdorff距离。

这些只是在图像分割中使用的一些损失函数。要了解更多，请查看这个：[https://github.com/JunMa11/SegLoss](https://link.zhihu.com/?target=https%3A//github.com/JunMa11/SegLoss)。

## **图像分割数据集**

如果你还在看，你可能会问自己从哪里可以获得一些数据集来开始。

让我们来看一些。

## **Common Objects in COntext — Coco Dataset**

COCO是一个大型的物体检测、分割和图像描述数据集。包含91个类。它有25万人拥有关键点，它包含80个对象类别。

链接：[https://www.tensorflow.org/datasets/catalog/coco](https://link.zhihu.com/?target=https%3A//www.tensorflow.org/datasets/catalog/coco)

## **PASCAL Visual Object Classes (PASCAL VOC)**

PASCAL有20个不同的类，9963张图片。训练/验证集是一个2GB的tar文件。

链接：[http://host.robots.ox.ac.uk/pascal/VOC/voc2012/](https://link.zhihu.com/?target=http%3A//host.robots.ox.ac.uk/pascal/VOC/voc2012/)

## **The Cityscapes Dataset**

这个数据集包含城市场景的图像。它可以用来评价视觉算法在城市场景中的性能。

链接：[https://www.cityscapes-dataset.com/downloads/](https://link.zhihu.com/?target=https%3A//www.cityscapes-dataset.com/downloads/)

The Cambridge-driving Labeled Video Database — CamVid

这是一个基于运动的分割和识别数据集。它包含32个语义类。

链接：[http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/](https://link.zhihu.com/?target=http%3A//mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/)